<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta name="keywords" content="天津大学 · 井佩光" />
    <title>TJUMMG主页</title>
    <link rel="stylesheet" href="common/css/bootstrap.min.css">
    <link rel="stylesheet" href="common/css/index.css">
    <link rel="shortcut icon" href="common/img/logo3.png">
    <style>
        /* 论文标签样式 */
        .tabs {
            display: flex;
            cursor: pointer;
            padding: 10px;
            background-color: rgb(255, 255, 255);
            border-radius: 8px;
        }
        .tab {
            margin-right: 10px;
            padding: 5px 20px;
            border-radius: 8px;
            font-size: 14px;
            background-color: #f0f0f0;
            color: #333;
            transition: background-color 0.3s, color 0.3s;
            font-weight: bold;
            border: 1px solid #ddd;
        }
        .tab.active {
            background-color: rgb(0, 70, 140);
            color: white;
            border: 1px solid rgb(0, 70, 140);
        }
        .tab:hover {
            background-color: #ddd;
        }
        .paper-list {
            display: none;
            margin-top: 20px;
        }
        .paper-list.active {
            display: block;
        }
        /* 标签样式 */
        .item {
            display: none; /* 默认显示所有标签 */
        }
        .show-more {
            cursor: pointer;
            color: blue;
            text-decoration: underline;
            margin-top: 20px;
            display: inline-block;
        }
        .show-less {
            cursor: pointer;
            color: blue;
            text-decoration: underline;
            margin-top: 20px;
            display: inline-block;
        }
        /* 提示框样式 */
        #notification {
            position: fixed;
            top: 20px;
            right: 20px;
            background-color: #4CAF50;
            color: white;
            padding: 10px;
            border-radius: 5px;
            display: none; /* 默认隐藏 */
            z-index: 1000;
        }
        /*标签样式*/
        .number-box {
            display: inline-block;
            width: 15px;
            height: 15px;
            line-height: 25px;
            text-align: center;
            border-radius: 3px;
            font-size: 16px;
            margin-right: 5px;
            color: white;
            font-weight: bold;
        }
        .box {
            font-size: 17px;
            width: 33%;  /* 每个div占30%的宽度 */
            padding: 0px;
            background-color: #ffffff;
            border: 1px solid #ffffff;
            text-align: left;
            border-radius: 5px;
        }
    </style>
</head>

<body>

<style>
A.applink:hover {border: 2px dotted #DCE6F4;padding:2px;background-color:#ffff00;color:green;text-decoration:none}
A.applink       {border: 2px dotted #DCE6F4;padding:2px;color:red;background:transparent;text-decoration:none}
A.info          {color:red;background:transparent;text-decoration:none}
A.info:hover    {color:green;background:transparent;text-decoration:underline}
</style>



    <div class="container-fluid" style="padding:0 10%;">
        <dic class="row">
            <div class="col-md-2 col-xs-2 col-sm-2 navbar-fixed-left" >
                <div class="warpper" >
                    <div class="logo" >
                    </div>
                    <ul class="nav nav-pills nav-stacked " >
                        <li role="presentation" class="active"><a href="index.html">hhh导师简介</a></li>
                        <li role="presentation" ><a href="member.html">多媒体组介绍</a></li>
                        <li role="presentation" ><a href="research.html">研究成果展示</a></li>
                    </ul>
                </div>
            </div>
            <div class="col-md-10 col-xs-10 col-sm-10 mb20" >
                <nav class="navbar navbar-default ">
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                          <span class="sr-only">Toggle navigation</span>
                        </button>
                      </div>
                    <div class="container-fluid " style="padding: 0 ;">
                      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1" >
                          <ul class="nav navbar-nav">
                              <li class="active"><a href="#personal">关于我</a></li>
                              <li><a href="#direct">研究方向</a></li>
                              <li><a href="#project">科研项目</a></li>
                              <li><a href="#paper">发表论文</a></li>
                              <li><a href="#services">学术兼职</a></li>
                              <li><a href="#address">通讯信息</a></li>
                          </ul>
                      </div>
                    </div>
                </nav>
                <div class="content">
                    <p id="personal" style="height:4rem;margin: 0;">&nbsp;</p>
                    <section>
                        <div class="my-auto" style="font-size: large;">
                            <p align="center">
                                <img style="border-radius: 12px;" height="213" src="common/img/bio.jpg" alt hspace="30" width="160" align="right" border="0" />
                            </p>
                            <h3 class="mb-0"><strong>Peiguang Jing · 井佩光</strong></h3>
                            </br>
                            <p><strong>职称：</strong>副教授/硕士生导师/博士团导师</p>
                            <p><strong>学院：</strong>天津大学电气自动化与信息工程学院</p>
                            <p><strong>学科专业：</strong>信息与通信工程</p>
                            <p><strong>个人主页：</strong><a href="https://peiguangjing.github.io/">https://peiguangjing.github.io/</a></p>
                            <p><strong>Google学术：</strong><a href="https://scholar.google.com/citations?user=Kr-OYyQAAAAJ&hl=zh-CN">https://scholar.google.com/citations?user=Kr-OYyQAAAAJ&hl=zh-CN</a></p>
                            
                            <ul>
                                <li><p>井佩光，博士，天津大学电气自动化学院副教授，硕导及博导师团导师，2018年毕业于天津大学信息与通信工程专业获工学博士学位，同年获天津大学优秀博士论文，并入选“北洋学者-青年骨干教师计划”，教育部学位与研究生教育评估监测专家库专家。</p>
                                </li>
                                <li><p>作为项目负责人及项目骨干参与并完成多项国家自然科学基金、天津市人工智能科技重大专项、天津基础科技计划项目、博士后科学基金面上项目、自主创新基金等科研项目。申请人聚焦于基于多模态学习的主观语义分析、情感分析等各种智能分析问题，从多媒体数据的内容理解与分析、管理及分发两个层面开展相关研究。</p>
                                </li>
                                <li><p>研究内容涵盖短视频/图像/音频的抽象级语义分析与识别、网络轻量化设计、多标签分类及自动标注，大规模快速检索，取得具有国际水平、国内领先的成果。已经累计发表60余篇国际SCI期刊论文及10余篇国内高水平中文期刊论文，谷歌学术引用超1000次, 知网单篇最高下载量超1900。</p>
                                </li>
                                <li><p>近五年，以第一或通讯作者发表中科院SCI一区或TOP期刊30余篇，ESI高被引论文2篇，ESI热点论文1篇，IEEE/ACM系列长文论文如IEEE TKDE/TIP/TCYB/TCSVT/TMM/IOT等二十余篇，累计授权20余项发明专利。担任两种国际SCI期刊的客座编委，担任包括众多顶级学术期刊在内的30余种SCI期刊的审稿人及多项国际会议程序委员会委员，如IEEE TPAMI / TNNLS / TGRS / TKDE / TMM / TCSVT / TCYB / SPL、ACM TWEB/TOMM/MM、中国科学:信息科学/技术科学等。</p>
                                </li>
                                <p style="color:red;">欢迎有志投身人工智能领域的热忱学子报考我的研究生或本科科研训练及毕设！2026年招生开启！（联系邮箱：<a href="mailto:pgjing@tju.edu.cn">pgjing@tju.edu.cn</a>）</p>
                                <p style="color:red;">欢迎与我联系申请天津大学博士或博士后职位！（联系邮箱：<a href="mailto:pgjing@tju.edu.cn">pgjing@tju.edu.cn</a>）</p>
                            </ul>
                        </div>
                    </section>
                    <p id="direct" style="height:4rem;margin: 0;">&nbsp;</p>
                    <section style="font-size: 1.8rem;">
                        <h3 ><strong>研究方向</strong></h3>
                        <ul>
                            <li><p><strong>短视频内容分析与理解：</strong>课题以短视频多模态融合为主线，面向短视频的内容分析与理解，管理与分发两个层次展开，研究内容涵盖短视频流行度预测、多标签分类及自动标注、记忆度预测、个性化推荐等。</p>
                            </li>
                            <li><p><strong>经典媒体数据语义分析：</strong>语义分析是视觉理解中高层认知的重点和难点问题，课题研究从经典多媒体数据的表示学习出发，针对多媒体数据的情感计算、时尚穿搭、工业缺陷检测等展开研究。</p>
                            </li>
                            <li><p><strong>多源图像融合及增强：</strong>多源图像融合及增强是指将来自不同传感器、不同视角或不同场景拍摄的多幅图像融合成一幅图像，以提高图像的质量、增强信息的丰富度和稳健性。</p>
                            </li>
                            <li><p><strong>人口老龄化与主动健康：</strong>随着老龄化人口比例及规模的急剧增强，课题面向以阿尔茨海默病为代表的退行性神经疾病开展智能筛查、诊断、康复相关研究工作，以延缓老龄化的进程，提高老年人健康水平。</p>
                            </li>

                        </ul>
                    </section>
                    <p id="project" style="height:4rem;margin: 0;">&nbsp;</p>
                    <section style="font-size: 1.8rem;">
                        <h3 ><strong>科研项目</strong></h3>
                        <ul>
                            <li><p>广西多源信息挖掘与安全重点实验室，短视频多标签分类关键技术研究，2023.01-2024.12，主持。</p>
                            </li>
                            <li><p>天津大学自主创新基金，基于三维眼动信息的阿尔茨海默病诊断新方法，2022.01-2022.12， 主持。</p>
                            </li>
                            <li><p>天津市自然科学基金青年项目，面向短视频语义分类的多模态数据表示方法研究，2020.04-2022.03，主持。</p>
                            </li>
                            <li><p>天津大学“北洋学者—青年骨干教师计划”，2020.01-2021.12，主持。</p>
                            </li>
                            <li><p>国家自然科学基金青年项目，新媒体环境下短视频事件检测研究, 2019/01—2021/12，主持。</p>
                            </li>
                            <li><p>博士后科学基金面上项目, 2019M651038, 面向短视频复杂事件检测的关键技术研究， 2019/01—2021/12，主持。</p>
                            </li>
                            <li><p>国家自然科学基金面上项目，61771338，消费级行人航位推算系统的关键算法研究，2018/01—2021/12，参与。</p>
                            </li>
                            <li><p>国家自然科学基金面上项目，61572356，数字图像/视频平滑滤波盲取证技术研究，2016/01—2019/12，参与。</p>
                            </li>
                            <li><p>国家自然科学基金面上项目，61572352,模拟视觉系统信息处理机制的生物启发式图像哈希研究，2016/01—2019/12，参与。</p>
                            </li>

                        </ul>
                    </section>


                    <p id="paper" style="height:4rem;margin: 0;">&nbsp;</p>
                    <section style="font-size: 1.8rem;">
                        <h3><strong>发表论文</strong></h3>
                        <div class="tabs">
                            <div class="tab" onclick="showPapers('2026')">近期论文</div>
                            <div class="tab" onclick="showPapers('2025')">2025</div>
                            <div class="tab" onclick="showPapers('2024')">2024</div>
                            <div class="tab" onclick="showPapers('2023')">2023</div>
                            <div class="tab" onclick="showPapers('2022')">2022及以前</div>
                            <div class="tab" onclick="showPapers('2021')">中文期刊</div>
                        </div>
                        
                        <div id="notification">所选论文的bib已复制 !</div>

                        <div id="2026" class="paper-list">
                            <ol>

                                 <li class="item"><p align="justify"> <b>ICCV2025</b> | MPBR: Multimodal Progressive Bidirectional Reasoning for Open-Set Fine-Grained Recognition
                                    [<a href="">pdf</a>] 
                                    [<a href="">code</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '')">bib</a>]            
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Wei Lu, He Zhao, Dubuke Ma, <b>Peiguang Jing</b>, LUFormer: A luminance-informed localized transformer with frequency augmentation for nighttime flare removal, Neural Networks, Volume 190, 2025, 107660, ISSN 0893-6080.
                                    [<a href="https://pdf.sciencedirectassets.com/271125/1-s2.0-S0893608025X00071/1-s2.0-S0893608025005404/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjECUaCXVzLWVhc3QtMSJGMEQCIHd2KAB3O%2Fa0gG2%2FzZxHoI1dnydkJjXHQnvjkN%2BBbKyiAiA2OMIvAgFrHyNgAHTc8yCMyTgZeXfmmjAItrKDVPPVmyqyBQg9EAUaDDA1OTAwMzU0Njg2NSIMUQSJAB5dfFP4UyA2Ko8FJwIZhWKklfqMUe2k2yff%2FuWN0EceIqhC%2FKO6Woel%2BuUkunPm4YDT%2FQP1LhsumUEGWvkGDfjttZDGLZNxCi6WrDtD822vDw3WBaTZInHmhVV7KwW44JKc9qT5o2%2FKf4di6HBVcdQaPEQEqfgEnH1E1xB8kJ1kY3Htb6NeAsTsoAG09%2Fv9%2BQOymcUZGEVGQ0Feahwn6My3G7wvbvNYpFLTMJqdGTANOSLrLZwH0HkTaHO8kybvOe4QikD30%2BC2%2F0CHWw1ZII%2BzoIuFFwhBTnoC83tB%2BAwASYx3oR2cC3uFi08qTLH4xRgMd7vTZXVhBwFXqfdAp2F%2Fo8GvK8d8bPV7VPKFvlpKEYus2iIBnmv3n68vTthizLmacarzgyrG4BvOy8iSgHyzaLgGn0rIUmf8lI1Tn0aJXPvAmsJnIumIrPalFp523gRVVMxpnv7SMtRdKE%2BeC%2BMC%2B7yhJpPlEbdXWtSxvpYGu24r7cQaSJgmjVheAeBoztsdyUyNm8tcGYjAP8ncNgF811OUDU5pOyay4ERHvSrz7%2BzOPEvoG53kiwhEcRNAvRp2iWCdI7BfgcyVJ15zB0t0DkK6v06rTdOv5a1zsx1oh8fA%2Fgua9JZiBX5ZA6vUciFtkOhN7zrq12zD7gNibC4J7Jz%2BkWAbTgA7lQNWoe8oGi1Oix0H5GmExO0ndiPm7B0vYwPLIBeHdVkYSC667ucmYT%2BAHPb5irk39lxm3GFtoPz5ypc3e1e6SIiLXF9OZsjr%2FaLGBtmXn7JMEQZLB47asGg4zRio5K8YEzgcUL0MZyRmjahlJxKbn%2FLZ9opDAYVo3v1Rfur1ljeqIvx5XUeAB9OUCXX2ZPsUGHXTKyQREGzQMNNkl0P35DDxqNfDBjqyAY6Zh0zpEaVpnfTl0APbvY59r5VetdL81Ljx%2BH4Lk3v%2F9iiZ4S2HW%2BjPoJ8ajJCZvNpt8tgB0zGp2YncwYYI3jxeajIO%2FhsbHtuJhHGPm0ekaA0gl%2FMD9JPvWAHXFqCjDjfnN%2BDvtWWvwyqUYl3%2B%2FyktTuRPaTI73oIcTYFW5FjhfopMXJCtKg2SJhMFCzSj15BXfV03ezxJDQTuiGtj2ooPC404MLfqNiVd0ajAlqpTBi0%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250715T045518Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY5EUIDS6Q%2F20250715%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8292d8d673de5dba4973df85fa81fac049e59824e29156ac19849bc72caec0b9&hash=fb9a02475f86388c6d7e3ae3c76249640b111973d40969c3321b637172f8aecd&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0893608025005404&tid=spdf-edab3ea5-f723-451a-9aa0-700b1e0b2df5&sid=f22364994026e54d6f2b0a7-e63ba6ed783fgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=190c5b51095602020454&rr=95f6aab3afb485bb&cc=cn&kca=eyJrZXkiOiJQMXVEcG9qVm9BdUowL245L1E0azhIa2lCTnlPYjB1UEZNU3crMThJSUhQSERHNGIyWWVwYXpjZ1k2OEFGbXhnMXA3MUVkbkZJdCt6MzRIOG9aNGRMTE0vcUNpcldVODh5UkcybTRpR0pGL2pGVUUwLzlCOGdWYlgvZ05xblJFWDhwRDJEWjY1RGIxQmtUN1RLSUNCSnBRRWtkQjVhTUdyYWFNLzBTM1BWaEl1eXBGK1BBPT0iLCJpdiI6IjNhMzNmNzA0ZjM2MDZjMjM5ODkxN2E3MThlOTIzYjk0In0=_1752555327519">pdf</a>] 
                                    [<a href="">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@article{LU2025107660,title = {LUFormer : A luminance-informed localized transformer with frequency augmentation for nighttime flare removal},journal = {Neural Networks},volume = {190},pages = {107660},year = {2025},issn = {0893-6080},doi = {https://doi.org/10.1016/j.neunet.2025.107660},url = {https://www.sciencedirect.com/science/article/pii/S0893608025005404},author = {Wei Lu and He Zhao and Dubuke Ma and Peiguang Jing},keywords = {Image restoration, Flare removal, Multi-domain information fusion, Wavelet transforms},abstract = {Flare caused by unintended light scattering or reflection in night scenes significantly degrades image quality. Existing methods explore frequency factors and semantic priors but fail to comprehensively integrate all relevant information. To address this, we propose LUFormer, a luminance-informed Transformer network with localized frequency augmentation. Central to our approach are two key modules: the luminance-guided branch (LGB) and the dual domain hybrid attention (DDHA) unit. The LGB provides global brightness semantic priors, emphasizing the disruption of luminance distribution caused by flare. The DDHA improves deep flare representation in both the spatial and frequency domains. In the spatial domain, it broadens the receptive field through pixel rearrangement and cross-window dilation, while in the frequency domain, it emphasizes and amplifies low-frequency components via a compound attention mechanism. Our approach leverages the LGB, which globally guides semantic refinement, to construct a U-shaped progressive focusing framework. In this architecture, the DDHA locally augments multi-domain features across multiple scales. Extensive experiments on real-world benchmarks demonstrate that the proposed LUFormer outperforms state-of-the-art methods. The code is publicly available at: https://github.com/HeZhao0725/LUFormer.}}')">bib</a>]            
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yuting Su, Enqi Su, Weiming Wang, <b>Peiguang Jing</b>, Dubuke Ma, Fu Lee Wang. Adversarial neighbor perception network with feature distillation for anomaly detection[J]. Expert Systems with Applications, 2025, 274: 126911.
                                    [<a href="https://pdf.sciencedirectassets.com/271506/1-s2.0-S0957417425X00063/1-s2.0-S0957417425005330/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCYq0Q%2BHnlmKl946A4ZefNBwFdnq%2B9HmqZ90JONwEofpgIhALdimQlffBIuUgLo0iNQnynWIE7D5Q4vuFlzql3lOXjTKrMFCEUQBRoMMDU5MDAzNTQ2ODY1Igy1udJTUyN19wdvn8QqkAWPW%2BM6eQA9SyScaznujkipWhc8lBcYopK5N35AifXWHHngHVKP0slZzEWf1e5BXZC0tGpinSJxgUNCB04Cx%2BUbeA1e3csgy2jn3KdpsJjLrIHH6USqLoLsyPw8IJFA2WIID5uJxXCn9lWWHvim4OkbRSR%2B2RsiaVJEGmMBGEznQoVQu0pqrv7%2BOyT0Zm0sUumLfklxx%2F5BR%2FRirKwzg1DH%2ByWMGQrCSto0eZS1UIUhls6gWfRQw2pXG6C5zm%2Bv%2FYr%2BBFG%2FQ0dPqIFLV60XHeEFe9kDLyC%2Bl2k3CQIKpiNdU4Sbd62b2s6cSRaxrFeEdti4gbptO6U7c2FB4yQAVnbZ3t5cwSikWqOeX3HRnYnLaDCFhk89agUUzJ3Um2nm6%2FfnCPN35GZoih9DnS6TBuutqhO%2FP4nhxnlw4waJ5phAGBBdINf0HatXORF8GVR6J%2FdB9XI7Q3UkyZuAOnDKQ8owiXNCAk7aZTkzBVobOSGpplk4%2Bt5NHFPbrvInC55K77J1%2FK3v9XUp3Cn2RemSKp%2BlGDo8iXUxvigmjup%2Fxh0r8yNU8t3hIy%2FRbvMaAglx21H6kychfldDiimeR4jHlraW%2B1M%2B6KDvyvPJH9nmHAE8%2FELvStMqlTh13Xn4FWYzIpUlxNicPJdADiuPgn1mixTRHfS4wI67Yohkv9VSq0DfAt8NBeL9cQI1ZHkZQa9Cs5WDQ0x9VQLMn6XdQaOL0LeJy4C7UCkLFDGvQ2FQp8sHOt2ZiNAOUs73A9v4EGOyWD4KUp6N2xxFkj2NShIzHjeMhRTtDSkrdo%2F1X7FuYkuYeNnvncFia3L7aIXJO3J535ZPi7qJhOE54432cbE%2FRkzjyacMll4MVReBxAdVyMyKJDCkq%2F6%2FBjqwAXkalfwd5qcBPa6E2jqPTeY4VMiivNuYmIj%2FzMSYUYEVDV9b2ZrVLVy4jKU0%2FUExIL9IT%2F1vJu%2F0zixRPrBFe3REibki3OFdZMOJ77uHtk2ObdWTU9Rg89Ahen8BsUTMSSQt9pxpASmQDchLFx9NlAl7NgWTMzV9%2FCnFv3HCRQSmeKzzMSfY9hlcdF7QRKLfgrKy21zuRmHlxeKzZRxLWYRlX06c6OYhZ8Mm%2FamOygpc&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250416T125548Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUUOQNEKJ%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=6e556688e94531b556fb04980fa19f78433f4e5b29dad7a35c9513adf13d077f&hash=6f3c98d33e774e6da2f85427864179254b7c1368db9fdde6008dd26b30072bef&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0957417425005330&tid=spdf-43a64672-31f2-456b-9643-44ebec4f93d1&sid=5b329c8456b1f446271952a8b57bb1750a63gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57540b52035f&rr=9313d6d07e83ddc0&cc=cn&kca=eyJrZXkiOiJSWmVDMFUvQ3gvdFhXV25EM1d3cUVrNHFlbHVYVzZPSDBUd2R6MWVtekdNc25mMkdGMGozQVVuazJKU1V4VTlaRjBSbHNNM09Pdk4zMEZXUHJqTDdsSHRsdmwyUmlXQ1VLVjc0MHF2ZElzNFFDc0UrZG9LZ09qSXJvQlNUMEgwQmVKakIzUEp1b2FUNzhSMTB3RkZTVTNPVEdORnV1aHBmNVhwS0x2M2RPSzJrTFJNbyIsIml2IjoiMWQ2ZTQ2NzUyMDNjNmJlYzE2OWI5N2JlMGM3ZmNjZjIifQ==_1744808153802">pdf</a>] 
                                    [<a href=" https://github.com/thesusu/ANP-FD">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@article{SU2025126911,title = {Adversarial neighbor perception network with feature distillation for anomaly detection},journal = {Expert Systems with Applications},volume = {274},pages = {126911},year = {2025},issn = {0957-4174},doi = {https://doi.org/10.1016/j.eswa.2025.126911},url = {https://www.sciencedirect.com/science/article/pii/S0957417425005330},author = {Yuting Su and Enqi Su and Weiming Wang and Peiguang Jing and Dubuke Ma and Fu Lee Wang},keywords = {Anomaly detection, Unsupervised methods, Neighbor perception, Spatial location, Feature distillation},abstract = {Anomaly detection has become a research hotspot in the field of intelligent manufacturing, which has attracted strong attention from academia and industry. Although the unsupervised methods based on reconstruction have shown promising results in anomaly detection, they still have the problems that the representation of abnormal features is not significantly different and the representation of normal features is not accurate enough. To solve these problems, we propose an adversarial neighbor perception network with feature distillation (ANP-FD) for anomaly detection, which includes multi-scale neighbor perception, robust distillation recovery, and co-attention adversarial detection modules. First, the multi-scale features pass through the neighbor awareness to predict accurate spatial location information to improve the reconstruction accuracy. Then, the abnormal features are eliminated by a robust trainable distillation structure, which expands the representation difference of abnormal features during reconstruction. In addition, the co-attention adversarial detection can accurately detect and locate anomalies in the multi-scale feature space. The experimental results on the MVTec, BTAD, MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our proposed method achieves better performance than current state-of-the-art (SOTA) approaches. Especially, the proposed method achieves 99.5 AUROC% and 94.5 AUPRO% on the MVTec. We also achieve superior performance in few-shot scenarios. Code: https://github.com/thesusu/ANP-FD.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yu Qiao, Wei Lu, <b>Peiguang Jing</b>, Weiming Wang, Yuting Su. Multimodal Dual-Graph Collaborative Network With Serial Attentive Aggregation Mechanism for Micro-Video Multi-Label Classification[J]. IEEE Transactions on Multimedia, 2025.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10924420">pdf</a>] 
                                    [<a href=" https://github.com/HappyField-glitch/MDGCN">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10924420,author={Qiao, Yu and Lu, Wei and Jing, Peiguang and Wang, Weiming and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={Multimodal Dual-Graph Collaborative Network With Serial Attentive Aggregation Mechanism for Micro-Video Multi-Label Classification}, year={2025},volume={},number={},pages={1-12},keywords={Correlation;Multi label classification;Videos;Graph convolutional networks;Data mining;Semantics;Feature extraction;Decoding;Training;Representation learning;Micro-video;multimodal representations;multi-label classification;graph convolutional network},doi={10.1109/TMM.2025.3542895}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Chen Zhao, Mengyuan Yu, Fan Yang, <b>Peiguang Jing</b>. VIIS: Visible and Infrared Information Synthesis for Severe Low-Light Image Enhancement. In Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision, 2025: 2174-2184.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10943586">pdf</a>] 
                                    [<a href=" https://github.com/Chenz418/VIIS">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@INPROCEEDINGS{10943586,author={Zhao, Chen and Yu, Mengyuan and Yang, Fan and Jing, Peiguang}, booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, title={VIIS: Visible and Infrared Information Synthesis for Severe Low-Light Image Enhancement}, year={2025},volume={},number={}, pages={2174-2184},keywords={Computer vision;Limiting;Fuses;Computational modeling;Noise reduction;Diffusion models;Image augmentation;Image restoration;Image enhancement;Image fusion},doi={10.1109/WACV61041.2025.00218}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yuting Su, Peng Ma, Weiming Wang, Shaochu Wang, Yuting Wu, Yun Li, <b>Peiguang Jing</b>. AMDANet: Augmented Multi-scale Difference Aggregation Network for Image Change Detection[J]. IEEE Transactions on Geoscience and Remote Sensing, 2025. 63,5616012
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10891412">pdf</a>] 
                                    [<a href=" https://github.com/mp-st/AMDANet">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10891412,author={Su, Yuting and Ma, Peng and Wang, Weiming and Wang, Shaochu and Wu, Yuting and Li, Yun and Jing, Peiguang},journal={IEEE Transactions on Geoscience and Remote Sensing}, title={AMDANet: Augmented Multiscale Difference Aggregation Network for Image Change Detection}, year={2025},volume={63},number={},pages={1-12},keywords={Feature extraction;Data mining;Transformers;Deep learning;Remote sensing;Electronic mail;Attention mechanisms;Accuracy;Vectors;Training;Change detection (CD);difference feature;feature aggregation;information augmentation},doi={10.1109/TGRS.2025.3542814}}')">bib</a>]             
                                </p>
                                </li>


                                <li class="item"><p align="justify"> Wei Lu, Yujia Zhai, Jiaze Han, <b>Peiguang Jing*</b>, Yu Liu, Yuting Su. VMemNet: A Deep Collaborative Spatial-Temporal Network with Attention Representation for Video Memorability Prediction. IEEE Transactions on Multimedia, 2024, 26: 4926-4937
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298788">pdf</a>] 
                                     [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10298788,author={Lu, Wei and Zhai, Yujia and Han, Jiaze and Jing, Peiguang and Liu, Yu and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={VMemNet: A Deep Collaborative Spatial-Temporal Network With Attention Representation for Video Memorability Prediction}, year={2024},volume={26},number={},pages={4926-4937},keywords={Visualization;Semantics;Feature extraction;Predictive models;Task analysis;Streaming media;Collaboration;Video memorability;Attention mechanism;Spatial-temporal features},doi={10.1109/TMM.2023.3327861}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Haoyi Sun, Liqiang Nie, Yun Li*, Yuting Su. Deep Multi-modal Hashing with Semantic Enhancement for Multi-label Micro-video Retrieval, IEEE Transactions on Knowledge and Data Engineering, 2024,36(10): 5080-5091
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10330746">pdf</a>] 
                                    [<a href=" https://github.com/haoyi199815/DMHSE">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10330746,author={Jing, Peiguang and Sun, Haoyi and Nie, Liqiang and Li, Yun and Su, Yuting},journal={IEEE Transactions on Knowledge and Data Engineering}, title={Deep Multi-Modal Hashing With Semantic Enhancement for Multi-Label Micro-Video Retrieval}, year={2024}, volume={36},number={10},pages={5080-5091},keywords={Semantics;Hash functions;Encoding;Representation learning;Convolutional neural networks;Quantization (signal);Kernel;Deep hashing;micro-video retrieval;multi-label;multi-modality},doi={10.1109/TKDE.2023.3337077}}')">bib</a>]             
                                </p>
                                </li>
                                
                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Kai Cui, Weili Guan, Liqiang Nie, Yuting Su. Category-aware Multimodal Attention Network for Fashion Compatibility Modeling. IEEE Transactions on Multimedia, 2024.25: 9120-9131
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10049142">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10049142,author={Jing, Peiguang and Cui, Kai and Guan, Weili and Nie, Liqiang and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={Category-Aware Multimodal Attention Network for Fashion Compatibility Modeling}, year={2023},volume={25},number={},pages={9120-9131},keywords={Visualization;Convolutional neural networks;Clothing;Representation learning;Semantics;Feature extraction;Correlation;Dynamic graph convolutional network;fashion compatibility modeling;multimodal representation},doi={10.1109/TMM.2023.3246796}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Xuan Zhao, Fugui Fan, Fan Yang, Yun Li, Yuting Su. Multimodal Progressive Modulation Network for Micro-video Multi-label Classification. IEEE Transactions on Multimedia, 2024.26: 10134-10144
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10572319">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10572319,author={Jing, Peiguang and Zhao, Xuan and Fan, Fugui and Yang, Fan and Li, Yun and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={Multimodal Progressive Modulation Network for Micro-Video Multi-Label Classification}, year={2024},volume={26},number={},pages={10134-10144},keywords={Modulation;Correlation;Semantics;Representation learning;Visualization;Task analysis;Fans;Micro-video;multi-label classification;multimodal representation},doi={10.1109/TMM.2024.3405724}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Kai Zhang, Xianyi Liu, Yun Li, Yu Liu, Yuting Su. Dual Preference Perception Network for Fashion Recommendation in the Social Internet of Things. IEEE Internet of Things Journal, 2024, 11(5): 7893-7903 
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10286302">pdf</a>] 
                                    [<a href=" https://github.com/KaiZhang1228/DP2Net">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10286302,author={Jing, Peiguang and Zhang, Kai and Liu, Xianyi and Li, Yun and Liu, Yu and Su, Yuting}, journal={IEEE Internet of Things Journal}, title={Dual Preference Perception Network for Fashion Recommendation in Social Internet of Things}, year={2024},volume={11},number={5},pages={7893-7903},keywords={Social Internet of Things;Smart homes;Task analysis;Transformers;Motion pictures;Clothing;Graph neural networks;Fashion recommendation;heterogeneous graph;preference perception;Social Internet of Things (SIoT)},doi={10.1109/JIOT.2023.3319386}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Kai Cui, Jing Zhang, Yun Li*, Yuting Su. Multimodal High-order Relationship Inference Network for Fashion Compatibility Modeling in Internet of Multimedia Things[J]. IEEE Internet of Things Journal, 2024,11(1): 353–365
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10149524">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10149524,author={Jing, Peiguang and Cui, Kai and Zhang, Jing and Li, Yun and Su, Yuting}, journal={IEEE Internet of Things Journal}, title={Multimodal High-Order Relationship Inference Network for Fashion Compatibility Modeling in Internet of Multimedia Things}, year={2024},volume={11},number={1}, pages={353-365},keywords={Visualization;Task analysis;Internet of Things;Correlation;Semantics;Feature extraction;Multimedia communication;Fashion compatibility modeling (FCM);high-order relationship inference;Internet of Multimedia Things (IoMT);multimodal representation learning},doi={10.1109/JIOT.2023.3285601}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Xiaoyu Liu, Xuehui Wang, Yuting Su. Deep Matrix Factorization with Complementary Semantic Aggregation for Micro-Video Multi-Label Classification. IEEE Signal Processing Letters, 2024,31: 1685-1689
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10345640">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10345640,author={Jing, Peiguang and Liu, Xiaoyu and Wang, Xuehui and Su, Yuting}, journal={IEEE Signal Processing Letters}, title={Deep Matrix Factorization With Complementary Semantic Aggregation for Micro-Video Multi-Label Classification}, year={2024},volume={31},number={},pages={1685-1689},keywords={Semantics;Decoding;Feature extraction;Correlation;Robustness;Matrix decomposition;Termination of employment;Micro-video;multi-label classification;deep matrix factorization;low-rank representation},doi={10.1109/LSP.2023.3340097}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Fugui Fan, Yuting Su, Liqiang Nie, <b>Peiguang Jing*</b>, Daozheng Hong, Yu Liu. Dual-domain Aligned Deep Hierarchical Matrix Factorization Method for Micro-video Multi-label Classification. IEEE Transactions on Multimedia, 2024,26: 2598-2607
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10214127">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10214127,author={Fan, Fugui and Su, Yuting and Nie, Liqiang and Jing, Peiguang and Hong, Daozheng and Liu, Yu},journal={IEEE Transactions on Multimedia}, title={Dual-Domain Aligned Deep Hierarchical Matrix Factorization Method for Micro-Video Multi-Label Classification}, year={2024},volume={26},number={},pages={2598-2607},keywords={Semantics;Correlation;Visualization;Task analysis;Matrix decomposition;Estimation;Training;Micro-video;multi-label classification;semantic alignment;deep matrix factorization},doi={10.1109/TMM.2023.3301224}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Fugui Fan, <b>Peiguang Jing</b>, Liqiang Nie, Haoyu Gu, Yuting Su*. SADCMF: Self-Attentive Deep Consistent Matrix Factorization for Micro-Video Multi-Label Classification. IEEE Transactions on Multimedia, 2024, 26: 10331-10341
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10540235">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10540235,author={Fan, Fugui and Jing, Peiguang and Nie, Liqiang and Gu, Haoyu and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={SADCMF: Self-Attentive Deep Consistent Matrix Factorization for Micro-Video Multi-Label Classification}, year={2024},volume={26}, number={},pages={10331-10341},keywords={Correlation;Semantics;Task analysis;Collaboration;Visualization;Social networking (online);Fans;Micro-video;multi-label classification;deep matrix factorization;self-attention},doi={10.1109/TMM.2024.3406196}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Xianyi Liu, Lijuan Zhang, Yun Li, Yu Liu, Yuting Su. Multimodal Attentive Representation Learning for Micro-video Multi-label Classification. ACM Transactions on Multimedia Computing, Communications and Applications. 2024, 20(6): 182, pp 1-23.
                                    [<a href="https://dl.acm.org/doi/pdf/10.1145/3643888">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{10.1145/3643888,author = {Jing, Peiguang and Liu, Xianyi and Zhang, Lijuan and Li, Yun and Liu, Yu and Su, Yuting},title = {Multimodal Attentive Representation Learning for Micro-video Multi-label Classification},year = {2024},issue_date = {June 2024},publisher = {Association for Computing Machinery},address = {New York, NY, USA},volume = {20},number = {6},issn = {1551-6857},url = {https://doi.org/10.1145/3643888},doi = {10.1145/3643888},abstract = {As one of the representative types of user-generated contents (UGCs) in social platforms, micro-videos have been becoming popular in our daily life. Although micro-videos naturally exhibit multimodal features that are rich enough to support representation learning, the complex correlations across modalities render valuable information difficult to integrate. In this paper, we introduced a multimodal attentive representation network (MARNET) to learn complete and robust representations to benefit micro-video multi-label classification. To address the commonly missing modality issue, we presented a multimodal information aggregation mechanism module to integrate multimodal information, where latent common representations are obtained by modeling the complementarity and consistency in terms of visual-centered modality groupings instead of single modalities. For the label correlation issue, we designed an attentive graph neural network module to adaptively learn the correlation matrix and representations of labels for better compatibility with training data. In addition, a cross-modal multi-head attention module is developed to make the learned common representations label-aware for multi-label classification. Experiments conducted on two micro-video datasets demonstrate the superior performance of MARNET compared with state-of-the-art methods.},journal = {ACM Trans. Multimedia Comput. Commun. Appl.},month = mar,articleno = {182},numpages = {23},keywords = {Micro-video, multimodal representations, multi-label, graph network}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Fangyu Zuo, <b>Peiguang Jing</b>, Jinglin Sun, Jizhong Duan, Yong Ji, Yu Liu. Deep Learning-based Eye-Tracking Analysis for Diagnosis of Alzheimer's Disease Using 3D Comprehensive Visual Stimuli, IEEE Journal of Biomedical and Health Informatics,2024,28(5): 2781-2793
                                    [<a href="https://arxiv.org/pdf/2303.06868">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10433663,author={Zuo, Fangyu and Jing, Peiguang and Sun, Jinglin and Duan, Jizhong and Ji, Yong and Liu, Yu},journal={IEEE Journal of Biomedical and Health Informatics}, title={Deep Learning-Based Eye-Tracking Analysis for Diagnosis of Alzheimer’s Disease Using 3D Comprehensive Visual Stimuli},  year={2024},volume={28}, number={5},pages={2781-2793},keywords={Visualization;Gaze tracking;Biomarkers;Three-dimensional displays;Medical diagnostic imaging;Alzheimers disease;Convolutional neural networks;Deep learning;Heat maps;Alzheimers disease (AD);eye-tracking;visual attention;convolutional neural network},doi={10.1109/JBHI.2024.3365172}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yun Li, Xianyi Liu, Lijuan Zhang, Haoyu Tian, <b>Peiguang Jing</b>. Multimodal semantic enhanced representation network for micro-video event detection. Knowledge-Based Systems, 2024, 301: 112255.
                                    [<a href="https://www.sciencedirect.com/science/article/pii/S095070512400889X">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{LI2024112255,title = {Multimodal semantic enhanced representation network for micro-video event detection},journal = {Knowledge-Based Systems},volume = {301},pages = {112255},year = {2024},issn = {0950-7051},doi = {https://doi.org/10.1016/j.knosys.2024.112255},url = {https://www.sciencedirect.com/science/article/pii/S095070512400889X},author = {Yun Li and Xianyi Liu and Lijuan Zhang and Haoyu Tian and Peiguang Jing},keywords = {Micro-video, Event detection, Multimodal representation, Visual concepts},abstract = {Currently, micro-videos have gained widespread acceptance as a prominent form of user-generated content across various social media platforms. Accurate event analysis of micro-videos can greatly enhance the many diverse social media platforms applications. Although some studies have shown promising results from a multimodal perspective, there is still a challenge in extracting informative cues from inaccurate modalities, particularly for text modality that is prone to inaccuracies and noise. In this paper, we propose a multimodal semantically enhanced representation network (MSERN) for micro-video event detection. To better address inaccurate and noisy text sentences, we first extract visual concepts in the form of adjective-noun pairs (ANPs), through a fine-grained common representation module, to complement the textual descriptions. To maximize the acquisition of modality-specific cues from both visual and textual modalities, we then implement a coarse-grained private representation module to ensure that private representations encompass unique facets of the modalities beyond the common perspective. Finally, because two modules will collaborate, the fine-grained common and coarse-grained private representations are integrated to ensure a reinforced micro-video representation. We evaluate our proposed method on a micro-video event detection dataset and the experimental results show a superior performance compared to the state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yuting Su, Weikang Wang, Jing Liu, <b>Peiguang Jing</b>. Collaborative spatial-temporal video salient object detection with cross attention transformer. Signal Processing, 2024, 224: 109612.
                                    [<a href="https://pdf.sciencedirectassets.com/271605/1-s2.0-S0165168424X00079/1-s2.0-S0165168424002317/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEND%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIELkbnFFLwE6Ug%2B6gp1PEoTX9dGwMEf369RGSbWp1KYtAiEAjzasGV6xOqiyvdYXMXK0kBrrufZIsO8u6fGGgaRVkUMqswUIWBAFGgwwNTkwMDM1NDY4NjUiDN1OPuoKo7bp6DofECqQBZk1WuhFJ0j3Dwov2LjVZjYlAUS4R%2FI8kw0u2HzhFI7kbW8AGRo9NWoeC3ilGNhLhv3QBrWGHRVDvZtLQU9IIaogEA3I5MU4iIDhMYN79hKqPXtOU209pRPN9Il04mRxrXRHDP%2F6mjCMg2OtNtDkM9tfS9aHpUqFkh4rj40C7g9XGO9%2BbRrHrmYdBnbwm%2BI2k85MBISrSbOe1uEBwsmrJOHsWygF5jQ9B%2BE1R8Pa37oYRKP5okTP6%2F9m5pJfYoLprKpWaY9suAJiR%2BYPJz%2FoGdE4LXFgGrDlEm%2F9%2FzYEGfNWxY5K1MZfhlRZwSqeEIjvuvRJzdkXZ6%2F%2BpK3ve%2F8FI44FXS1A4o2FGWFhhNDyky3UaOspzUTUivudCi0uh9sha8e2pRLQUGE%2BU2M%2B8np8FOJ9lTeVR7pNSwcRfJCcwkIo%2FRBHloaeJN9m1hEEcr%2BwXhdTxORtb8qHAaeCWrkjhkItbqZJx9Gm0kwA9t7E6j7eUQ6otcniXJWB5nXxeo2nRq6hAO%2BgcbsKUcbav8pVLp2riPp%2Bdr1lFwPYeLCMeqTzNLCn6%2B2ZaGqprSNBu%2BbSJHf6213l6ubbD3L90qHo8qxFV%2BvOh0t0hcnoU2615GD2pX9ji53Scn11xrsTXEXtX0GkLMmCKQw1EbLZcTvvTwNhzX5wQr0cNRQh%2Bd%2F0BwrsdtKDmFaiAsYhCve9HsmsycXK%2FGVj2amRbYdb%2FOrIzo8glK7%2B1EckXcD2VcYY9vN2LF%2FcUPKa3vNUMf7KWCZrs%2F3UqlJagmvg3DvsrcqqHssZqS4m982DqHb4wqfysA16AlHTZoxVQ4kzp7WtbhGeurbv86GOqdN3u6sOOyDDeFTd8sdq6vZpJd%2BfKLNUVW0fMOTQgsAGOrEBb5UJtoMLFmSkjIgnRASGY8LPLz%2F79t1nCQaj8FHd%2F4531QBxAl%2F%2B17tVI5%2B53189Yvf3qDww%2BQcwOUVUJcIbS0pWslTzn0I2u5ptHCeY25gBqHZ%2BOCMcUxoskiZTz8WEKe8zDagUCIS61csSqLfOQpn%2FoYAztKuZqKoLRCsy3cmz39R8SoGCC32UqnJhSvK81d8JRNvzYqcScCiYbRz8MyVpPUV4kpfQwnYkV54UWaCu&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T075406Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7AMCNL2K%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=412c33d6d11a8784f803d9e3819b64bf3e371118d7b700469200f02bb885a125&hash=446d9b71178609e40941a3c604cfc031fe468dee01d01319e356e3e58c07476c&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0165168424002317&tid=spdf-50c065e2-a499-4ae5-8757-1fa1da381470&sid=df1a433c23951642440ae8189ae06de94898gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57065a05540a&rr=931a5a3e1a8a0f2c&cc=cn">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{SU2024109612,title = {Collaborative spatial-temporal video salient object detection with cross attention transformer},journal = {Signal Processing},volume = {224},pages = {109612},year = {2024},issn = {0165-1684},doi = {https://doi.org/10.1016/j.sigpro.2024.109612},url = {https://www.sciencedirect.com/science/article/pii/S0165168424002317},author = {Yuting Su and Weikang Wang and Jing Liu and Peiguang Jing},keywords = {Video salient object detection, Siamese feature extractor, Deep level set, Vision transformer},abstract = {Noticing the effectiveness of explicit motion encoding in optical flow, a variety of recent works employ flow-guided two-branch structures to handle the video salient object detection (VSOD) task. However, most of them ignore the semantic gap between the moving objects and the salient objects. Besides, the long-range dependency is not sufficiently investigated due to the local convolution operation. To tackle these problems, in this paper, we propose a novel collaborative spatio-temporal feature fusion method based on the cross-attention transformer for VSOD. A Siamese feature extractor is introduced to jointly optimize the motion and static feature extraction with reduced network parameters. The deep level set method is employed to segment the moving objects from optical flow, and thus the dependence on saliency groundtruth is greatly reduced. Moreover, a cross-attention transformer is proposed to jointly optimize and fuse the static and motion features, as well as investigate long-range dependency. Experimental results on six commonly used video salient object datasets demonstrate that our method achieves state-of-the-art performance among all VSOD algorithms.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Huaiyan Jiang, Han Wang, T Pan, Y Liu, <b>P Jing</b>, Y Liu. Mobile Application and Machine Learning-Driven Scheme for Intelligent Diabetes Progression Analysis and Management Using Multiple Risk Factors. Bioengineering, 2024, 11(11): 1053.
                                    [<a href="https://www.mdpi.com/2306-5354/11/11/1053">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@Article{bioengineering11111053,AUTHOR = {Jiang, Huaiyan and Wang, Han and Pan, Ting and Liu, Yuhang and Jing, Peiguang and Liu, Yu},TITLE = {Mobile Application and Machine Learning-Driven Scheme for Intelligent Diabetes Progression Analysis and Management Using Multiple Risk Factors},JOURNAL = {Bioengineering},VOLUME = {11},YEAR = {2024},NUMBER = {11},ARTICLE-NUMBER = {1053},URL = {https://www.mdpi.com/2306-5354/11/11/1053},PubMedID = {39593713},ISSN = {2306-5354},ABSTRACT = {Diabetes mellitus is a chronic disease that affects over 500 million people worldwide, necessitating personalized health management programs for effective long-term control. Among the various biomarkers, glycated hemoglobin (HbA1c) is a crucial indicator for monitoring long-term blood glucose levels and assessing diabetes progression. This study introduces an innovative approach to diabetes management by integrating a mobile application and machine learning. We designed and implemented an intelligent application capable of collecting comprehensive data from diabetic patients, creating a novel diabetes dataset named DiabMini with 127 features of 88 instances, including medical information, personal information, and detailed nutrient intake and lifestyle. Leveraging the DiabMini, we focused the analysis on HbA1c dynamics due to their clinical significance in tracking diabetes progression. We developed a stacking model combining eXtreme Gradient Boosting (XGBoost), Support Vector Classifier (SVC), Extra Trees (ET), and K-Nearest Neighbors (KNN) to explore the impact of various influencing factors on HbA1c dynamics, which achieved a classification accuracy of 94.23%. Additionally, we applied SHapley Additive exPlanations (SHAP) to visualize the contributions of risk factors to HbA1c dynamics, thus clarifying the differential impacts of these factors on diabetes progression. In conclusion, this study demonstrates the potential of integrating mobile health applications with machine learning to enhance personalized diabetes management.},DOI = {10.3390/bioengineering11111053}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Fugui Fan, Yuting Su, Yun Liu*, <b>Peiguang Jing</b>, Kaihua Qu, Yu Liu. Multimodal deep hierarchical semantic-aligned matrix factorization method for micro-video multi-label classification. Information Processing & Management, 2024, 61(5): 103798.
                                    [<a href="https://pdf.sciencedirectassets.com/271647/1-s2.0-S0306457324X00035/1-s2.0-S0306457324001572/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIHRkeXEKS%2BYjn8nr6EPZhx4GfkFnlXw8IEQacBMsAx6eAiEA%2BY%2FevYnfIzGdAiw%2BzOmUzVyMVuDANlX7VST57mCIjVMqswUIWhAFGgwwNTkwMDM1NDY4NjUiDJ8IoV9IjuVT70%2FazCqQBWaUFzIWX8F5E18WPCrLvG%2BRAKldkOCkd%2FsWrSm0gJK7u0yHT3slRdv0gdubVdOP9NoZWOZVBZyx7YH8sTswu0lYvsP9PPhzdJgBSO06ef%2FE22%2F3HJdkk51l8T3TLdFXOmHx3Q8U9LOM12tpD2JJNnn1%2BmImpvUkNKljW%2FEJVf8aKckx7IAyFYVpPbXp07l0u43qOvlUWry36B7YlnBubefxen90%2Fp9G4t4SM9TL5xn%2FhNzc1Blvvh1XuFPut9PvvpQjdSeou4wyTb%2B2U23Yi4ib20wx3OoWEd0%2FqfZIg9G0RcFuYQoMXUGpmocQlAXxWWFIdMWpVIYRuA1e2ItTo20mP8fxv%2F09GK9WsNt1%2FuGdzMFivMQM%2Bw771l442sU1vb1o1HXRPpQg29QkvedGXg0rfR%2Bel99uGS%2FoH2Do22jYiaoWfSkP0SebY%2B6xuFEDYmrGsJ3hkt992vFfDj5kvurA130TW9XsKqwPtBwt3K0EzCFmqskbz16UxM8mOOnaok3750VLqKfm0SVaV1ZaVQIwj5o8uFLTNYfQ7S9oKK1YVFpFfSpPQOOw6USIo%2FwpP%2Byvv9iTzn7womcDpGkM3%2FZr9O9S6l%2BaC9SPG6waUx048Otm6LBZdH1xK87emKzGx5BHMsv8RKE8GoxJkzopRS0gzqk3S%2B62aP3HmTfOkG2ABN%2F%2BNBX1%2FcvG96PsBa0QXhaKrPvX4%2Fzk3fIf%2FCY6y55uydh%2BFoX%2F3J4nLMXefzirskEyEmPB3pVjc2IqasjExAnOtXNtymg6u%2BvogHSmrPFEAPQuY5Q1a84kLxHImpkSeJQVHoTAsSfcTyD1I6hMgWJwjmEM3Bht5mM2Bmp924W0143d2s1iHvgqZSRbTA7fMNX2gsAGOrEBEzvtD%2BeUMbgNF1Nb69RvHPvceGi6bwspUuLPDmrMRcy81%2F5axwSCEngJpJuPcIS6Y27BJFi%2BVqXCFUAd4VBQ4jfnCutDfxCyOmxD1AUk0sMjW5HlMpQr9RtHoPfsVT%2Bp5bcLxxhuNuBtfVEGtSRcsRaSfWSFPWIa32oHumd1SZeUaFFH5ZbYIBxs7D4KqNb%2F9TFGDC8YQHUgmOSIAUUjq9xnnNF0Ce3DwLvfk8q0ArWh&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T084616Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6IGWISCJ%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=2974e14487ac500149e3d7cd6d472ef46e95b15c0c52ab277837965e18b2bf0e&hash=eba5b4f808aec8d691ea93d05a95ef07d559b7c246051954c1ebaafdac8704f3&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0306457324001572&tid=spdf-9b3bfb43-a843-47a2-9d23-9024f23d03b5&sid=df1a433c23951642440ae8189ae06de94898gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57060e52065a&rr=931aa6a528dfe2dd&cc=cn">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{FAN2024103798,title = {Multimodal deep hierarchical semantic-aligned matrix factorization method for micro-video multi-label classification},journal = {Information Processing & Management}volume = {61},number = {5},pages = {103798},year = {2024},issn = {0306-4573},doi = {https://doi.org/10.1016/j.ipm.2024.103798},url = {https://www.sciencedirect.com/science/article/pii/S0306457324001572},author = {Fugui Fan and Yuting Su and Yun Liu and Peiguang Jing and Kaihua Qu and Yu Liu},keywords = {Micro-video understanding, Multi-label learning, Multimodal learning, Low-rank learning, Deep matrix factorization},abstract = {As one of the typical formats of prevalent user-generated content in social media platforms, micro-videos inherently incorporate multimodal characteristics associated with a group of label concepts. However, existing methods generally explore the consensus features aggregated from all modalities to train a final multi-label predictor, while overlooking fine-grained semantic dependencies between modality and label domains. To address this problem, we present a novel multimodal deep hierarchical semantic-aligned matrix factorization (DHSAMF) method, which is devoted to bridging the dual-domain semantic discrepancies and the inter-modal heterogeneity gap for solving the multi-label classification task of micro-videos. Specifically, we utilize deep matrix factorization to individually explore the hierarchical modality-specific representations. A series of semantic embeddings is introduced to facilitate latent semantic interactions between modality-specific representations and label features in a layerwise manner. To further improve the representation ability of each modality, we leverage underlying correlation structures among instances to adequately mine intra-modal complementary attributes, and maximize the inter-modal alignment by aggregating consensus attributes in an optimal permutation. The experimental results conducted on the MTSVRC and VidOR datasets have demonstrated that our DHSAMF outperforms other state-of-the-art methods by nearly 3% and 4% improvements in terms of the AP metric.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Jing Liu, Lele Sun, Weizhi Nie, <b>Peiguang Jing</b>, Yuting Su. Graph Disentangled Contrastive Learning with Personalized Transfer for Cross-Domain Recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(8): 8769-8777.
                                    [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28723">pdf</a>]              
                                </p>
                                </li>   


                                <li class="item"><p align="justify"> Wei Lu, Jiaxin Lin, <b>Peiguang Jing*</b>, Yuting Su. A Multimodal Aggregation Network With Serial Self-Attention Mechanism for Micro-Video Multi-Label Classification. IEEE Signal Processing Letters, 2023, 30: 60-64.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10032700">pdf</a>] 
                                    [<a href=" https://github.com/peiguangjing/MANET">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10032700,author={Lu, Wei and Lin, Jiaxin and Jing, Peiguang and Su, Yuting}, journal={IEEE Signal Processing Letters}, title={A Multimodal Aggregation Network With Serial Self-Attention Mechanism for Micro-Video Multi-Label Classification}, year={2023},volume={30}, number={},pages={60-64},keywords={Feature extraction;Visualization;Correlation;Trajectory;Task analysis;Mobile ad hoc networks;Measurement;Micro-video;multi-label classification;multimodal;self-attention},doi={10.1109/LSP.2023.3240889}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yuting Su, Wei Zhao, <b>Peiguang Jing*</b>, Liqiang Nie. Exploiting Low-rank Latent Gaussian Graphical Model Estimation for Visual Sentiment Distribution. IEEE Transactions on Multimedia, 2023, 25: 1243-1255
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9672699">pdf</a>] 
                                    [<a href="https://github.com/peiguangjing/LGGME">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9672699,author={Su, Yuting and Zhao, Wei and Jing, Peiguang and Nie, Liqiang},journal={IEEE Transactions on Multimedia}, title={Exploiting Low-Rank Latent Gaussian Graphical Model Estimation for Visual Sentiment Distributions}, year={2023},volume={25}, number={},pages={1243-1255},keywords={Correlation;Visualization;Graphical models;Covariance matrices;Multivariate regression;Estimation;Sentiment analysis;Gaussian graphical model;low-rank representation;visual sentiment analysis},doi={10.1109/TMM.2022.3140892}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Wei Lu, Desheng Li, Liqiang Nie, <b>Peiguang Jing*</b>, Yuting Su. Learning Dual Low-rank Representation for Multi-label Micro-video Classification. IEEE Transactions on Multimedia, 2023, 25:77-89
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9585369">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9585369,author={Lu, Wei and Li, Desheng and Nie, Liqiang and Jing, Peiguang and Su, Yuting}, journal={IEEE Transactions on Multimedia}, title={Learning Dual Low-Rank Representation for Multi-Label Micro-Video Classification}, year={2023},volume={25},number={},pages={77-89},keywords={Correlation;Matrix decomposition;Task analysis;Semantics;Dictionaries;Estimation;Multimedia Web sites;Micro-video;multi-label classification;multi-modality;low-rank representation},doi={10.1109/TMM.2021.3121567}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Xianyi Liu, Ji Wang, Yinwei Wei, Liqiang Nie, Yuting Su. StyleEDL: Style-Guided High-order Attention Network for Image Emotion Distribution Learning. In Proceedings of ACM International Conference on Multimedia. 2023: 853-861.
                                    [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612040">pdf</a>] 
                                    [<a href="  https://github.com/liuxianyi/StyleEDL">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@inproceedings{10.1145/3581783.3612040,author = {Jing, Peiguang and Liu, Xianyi and Wang, Ji and Wei, Yinwei and Nie, Liqiang and Su, Yuting},title = {StyleEDL: Style-Guided High-order Attention Network for Image Emotion Distribution Learning},year = {2023},isbn = {9798400701085},publisher = {Association for Computing Machinery},address = {New York, NY, USA},url = {https://doi.org/10.1145/3581783.3612040},doi = {10.1145/3581783.3612040},abstract = {Emotion distribution learning has gained increasing attention with the tendency to express emotions through images. As for emotion ambiguity arising from humans subjectivity, substantial previous methods generally focused on learning appropriate representations from the holistic or significant part of images. However, they rarely consider establishing connections with the stylistic information although it can lead to a better understanding of images. In this paper, we propose a style-guided high-order attention network for image emotion distribution learning termed StyleEDL, which interactively learns stylistic-aware representations of images by exploring the hierarchical stylistic information of visual contents. Specifically, we consider exploring the intra- and inter-layer correlations among GRAM-based stylistic representations, and meanwhile exploit an adversary-constrained high-order attention mechanism to capture potential interactions between subtle visual parts. In addition, we introduce a stylistic graph convolutional network to dynamically generate the content-dependent emotion representations to benefit the final emotion distribution learning. Extensive experiments conducted on several benchmark datasets demonstrate the effectiveness of our proposed StyleEDL compared to state-of-the-art methods. The implementation is released at: https://github.com/liuxianyi/StyleEDL.},booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},pages = {853–861},numpages = {9},keywords = {emotion distribution learning, high-order, style-guided, stylistic gcn},location = {Ottawa ON, Canada},series = {MM 23}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Wei Lu, Yang Jiang, <b>Peiguang Jing</b>, Jinghui Chu, Fugui Fan. A Novel Channel Pruning Approach based on Local Attention and Global Ranking for CNN Model Compression. In Proceedings of IEEE International Conference on Multimedia and Expo, 2023: 1433-1438.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10220022">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@INPROCEEDINGS{10220022, author={Lu, Wei and Jiang, Yang and Jing, Peiguang and Chu, Jinghui and Fan, Fugui},booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)}, title={A Novel Channel Pruning Approach based on Local Attention and Global Ranking for CNN Model Compression}, year={2023},volume={},number={},pages={1433-1438},keywords={Training;Image coding;Correlation;Convolutional neural networks;Noise measurement;Model Compression;Channel Pruning;Noisy Weight;Attention Mechanism},doi={10.1109/ICME55011.2023.00248}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yun Li, Shuyi Liu, Xuejun Wang, <b>Peiguang Jing*</b>. Self-supervised deep partial adversarial network for micro-video multimodal classification. Information Sciences, 2023, 630: 356-369.
                                    [<a href="https://pdf.sciencedirectassets.com/271625/1-s2.0-S0020025523X00073/1-s2.0-S0020025522014177/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEND%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIE3rFT7uGzZ3zAhxXoJQMRYblePPUhB89cO1xMr66dcjAiBHUFfHwD7MEgaMsA7wliJOY%2BiAVxvFBiO7Yg5AJ%2FJuMSqzBQhYEAUaDDA1OTAwMzU0Njg2NSIMSBlmIlL9ISltJyeuKpAFNNBLSdDuSEhNJXcy6Fqni5PwBAhSuAChVkfxUZLrfvuZcQmb7u0UMpdsg1d0Al3zN4pqdSfA2OVwwIpKAJJ0W2g27ShJT3Ciy%2BHB92zTqMzEsO0GYHMKaU8J%2BS4hzadRGVybG1p8%2FF6AfOijzStNcwUuHMdDR5ERtin6Han6UY3j7e7TsEcro6Ubx0pMDuvMoQNTqjzMTFhnADiizVCa7e3vKVI8gic8K8os6cVCHyGgWCeR7LReczesLtqWxcAta0HYvWpe0L6X6aK%2BNGqDVm8kAY4d8qo3MmyHIdF75ye0fGhvxhBRCL2s1Jo8bkGUmVtaeF91oeSIaFVLKDBPexOVq4VwhtXE%2F5ngSq62z0ozuDMscf3g%2BP0f04GRvqkdhvuPvNvuiGwhgan%2Bz91JB0vkjpy4ZODpjCJx%2Bm8WPQCa%2F4NH%2BmFsge0QLdlnCk9P8IZ2OidRble31UHPHpEspEbyRP9PgCLGxN9eZuNgHOlbbHjWAaeJDys49iXgWK9g92NtdFoun%2BqHL73vyNJ9ZoMpyRqt5ki%2Fu0lpyacW0ZyL4b3y5c%2BpVjhc4n7uUCw45jiHyu5pypcGbJG78GBWVx8kiZ%2B2i1QN8zgstC9MYbsNUQUAlxS%2FcFzIocX9wGvtbX2IGWYJjlkeQ9N3pV2K%2FgmVCp6lKm0k6sZKB8iG9I%2Bb8j0tGqpRJHzMiNCgXpvpA71U0JYq1Z6ZZ%2FAalXtYsw9ZfOpQNFIcLmXg0Au5Rrcu4yA4M0shQXjyweV2Y992SPwymwilCDFo2vDF57R6dbuZHngZhoLpMWlqjuICV9MzTy7FsLBuMzCoBWKe4iZ%2BizXy%2F3xpSZ0MZ3IPGBiJoW8JwejKNlx%2FqffUMmSwC7Ywr9eCwAY6sgGV8ybzVfsG98jjbddINqkRqPAWY0GBmEDR6njKKTkiRcdUM233o%2B1uK90OniAL8EHg9R%2BmOdt73UgHgyo4W%2BEsSsvYfaxR%2FAy%2Bll5KiTs4sb0fsxIbF6h0ywj0QXyVUt%2Fb15d0QfyhYK1WxZmUKIZsZlZl%2FqQQbYnKUUnwav0S6WNc0gtKSZbp%2BejqBL5i3mjm3rxMVQF%2FVGQrlnVyVFSheP2Rp8nfwFJ7%2BDNsNDlGjxh%2B&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T083420Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYTS5KHDKB%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=64263521b5a40c8b8c29c771e84581589b9b53e51b5ede8def6dc3a05cdfbff2&hash=cdbb43ef129f030256be725fa54650751f7b9b8aaf8f67e4031d6174dc07ec0e&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0020025522014177&tid=spdf-f89fecc9-0276-494a-a794-b01d6bef3ca0&sid=df1a433c23951642440ae8189ae06de94898gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57065651550b&rr=931a952de836b852&cc=cn&kca=eyJrZXkiOiJNMzB0UFdISzlidmtVWGpBZTdxRWVzQUZPOHFqd2QwNnpaQ3lBaGk2UWpKL2I2b0Z0c2h0RDg0S0xVaXc1ZlNOUEVycFBraXUvMmhqUXlLTTNiSWxmZHNXelZlclh3YWhJY1crcDFxRjJCWDh0YzRRODNjYWxXck5veHpENHdWV0tmcVhxa1ptd0FVeHFFTWxZRTVIU09UYncydHVpTU5sUTgweEhLVy9nbHdZWXAxayIsIml2IjoiZWI2ZGMxYzdiYTRiM2QwMTMwYTc0ZDlmNzE3ZTc1MmEifQ==_1744878866938">pdf</a>] 
                                    [<a href=" https://github.com/peiguangjing/SDMAN">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@article{LI2023356,title = {Self-supervised deep partial adversarial network for micro-video multimodal classification},journal = {Information Sciences},volume = {630},pages = {356-369},year = {2023},issn = {0020-0255},doi = {https://doi.org/10.1016/j.ins.2022.11.111},url = {https://www.sciencedirect.com/science/article/pii/S0020025522014177},author = {Yun Li and Shuyi Liu and Xuejun Wang and Peiguang Jing},keywords = {Micro-video classification, Multimodal representation, Self-supervised learning},abstract = {Micro-videos have gained popularity on various social media platforms because they provide a great medium for real-time storytelling. Although micro-videos can be naturally characterized by several modalities, for situations with uncertain missing modalities, a flexible multimodal representation learning framework that integrates complementary and consistent information has been difficult to develop. To better deal with the issue regarding incomplete modalities in multimodal micro-video classification, in this paper, we propose a self-supervised deep multimodal adversarial network (SDMAN) to learn comprehensive and robust micro-video representations. Specifically, we first consider a parallel multi-head attention (MHA) encoding module that simultaneously learns the representations of complete and incomplete modality groupings. We then present a multimodal self-supervised cycle generative adversarial network module, in which multiple generative adversarial networks are explored to transfer the information obtained from the complete modality grouping to the incomplete modality groupings. As a result, complementarity and consistency are mutually promoted among the modalities. Furthermore, experiments conducted on a large-scale micro-video dataset demonstrate that the SDMAN performs better than the state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yunpeng Yin, Han Wang, Shuai Liu, <b>Peiguang Jing</b>, Yu Liu. Internet of Things for Diagnosis of Alzheimer’s Disease: A Multimodal Machine Learning Approach Based on Eye Movement Features. IEEE Internet of Things Journal, 2023,10(13): 11476-11485
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10044924">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10044924,author={Yin, Yunpeng and Wang, Han and Liu, Shuai and Sun, Jinglin and Jing, Peiguang and Liu, Yu},journal={IEEE Internet of Things Journal}, title={Internet of Things for Diagnosis of Alzheimer’s Disease: A Multimodal Machine Learning Approach Based on Eye Movement Features}, year={2023}, volume={10},number={13},pages={11476-11485},keywords={Feature extraction;Internet of Things;Diseases;Cloud computing;Sensitivity;Task analysis;Gaze tracking;Alzheimer’s disease (AD);eye tracking;feature fusion;Internet of Things (IoT);multimodal machine learning (MMML);oculomotor behavior},doi={10.1109/JIOT.2023.3245067}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Jinglin Sun, Zhipeng Wu, Han Wang, <b>Peiguang Jing</b> and Yu. Liu. A Novel Integrated Eye-Tracking System With Stereo Stimuli for 3-D Gaze Estimation, IEEE Transactions on Instrumentation and Measurement, vol. 72, pp. 1-15, 2023,
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9964289">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9964289,author={Sun, Jinglin and Wu, Zhipeng and Wang, Han and Jing, Peiguang and Liu, Yu}, journal={IEEE Transactions on Instrumentation and Measurement}, title={A Novel Integrated Eye-Tracking System With Stereo Stimuli for 3-D Gaze Estimation}, year={2023}, volume={72},number={},pages={1-15},keywords={Three-dimensional displays;Gaze tracking;Estimation;Cameras;Tracking;Head;Glass;Eye-tracking;gaze estimation;integrated system;multisource feature;stereo imaging},doi={10.1109/TIM.2022.3225009}}')">bib</a>]             
                                </p>
                                </li>


                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Jing Zhang, Liqiang Nie, Shu Ye, Jing Liu, Yuting Su*. Tripartite Graph Regularized Latent Low-Rank Representation for Fashion Compatibility Prediction. IEEE Transactions on Multimedia, 2022, 24: 1277-1287. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9367009">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9367009,author={Jing, Peiguang and Zhang, Jing and Nie, Liqiang and Ye, Shu and Liu, Jing and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={Tripartite Graph Regularized Latent Low-Rank Representation for Fashion Compatibility Prediction}, year={2022},volume={24},number={},pages={1277-1287},keywords={Feature extraction;Task analysis;Correlation;Semantics;Clothing;Visualization;Industries;Correlation;fashion compatibility;graph regularization;latent low-rank representation},doi={10.1109/TMM.2021.3062736}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Jinglin Sun, Yu Liu, Hao Wu, <b>Peiguang Jing*</b>, Yong Ji. A novel deep learning approach for diagnosing Alzheimer's disease based on eye-tracking data. Frontiers in Human Neuroscience, 2022,16: 01-12 
                                    [<a href="https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.972773/full ">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10.3389/fnhum.2022.972773, AUTHOR={Sun, Jinglin  and Liu, Yu  and Wu, Hao  and Jing, Peiguang  and Ji, Yong },TITLE={A novel deep learning approach for diagnosing Alzheimers disease based on eye-tracking data}, JOURNAL={Frontiers in Human Neuroscience},    VOLUME={Volume 16 - 2022},YEAR={2022},URL={https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.972773},DOI={10.3389/fnhum.2022.972773},ISSN={1662-5161},ABSTRACT={<p>Eye-tracking technology has become a powerful tool for biomedical-related applications due to its simplicity of operation and low requirements on patient language skills. This study aims to use the machine-learning models and deep-learning networks to identify key features of eye movements in Alzheimers Disease (AD) under specific visual tasks, thereby facilitating computer-aided diagnosis of AD. Firstly, a three-dimensional (3D) visuospatial memory task is designed to provide participants with visual stimuli while their eye-movement data are recorded and used to build an eye-tracking dataset. Then, we propose a novel deep-learning-based model for identifying patients with Alzheimers Disease (PwAD) and healthy controls (HCs) based on the collected eye-movement data. The proposed model utilizes a nested autoencoder network to extract the eye-movement features from the generated fixation heatmaps and a weight adaptive network layer for the feature fusion, which can preserve as much useful information as possible for the final binary classification. To fully verify the performance of the proposed model, we also design two types of models based on traditional machine-learning and typical deep-learning for comparison. Furthermore, we have also done ablation experiments to verify the effectiveness of each module of the proposed network. Finally, these models are evaluated by four-fold cross-validation on the built eye-tracking dataset. The proposed model shows 85% average accuracy in AD recognition, outperforming machine-learning methods and other typical deep-learning networks.</p>}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Yuechen Shang, Liqiang Nie, Yuting Su*, Jing Liu, Meng Wang. Learning Low-rank Sparse Representations with Robust Relationship Inference for Image Memorability Prediction. IEEE Transactions on Multimedia, 2021, 23: 2259-2272 (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9141424">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9141424,author={Jing, Peiguang and Shang, Yuechen and Nie, Liqiang and Su, Yuting and Liu, Jing and Wang, Meng}, journal={IEEE Transactions on Multimedia}, title={Learning Low-Rank Sparse Representations With Robust Relationship Inference for Image Memorability Prediction}, year={2021},volume={23},number={},pages={2259-2272},keywords={Visualization;Robustness;Sparse matrices;Correlation;Predictive models;Task analysis;Adaptation models;Image memorability prediction;low-rank;sparse;relationship structure},doi={10.1109/TMM.2020.3009485}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Yuting Su*, Zhengnan Li, Liqiang Nie. Learning robust affinity graph representation for multi-view clustering. Information Sciences, 2021 544: 155-167. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://pdf.sciencedirectassets.com/271625/1-s2.0-S0020025520X00296/1-s2.0-S0020025520306575/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIDuJzN8UMX%2BkkoeaeyxodFOT%2BIfcToWWbswd2ueNsPNwAiBJRkzcg17CDTlUOBBNgNMtChdlQJ6nEEcg4BR6aw%2BAxCqzBQhcEAUaDDA1OTAwMzU0Njg2NSIMt4sE9JrKubhIANmVKpAFq74qhfMTknoS0%2F8jpisS6zKa%2BnEhIdauK%2Bh7C7JanVRLslMWq33BQOv%2Ffq4ZCOZNqrc7WBbc1QQ%2BFp%2Fb%2FL8xCjK3I6YjZiUQrZAvZaqi%2FSzVIRe7nTIVZ8gr1b2MqJrBlb2jYBcvDdZc%2FGLFedp7Oob1T1RxqzBysdMPBXWC%2FbiIPpR8W8GrTaxAVECZOL7DumunYxLEvxwL9p%2BRZL331gvK3M2gQwwtHstWRoE7xXNl%2FP0ZySettW17dQXGYvbfQ%2B8twq9LC3rS30dGogX5q4egth3BLpIIf3%2Bf90FyiRpofHjhiF3DqI%2BGum16KqFdO4fTXXXHFqQZkOfmY9T4x2ok2xKIQGwEQyUvLwN8bhGoI0hcq4e%2BNtuNh7DXbXwhSxSyS3F8n7FbAkjKnWC%2BTld5%2FudD0xuNtF3YNychG5HO%2B1XyHyZuBzfaIc1jGZx7RlRo1Th%2FPVQU5w9HlR0Xg4%2Fiy%2F9CVOn0PnE5ecofKJmbGZ3Pj0IY4X2yp5QUoJRUG63%2BLXnl8Kw4uACO7QacBHTXHxNOhwCLYcTFAGlEVVIryztyt4Eissz%2B2qQUkmTr31kPtt%2BYxW5zWsJU4DS1EXcB%2BmtBpwEQi0wxvq0BzIDFXkIX5lfXjj5lQzJO65t9c%2BBP7IuliGNtP%2Fcf%2BtmR%2F6mtSgyMsbIV4hfjxh7reZx2hSHz%2F2MR12HTiy%2BiSKwomRl3Bjl1tbtWM9RT4TepmR8LSZiyvR8bZyBQkOhETmIK8eZBioAzSBTfwAbgMyu1Y7DHxgo5dsLLd2k2gY%2FmGah8uVQJcPcbV0ZM%2BjUkMTXkIykCgXmPvuHXVgNQgP195IT48GNCgFtz1QR2OuRox3z4gtjpDkiF5ng8TaQ%2FtqYwma2DwAY6sgHQyNhF5IDg1xQTfx80x9DdiMytomPm2gL0G9SUDTKLvTi%2BKzhWM%2FOqNhw1z8M%2FvQKOqVVH5XuVjBCKUjxflmXkSJuGP8gPYmvaEmdAgB4sZqOl2aPjJGYYbjdBNk06XfN4TiWyLNGrmxa2VOEA%2FIW1gqmA9mkaxnxYZs9bFIFkvxq2ZKk%2F8zxy0wI5195UzR5khP26xQAY9z8jlDy5DCWvVLQpiG1tqfg9%2BXI4x4pWO25H&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T104045Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY5QZGX7PU%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0df94fbf1593d03c18ec03d4606c192b2f9d5c94d67515b2325b419fa9d0ce24&hash=ce91e4d921908aeadc50e052f267ee3a01402f180da0f7ee23a2e4a6997cd75e&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0020025520306575&tid=spdf-19dae603-0ee3-4c70-9baf-5d493b851468&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57055b015257&rr=931b4e58cb26e2f7&cc=cn&kca=eyJrZXkiOiIxZHgxTVFRUTJGV1lVQmtWb0pMVnBTbUhydHRYVWhadFFMSysvYVhnRFk4WUxoU3hTMldJSXowUTBwMVE4cVg0TjJVbHFpWHdIOHpYemJoSGJnZExKSFpkYk1VeEx2VXhjZHpDM28vQUZ4aVNhVU92RWNlcklxcTBmZXVBb1pFWGhtemdIOUs4eHdPZXlnMk1UU0dBTHdWY04zUnhnTEZjd3FDeUZTcnZ3bU1uS3BBPSIsIml2IjoiM2I1MzBiMGMzOGY1NWY2MzEzNDZlMzc1ZWNlN2U0ZWQifQ==_1744886449846">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{JING2021155,title = {Learning robust affinity graph representation for multi-view clustering},journal = {Information Sciences},volume = {544},pages = {155-167},year = {2021},issn = {0020-0255},doi = {https://doi.org/10.1016/j.ins.2020.06.068},url = {https://www.sciencedirect.com/science/article/pii/S0020025520306575},author = {Peiguang Jing and Yuting Su and Zhengnan Li and Liqiang Nie},keywords = {Multi-view clustering, Feature selection, Graph representation, Grassmann manifold},abstract = {Recently, an increasingly pervasive trend in real-word applications is that the data are collected from multiple sources or represented by multiple views. Owing to the powerful ability of affinity graph in capturing the structural relationships among samples, constructing a robust and meaningful affinity graph has been extensively studied, especially in spectral clustering tasks. However, conventional spectral clustering extended to multi-view scenarios cannot obtain the satisfactory performance due to the presence of noise and the heterogeneity among different views. In this paper, we propose a robust affinity graph learning framework to deal with this issue. First, we employ an improved feature selection algorithm that integrates the advantages of hypergraph embedding and sparse regression to select significant features such that more robust graph Laplacian matrices for various views on this basis can be constructed. Second, we model hypergraph Laplacians as points on a Grassmann manifold and propose a Consistent Affinity Graph Learning (CAGL) algorithm to fuse all views. CAGL aims to learn a latent common affinity matrix shared by all Laplacian matrices by taking both the clustering quality evaluation criterion and the view consistency loss into account. We also develop an alternating descent algorithm to optimize the objective function of CAGL. Experiments on five publicly available datasets demonstrate that our proposed method obtains promising results compared with state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yuting Su, Junyu Xu, Daozheng Hong, Fugui Fan, Jing Zhang*, <b>Peiguang Jing</b>. Deep low-rank matrix factorization with latent correlation estimation for micro-video multi-label classification. Information Sciences, 2021, 575: 587-598.
                                    [<a href="https://pdf.sciencedirectassets.com/271625/1-s2.0-S0020025521X00217/1-s2.0-S0020025521007118/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIGH3Mg3rrYvVf%2BbX062HR6vHqEH76XU%2FpDI%2Bk0olE6RPAiAYmehfd%2BEswZ6g4YJQcmgnNjUuBObgjlpD4tNcfnl%2BPSqzBQhbEAUaDDA1OTAwMzU0Njg2NSIMGj0wTFnXElCJPMJAKpAFdgvIfZcj8d9LMcDQgSvaaLNawFEwWebInf2nKLnqswdVQbUMNawe0wskYKySivy3BN4QPVmY69jXdJWsbzuDY6esBH8aFE%2BET%2FjtF8FYjPDxAnKFEggfpDnAJx1JLxk65TbVIm4YPPUtgX%2BTRl1y3YlKja0Yn6iWOfgPkomsgk1pabYXRCKU%2FFOMdLrtOUyetrOpllPMW9K7947mpCiG4EHaNO8Dw4vt7UQAR1xDzpErnQNaTzqYOQxV9RE4oSSHI2U49rgaZAQmdRUZaTErSLf6AP%2BXtRZDQ%2FfqVlmhQkTtRA41i5Guu2tv7UkE4sc7plsshY30DMh2iwl4YnmSZkQLT%2B1OR5UZx8m81a3K81mIXqm21K6gqdNFGfeQhwko5Fy642485D%2Fn9zWAAa5z90tHCAo7MRCAghElUmI%2B3XNVL0FH7SapuRUraOExWFxTWp97LaqBoAce63Q1kLKL2lbCLNRZetA%2FXsRYZcVVuntSTF0CDbkCtgja0c1Z8o%2BQHBWMilNsc4BMmFnmX98ABX5AxdW7P4%2Bic0eIY4VIZJxt0QQHP%2Bc%2FaosL%2FOhhQQUNXV8L1lMTRNRwqWMGbJA1K8GTCBxHbp9iW1bBaqrh93iuqD0PbPyu5%2BQfvfLVGLuUJVxOIKEbZqXkevbE3ytR1iYWyYMAhd7jn7Mm5B2WoDtca1cue3Veu%2BnqkO2bnMjGSOrCgVP6TEA9qX2HG0xf9XDcW4xs3vS6wwWCxyO8iLME9G%2FvPSFGvGZ7IWdx0TcUZObufLwLt2mpVZoRoTFbUtVq%2BldUW6UYJi6VknwnG4itauehlTbUiL8ozDLWNzB67glEQGtXfz%2Fcg9PHxkZFa7Zvj%2Bi1pnFEFetyqJCJVl4wzJqDwAY6sgGSV3t9aC9jqfSoy6BllsZXbnntI5kFkUi0M2eS6pI6jwwONhMvlotq1DF2Q7cH1MSB8ccOp6qoRz5c3ekoqa7V4t3W%2FSecLzwKzzj5E4EnU3cU5qGN4b0n%2F2TfuPqgK0lw%2BBvnyM0%2FpRv8shCtn51qvNVSfQWH%2BtGpDkTtBtalEDl7UYC7PJwtijvWuahpiXcc%2F%2Bm4mbnSg7IFqFyLT9xIYPQ8kDBbi4uabvoAy3dsW0Gs&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T104343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6MRHU7B7%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=a65eb6e3c0446fadcc6032249c68800b10a2706137164761b47aac528a95607a&hash=5ae0fb4740dda58252e041a35dde9bc4ebe48d58c3540c3b351a6df3b9b48bb2&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0020025521007118&tid=spdf-002b5cf0-0500-44c1-8612-316087e1276e&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57055a56055d&rr=931b52b2f950e2f7&cc=cn">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{SU2021587,title = {Deep low-rank matrix factorization with latent correlation estimation for micro-video multi-label classification},journal = {Information Sciences},volume = {575},pages = {587-598},year = {2021},issn = {0020-0255},doi = {https://doi.org/10.1016/j.ins.2021.07.021},url = {https://www.sciencedirect.com/science/article/pii/S0020025521007118},author = {Yuting Su and Junyu Xu and Daozheng Hong and Fugui Fan and Jing Zhang and Peiguang Jing},keywords = {Deep matrix factorization, Micro-video, Multi-label learning, Label correlation, Low-rank constraint},abstract = {Currently, micro-videos are becoming an increasingly prevailing form of user-generated contents (UGCs) on various social platforms. Several studies have been conducted to explore the semantics of micro-videos and the behavior of individuals for various tasks, such as venue categorization, popularity prediction, and personalized recommendation. However, few studies have been dedicated to solving micro-video multi-label classification. More importantly, learning intrinsic and robust feature representations for micro-videos is still a complicated and challenging problem. In this paper, we propose a deep matrix factorization with latent correlation estimation (DMFLCE) for micro-video multi-label classification. In DMFLCE, we develop a deep matrix factorization component constrained by a low-rank constraint to learn the lowest-rank representations for micro-videos and the intrinsic characterizations for latent attributes simultaneously. To explicitly exhibit the dependencies of the learned latent attributes and labels for improved classification performance, we construct two inverse covariance estimation components to automatically encode correlation patterns with respect to the latent attributes and labels. Experiments conducted on a publicly available large-scale micro-video dataset demonstrate the effectiveness of our proposed method compared with state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Jing Liu, Xin Wen, Weizhi Nie, Yuting Su, <b>Peiguang Jing</b>, Xiaokang Yang. Residual-guided multiscale fusion network for bit-depth enhancement. IEEE Transactions on Circuits and Systems for Video Technology, 2021, 32(5): 2773-2786.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9491068">pdf</a>] 
                                    [<a href=" https://github.com/TJUMMG/RMFNet">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9491068,author={Liu, Jing and Wen, Xin and Nie, Weizhi and Su, Yuting and Jing, Peiguang and Yang, Xiaokang},journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={Residual-Guided Multiscale Fusion Network for Bit-Depth Enhancement},  year={2022},volume={32},number={5},pages={2773-2786}, keywords={Radio frequency;Feature extraction;Task analysis;Image edge detection;Image restoration;Noise measurement;Spatial resolution;Bit-depth enhancement;multiscale architecture;shuffling operations;residual guidance;feature fusion},doi={10.1109/TCSVT.2021.3098707}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Fugui Fan, Yuting Su, <b>Peiguang Jing</b>, Wei Lu*. A dual rank-constrained filter pruning approach for convolutional neural networks. IEEE Signal Processing Letters, 2021, 28: 1734-1738.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9506816">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9506816,author={Fan, Fugui and Su, Yuting and Jing, Peiguang and Lu, Wei},journal={IEEE Signal Processing Letters}, title={A Dual Rank-Constrained Filter Pruning Approach for Convolutional Neural Networks}, year={2021},volume={28},number={},pages={1734-1738},keywords={Manifolds;Adaptation models;Correlation;Adaptive systems;Adaptive filters;Computer architecture;Information filters;Filter pruning;Grassmann manifold;adaptive affinity graph;low-rank;high-rank},doi={10.1109/LSP.2021.3101670}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Xiao Jin, <b>Peiguang Jing</b>, Jiesheng Wu, Jing Xu, Yuting Su. Visual Sentiment Classification via Low-rank Regularization and Label Relaxation. IEEE Transactions on Cognitive and Developmental Systems, 2021, 14 (4): 1678-1690.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9658319">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9658319,author={Jin, Xiao and Jing, Peiguang and Wu, Jiesheng and Xu, Jing and Su, Yuting},journal={IEEE Transactions on Cognitive and Developmental Systems}, title={Visual Sentiment Classification via Low-Rank Regularization and Label Relaxation}, year={2022},volume={14},number={4},pages={1678-1690}, keywords={Visualization;Sparse matrices;Feature extraction;Machine learning;Dictionaries;Learning systems;Classification algorithms;Sentiment analysis;Dictionary learning;low-rank regularization;subspace learning;visual sentiment classification},doi={10.1109/TCDS.2021.3135948}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Jinghui Chu, Jiawei Feng, <b>Peiguang Jing</b>, Wei Lu. Joint Co-Attention And Co-Reconstruction Representation Learning For One-Shot Object Detection. In Proceedings of IEEE International Conference on Image Processing, 2021: 2229-2233.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9506387">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@INPROCEEDINGS{9506387,author={Chu, Jinghui and Feng, Jiawei and Jing, Peiguang and Lu, Wei},booktitle={2021 IEEE International Conference on Image Processing (ICIP)}, title={Joint Co-Attention And Co-Reconstruction Representation Learning For One-Shot Object Detection}, year={2021},volume={},number={},pages={2229-2233},keywords={Training;Degradation;Correlation;Conferences;Object detection;Feature extraction;Proposals;Object detection;one-shot learning;high-order feature fusion;low-rank co-reconstruction},doi={10.1109/ICIP42928.2021.9506387}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Wenjie Wang, Lingyu Duan, Hao Jiang, <b>Peiguang Jing</b>, Xuemeng Song, Liqiang Nie. Market2Dish: health-aware food recommendation. ACM Transactions on Multimedia Computing, Communications, and Applications, 2021, 17(1): 1-19.
                                    [<a href="https://dl.acm.org/doi/pdf/10.1145/3418211">pdf</a>] 
                                    [<a href=" https://github.com/WenjieWWJ/FoodRec">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@article{10.1145/3418211,author = {Wang, Wenjie and Duan, Ling-Yu and Jiang, Hao and Jing, Peiguang and Song, Xuemeng and Nie, Liqiang},title = {Market2Dish: Health-aware Food Recommendation},year = {2021},issue_date = {February 2021},publisher = {Association for Computing Machinery},address = {New York, NY, USA},volume = {17},number = {1},issn = {1551-6857},url = {https://doi.org/10.1145/3418211},doi = {10.1145/3418211},abstract = {With the rising incidence of some diseases, such as obesity and diabetes, the healthy diet is arousing increasing attention. However, most existing food-related research efforts focus on recipe retrieval, user-preference-based food recommendation, cooking assistance, or the nutrition and calorie estimation of dishes, ignoring the personalized health-aware food recommendation. Therefore, in this work, we present a personalized health-aware food recommendation scheme, namely, Market2Dish, mapping the ingredients displayed in the market to the healthy dishes eaten at home. The proposed scheme comprises three components, namely, recipe retrieval, user health profiling, and health-aware food recommendation. In particular, recipe retrieval aims to acquire the ingredients available to the users and then retrieve recipe candidates from a large-scale recipe dataset. User health profiling is to characterize the health conditions of users by capturing the textual health-related information crawled from social networks. Specifically, to solve the issue that the health-related information is extremely sparse, we incorporate a word-class interaction mechanism into the proposed deep model to learn the fine-grained correlations between the textual tweets and pre-defined health concepts. For the health-aware food recommendation, we present a novel category-aware hierarchical memory network–based recommender to learn the health-aware user-recipe interactions for better food recommendation. Moreover, extensive experiments demonstrate the effectiveness of the health-aware food recommendation scheme.},journal = {ACM Trans. Multimedia Comput. Commun. Appl.},month = apr,articleno = {33},numpages = {19},keywords = {User health profiling, health-aware food recommendation, recipe retrieval}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Shu Ye, Liqiang Nie, Jing Liu, Yuting Su*. Low-rank regularized multi-representation learning for fashion compatibility prediction. IEEE Transactions on Multimedia, 2020, 22(6): 1555-1566. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8853293">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8853293,author={Jing, Peiguang and Ye, Shu and Nie, Liqiang and Liu, Jing and Su, Yuting}, journal={IEEE Transactions on Multimedia}, title={Low-Rank Regularized Multi-Representation Learning for Fashion Compatibility Prediction}, year={2020},volume={22},number={6},pages={1555-1566},keywords={Sparse matrices;Matrix decomposition;Clothing;Feature extraction;Task analysis;Manifolds;Visualization;Image understanding;fashion compatibility;low-rank constraint;sparse representation;subspace learning},doi={10.1109/TMM.2019.2944749}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yuting Su, Daozheng Hong, Yang Li, <b>Peiguang Jing*</b>. Low-Rank Regularized Deep Collaborative Matrix Factorization for Micro-Video Multi-Label Classification. IEEE Signal Processing Letters, 2020, 27: 740-744. 
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9050869">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9050869,author={Su, Yuting and Hong, Daozheng and Li, Yang and Jing, Peiguang}, journal={IEEE Signal Processing Letters}, title={Low-Rank Regularized Deep Collaborative Matrix Factorization for Micro-Video Multi-Label Classification}, year={2020},volume={27},number={},pages={740-744}, keywords={Matrix decomposition;Feature extraction;Collaboration;Covariance matrices;Task analysis;Correlation;Semantics;Micro-video;multi-label classification;deep matrix factorization;low-rank},doi={10.1109/LSP.2020.2983831}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Yuting Su*, Xiao Jin, Chengqian Zhang. High-Order Temporal Correlation Model Learning for Time-Series Prediction. IEEE Transactions on Cybernetics, 2019, 49(6): 2385-2397.(<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8359393">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8359393,author={Jing, Peiguang and Su, Yuting and Jin, Xiao and Zhang, Chengqian}, journal={IEEE Transactions on Cybernetics}, title={High-Order Temporal Correlation Model Learning for Time-Series Prediction}, year={2019},volume={49},number={6},pages={2385-2397},keywords={Tensile stress;Predictive models;Hidden Markov models;Data models;Time series analysis;Correlation;Analytical models;Autoregressive (AR);temporal correlation;tensor decomposition;time-series prediction},doi={10.1109/TCYB.2018.2832085}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Wei Lu, Fugui Fan, Jinghui Chu, <b>Peiguang Jing*</b>, Yuting Su. Wearable Computing for Internet of Things: A Discriminant Approach for Human Activity Recognition. IEEE Internet of Things Journal, 2019, 6(2): 2749-2759. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8480641">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8480641,author={Lu, Wei and Fan, Fugui and Chu, Jinghui and Jing, Peiguang and Yuting, Su}, journal={IEEE Internet of Things Journal}, title={Wearable Computing for Internet of Things: A Discriminant Approach for Human Activity Recognition}, year={2019},volume={6},number={2},pages={2749-2759},keywords={Feature extraction;Accelerometers;Internet of Things;Activity recognition;Medical services;Data mining;Transforms;Dimensionality reduction;human activity recognition (HAR);Internet of Things (IoT);S transform (ST);wearable computing},doi={10.1109/JIOT.2018.2873594}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Yuting Su, Liqiang Nie, Humin Gu, Jing Liu*, Meng Wang. A Framework of Joint Low-rank and Sparse Regression for Image Memorability Prediction. IEEE Transactions on Circuits and Systems for Video Technology, 2019,29(5): 1296-1309. (<i><strong>ESI高被引</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8353302">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8353302,author={Jing, Peiguang and Su, Yuting and Nie, Liqiang and Gu, Huimin and Liu, Jing and Wang, Meng}, journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={A Framework of Joint Low-Rank and Sparse Regression for Image Memorability Prediction}, year={2019},volume={29},number={5},pages={1296-1309},keywords={Sparse matrices;Visualization;Robustness;Matrix decomposition;Task analysis;Approximation algorithms;Heuristic algorithms;Image memorability prediction;low-rank;sparse regression;subspace learning},doi={10.1109/TCSVT.2018.2832095}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Yuting Su, Zhengnan Li, Jing Liu, Liqiang Nie. Low-rank regularized tensor discriminant representation for image set classification. Signal Processing, 2019, 156: 62-70.
                                    [<a href="https://pdf.sciencedirectassets.com/271605/1-s2.0-S0165168418X00115/1-s2.0-S0165168418303505/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQC8Cm5u9w1SFohjM6VkImmuvyUAT%2F3D0ECB%2FbAZHG8DvAIhAJ7kHay5lVmWKDfTbKiWTX6mPlgbz3OnXLr68QyRNfZnKrMFCFsQBRoMMDU5MDAzNTQ2ODY1Igwkmn3ZyxbWePRHfUgqkAVsILY3S2T4WcYi7gsAuTSVNTu58TUmSBGLEtC97oWTaYfu4NOuF9Sms6e9E1Ke%2FtpI5asyqT%2B0j5Mjf3crhVPRjs5HAGdkCfHd%2BcniWcCQ0e4cGkJhoIsvzZAIbW5sSbPpayGjsWZ653W%2B8yA5E2oIDKxHuIQ1ecuafGCyFkP%2BRCa6jYZxcC7SFElt7IcwGy%2FzmZQ%2FTjVtVFQNLCN3Y8LRxQs7IayXm%2FvBRPnDfu2amF0YsP2f6%2FxPtBKoXi%2BsRnxS%2BpPYGT1NPVDeDnczv5wnVCGK3%2BfpOseW10GV9Fudb3%2Ba18IElvolBVPXcylwuG3obpIjsS1ZxWo4h9tWBt4r%2BC7BPNO1nmhbcT1VA1WRplJgudSUcQIp0wiErrFhtENdJSeyyJCdvCznI7xHL1hD6NX3l4VzOvuz7I7U7uOJ5xUIrKSevFuk9FmhIH0bMk5GTWjBQmDHdljS7u%2FIyO4Rk9QeT%2BRYbnxYK1oo1ZKwvJaa6eBP7G2GDcVEKFVVX0JyJQx%2FqboEsy5P8j%2FYRNzwfh0hmekAsOhmQMIYbgVDCh%2BgtnydiIKxVPeesfsCSKJOBj71XHT52GYe5i7kUQZf88BqnA55XcqbNULqPdBxoM1cUmY%2BW2cUkzbJctA6MXCc3PSotA6y7eNcIWJPEA%2B8WciWpEbegu0wvyDJJQN61u34Zk3ERMonZqTa0uW9RaYLc00JtViL1m%2BRO%2BE0%2F3RU35gmcW%2FxM6zkDP7iagJwhuzvEI6gYPL2H9NGi%2Fg3C%2FD5aUFaOfpjMl0aCxGhOPfPeM4GkjU54YLbTfNeXlRPtkD295x%2FFRcSCXQkLcmRyJ7kLFnwk887RsMqK5%2BBFMtu3j9tv5pNnAqPwcJyWPJvSTD0noPABjqwAeRsJkELHdS7XRdmJjIN7sOT7w6oPHb9p33Fb822WidaCpziEZ18d6QM4NYuU%2BqswPn0RaTPsTLrU6wBhhkaGegEmPP3QVy17x4saJ5hz8ZTbrWruBw3HbZ2BHzPStduowZW1MFS0airPBh1HfnO67VQTPv2dXzO%2F981%2BIMoIJQSj2%2FdE3Z%2FdrIih%2BlKWwIfKoiEUmebGSz6IHgnJ66IS%2FhW4VS264HI6BIdg0%2FcYkJv&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T111427Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7HDTEJ6F%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=432ea077112a1c29e5c7fae0c026fcb802483d760df6ee3857bfa58e66dc2162&hash=0afb427a628043b2bdd455cb07aa33ee46caa5ed20ecd8bbfd4fbbf6c63f654e&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0165168418303505&tid=spdf-09e2ccd8-4907-48f4-9c4f-d533789cdf4c&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c570558020556&rr=931b7fb91d3df9ad&cc=cn&kca=eyJrZXkiOiJ5UndETlQ1Zm1OMWtiUFZUWXJWVzlOdEZ4SDFQR1VhVHZ4ZWpveGpLR3N1TzMxbXpHK2xkQkNnRUR4aWIvdmhMckpxaEYyUXRPbytJTFlQTUdHbll0WmdWTUx0SkRjakRCb0ExNk90bjRmSFRjT2xWNG9CMmg2T0ljSUQ1OHJXSVpLR2I5T1c0V0g4Q0RIRUY1SXJMTjlhYmFFU0xpbGVobUMrNytBRWw3QnFYd21FPSIsIml2IjoiZThhM2EyMWNhYzEyNjMzZDZlZjQxNGE5NzE3ZDIzNmQifQ==_1744888472266">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{JING201962,title = {Low-rank regularized tensor discriminant representation for image set classification},journal = {Signal Processing},volume = {156},pages = {62-70},year = {2019},issn = {0165-1684},doi = {https://doi.org/10.1016/j.sigpro.2018.10.018},url = {https://www.sciencedirect.com/science/article/pii/S0165168418303505},author = {Peiguang Jing and Yuting Su and Zhengnan Li and Jing Liu and Liqiang Nie},keywords = {Image set classification, Low-rank, Tensor discriminant representation, Grassmann manifold},abstract = {Although image set classification has attracted great attention in computer vision and pattern recognition communities, however, learning a compact and discriminative representation is still a challenge. In this paper, we present a novel tensor discriminant representation learning method to better solve the image classification task. Specifically, we first exploit the advantages of Grassmann manifold and tensor to model image sets as a high-order tensor. We then propose a transductive low-rank regularized tensor discriminant representation algorithm referred as LRRTDR to learn more intrinsic representations for image sets. Our proposed LRRTDR mainly contains two components: low-rank tensor embedding and discriminant graph embedding. The low-rank tensor embedding is to learn the lowest-rank representation from a low-dimensional subspace, which is spanned by a set of latent basis matrices. The discriminant graph embedding is to further enhance the discriminant ability of the learned representations under the graph embedding framework. To solve the optimization problem of LRRTDR, we develop an alternating direction scheme based on Iterative Shrinkage Thresholding Algorithm (ISTA). Experimental results on five publicly available datasets demonstrate that our proposed algorithm not only converges with few iterations, but also achieves better accuracy compared with state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>
                                
                                <li class="item"><p align="justify"> Jing Liu, Wanning Sun, Yuting Su, <b>Peiguang Jing*</b>, Xiaokang Yang. BE-CALF: bit-depth enhancement by concatenating all level features of DNN, IEEE Transactions on Image Processing, 2019, 28 (10), 4926-4940. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8713480">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8713480,author={Liu, Jing and Sun, Wanning and Su, Yuting and Jing, Peiguang and Yang, Xiaokang},journal={IEEE Transactions on Image Processing}, title={BE-CALF: Bit-Depth Enhancement by Concatenating All Level Features of DNN}, year={2019},volume={28},number={10},pages={4926-4940},keywords={Image reconstruction;Feature extraction;Image color analysis;Deep learning;Task analysis;Signal processing algorithms;Neural networks;Bit-depth enhancement;deep learning;convolutional neural network;high dynamic range imaging;skip connections},doi={10.1109/TIP.2019.2912294}}')">bib</a>]             
                                </p>
                                </li>
                                
                                <li class="item"><p align="justify"> Jing Liu, Pingping Liu, Yuting Su, <b>Peiguang Jing*</b>, Xiaokang Yang. Spatiotemporal Symmetric Convolutional Neural Network for Video Bit-Depth Enhancement, IEEE Transactions on Multimedia, 2019, 21(9),2397-2406.  (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8636159">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8636159,author={Liu, Jing and Liu, Pingping and Su, Yuting and Jing, Peiguang and Yang, Xiaokang}, journal={IEEE Transactions on Multimedia}, title={Spatiotemporal Symmetric Convolutional Neural Network for Video Bit-Depth Enhancement}, year={2019},volume={21},number={9},pages={2397-2406},keywords={Image resolution;Convolutional codes;Spatiotemporal phenomena;Dynamic range;Transforms;Distortion;Correlation;Video bit-depth enhancement;encoder-decoder network;spatiotemporal symmetry;convolutional neural networks;feature fusion},doi={10.1109/TMM.2019.2897909}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Xue Dong, Xuemeng Song, Fuli Feng, <b>Peiguang Jing</b>, Xin-Shun Xu, Liqiang Nie. Personalized Capsule Wardrobe Creation with Garment and User Modeling. In Proceedings of ACM International Conference on Multimedia, 2019: 302-310.
                                    [<a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3350905">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@inproceedings{10.1145/3343031.3350905,author = {Dong, Xue and Song, Xuemeng and Feng, Fuli and Jing, Peiguang and Xu, Xin-Shun and Nie, Liqiang},title = {Personalized Capsule Wardrobe Creation with Garment and User Modeling},year = {2019},isbn = {9781450368896},publisher = {Association for Computing Machinery},address = {New York, NY, USA},url = {https://doi.org/10.1145/3343031.3350905},doi = {10.1145/3343031.3350905},abstract = {Recent years have witnessed a growing trend of building the capsule wardrobe by minimizing and diversifying the garments in their messy wardrobes. Thanks to the recent advances in multimedia techniques, many researches have promoted the automatic creation of capsule wardrobes by the garment modeling. Nevertheless, most capsule wardrobes generated by existing methods fail to consider the user profile, including the user preferences, body shapes and consumption habits, which indeed largely affects the wardrobe creation. To this end, we introduce a combinatorial optimization-based personalized capsule wardrobe creation framework, named PCW-DC, which jointly integrates both garment modeling (textiti.e., wardrobe compatibility) and user modeling (textiti.e., preferences, body shapes). To justify our model, we construct a dataset, named bodyFashion, which consists of $116,532$ user-item purchase records on Amazon involving 11,784 users and 75,695 fashion items. Extensive experiments on bodyFashion have demonstrated the effectiveness of our proposed model. As a byproduct, we have released the codes and the data to facilitate the research community.},booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},pages = {302–310},numpages = {9},keywords = {user modeling, fashion analysis, compatibility learning},location = {Nice, France},series = {MM 19}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Yuting Su*, Liqiang Nie, Xu Bai, Jing Liu, Meng Wang. Low-rank Multi-view Embedding Learning for Micro-video Popularity Prediction. IEEE Transactions on Knowledge and Data Engineering, 2018, 30(8): 1519-1532. (<i><strong>ESI高被引</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8233154">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8233154,author={Jing, Peiguang and Su, Yuting and Nie, Liqiang and Bai, Xu and Liu, Jing and Wang, Meng},journal={IEEE Transactions on Knowledge and Data Engineering}, title={Low-Rank Multi-View Embedding Learning for Micro-Video Popularity Prediction}, year={2018}, volume={30},number={8},pages={1519-1532}, keywords={Videos;Feature extraction;Sparse matrices;Matrix decomposition;Noise measurement;Social network services;Low-rank learning;multi-view fusion;subspace learning;popularity prediction;micro-video analysis},doi={10.1109/TKDE.2017.2785784}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Anan Liu, Yingdi Shi, <b>Peiguang Jing*</b>, Jing Liu, Yuting Su. Structured Low-rank Inverse-covariance Estimation for Visual Sentiment Distribution Prediction. Signal Processing, 2018, 152: 206-216. 
                                    [<a href="https://pdf.sciencedirectassets.com/271605/1-s2.0-S0165168418X00073/1-s2.0-S0165168418301981/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQD4oIz5RlZgDdkt40B%2FdMVvunDpW3mXY%2FAvO97%2B6A62XQIhAKQv1cZasIPjxU3sI%2FumbknBenfrMyQoRY8OfdVV7N7AKrMFCFwQBRoMMDU5MDAzNTQ2ODY1IgzT3VJde88fjrm7CFoqkAWVcIJoFYKLGeTNa80MDS9BP4OXAjQPQrvEvPpQWB1dbvVgGkZSenIsutJFNc%2BECVIwFv29%2Fn52HHBjFBONqL2VoMu3d5mA%2BWx83OQJnE%2BFSpEz0S5fKQKH4mKdk8b4O4mzIzwIA0Sxh7FleaciTNWd4JPLjJOdVNswDGnXO5xFRz9JQGtLszj%2FwhC7VjPyLWNGatHLNtNl8HdsqNECVklXsyydK1xqh5SEmmIVFxUmrl7WK85p7sfnmH2mBzhNPe7OsYN7vnhtngC%2BkA80eXdBnMtEignbqBOT1LTF7sJMnBPogB2OBCEi%2BQ2hH%2F5FzMsU3CCcGrHH8ZqTPlIYHo82ltylPX8UGbHDkxFDbia8Z6X6UfDs3snL7UieXUdzvEOxP5PrXOfyDWCRy3fHlN9pgmxtL%2BcXeAZZOSFLS%2FVVIojH4EQdXqGnXqidKkjyO0oOhRzLH8DQj4A1lfEodrHc4hDATrkVhHgcUdwNZ91pAuloOLDqeWVTZKOotIWcOQ63f21BGn%2BdYYbTVJB8SzcSmhJnm7ZtGxeQVWkvXaMlxjTjogqrgD6sfzTLX13eif%2Fz1GD1J0qmL0WnKntS6ou8nSO6RO9mnk4c0v%2FQGOQTPnLZ%2B51MTwPs2%2BSiSTJa3EJeRXRjpOCWYy9qyxu5X7hi3x7Dszl2u7AndNmvfBVMGy%2BzouSak1biDFClDc9LQI1YnCftG%2BRBSi3kRmxPMrSVvYExUKV4hMV%2BeUnBvPyxwuNoumkHu0epA4iXzYSxkQ67g3iMB04ojChCTYLdljNUdh9jp%2BEEpc44wi6SFaJxiR9opOSU2zVJzLvZgaP7WzMOEPJWjSL9U6jH1GEqGk2okpH63ELLQ3u7dO%2FZy3byHDCLsIPABjqwASa6f93vmDJVwGfTI29Vhw0RN4Meu%2FB%2BLDFVgU1QUNofc8SXQ2QuPqJmdC%2FxMo4C%2FuYjeJldiY5%2FVsWlm9KaRlpkGJM1Vvo7R6K2fBSHdYllueZmZY9GWd92kje28viJo5bImrvtspPYbVw06dZMhK6r63vQukC5%2Bfa0HWzMFAGVWB02tgtvaxb8322LarsuDwBXq1nP9AspeAV82sGlAumIzxhmlO4wPZabi9DiEMzH&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T114413Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYW3ZNN2BF%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0feca1d76919b9f923acebe4e523492d802bc316c02f3ebabaa4f8b55ff5d45c&hash=3d8188b755d5cc3396deb080fdebebeb0d8ba2af3046499a332d07a434bde3f6&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0165168418301981&tid=spdf-1db732c0-725b-48d6-b063-fe82a162004e&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57050e06525d&rr=931bab52d8fbdd36&cc=cn">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{LIU2018206,title = {Structured low-rank inverse-covariance estimation for visual sentiment distribution prediction},journal = {Signal Processing},volume = {152},pages = {206-216},year = {2018},issn = {0165-1684},doi = {https://doi.org/10.1016/j.sigpro.2018.06.001},url = {https://www.sciencedirect.com/science/article/pii/S0165168418301981},author = {Anan Liu and Yingdi Shi and Peiguang Jing and Jing Liu and Yuting Su},keywords = {Image sentiment, Label distribution learning, Structured sparsity, Low-rank},abstract = {Visual sentiment analysis has aroused considerable attention with the increasing tendency of expressing sentiments via images. Most previous studies mainly focus on predicting the most dominant sentiment categories of images while neglecting the sentiment ambiguity problem caused by the fact that the image sentiments elicited from viewers are very subjective and different. To tackle this problem, many research efforts have been devoted to visual sentiment distribution prediction, in which an image is characterized by a distribution over a set of sentiment labels rather than a single label or multiple labels. In this paper, we propose a structured low-rank inverse-covariance estimation algorithm for visual sentiment distribution prediction. The proposed model incorporates low-rank and inverse-covariance regularization terms into a unified framework to learn more robust feature representation and more reasonable prediction model simultaneously. In particular, low-rank regularization term plays a pivotal role in capturing the low-rank structure embedded in data and seeking the lowest-rank representation of samples in a latent low-dimensional subspace. Inverse-covariance regularization term is introduced to enforce the structured sparsity of regression coefficients by taking the multi-output structure into account. We also develop an alternative heuristic optimization algorithm to optimize our objective function. Experiment results on three publicly available datasets, i.e., Emotion6, Flickr_LDL and Twitter_LDL, using six measurements demonstrate the superior prediction performance compared with state-of-the-art algorithms.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Jing Zhang, Xinhui Li, <b>Peiguang Jing*</b>, Jing Liu, Yuting Su. Low-rank regularized heterogeneous tensor decomposition for subspace clustering, IEEE Signal Processing Letters, 2018, 25 (3), 333-337.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8025385">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8025385,author={Zhang, Jing and Li, Xinhui and Jing, Peiguang and Liu, Jing and Su, Yuting},journal={IEEE Signal Processing Letters}, title={Low-Rank Regularized Heterogeneous Tensor Decomposition for Subspace Clustering}, year={2018},volume={25},number={3},pages={333-337},keywords={Tensile stress;Signal processing algorithms;Matrix decomposition;Clustering algorithms;Robustness;Sparse matrices;Algorithm design and analysis;Low-rank;subspace clustering;tensor decomposition},doi={10.1109/LSP.2017.2748604}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Jizhong Duan, Yu Liu, <b>Peiguang Jing</b>. Efficient operator splitting algorithm for joint sparsity-regularized SPIRiT-based parallel MR imaging reconstruction[J]. Magnetic Resonance Imaging, 2018, 46: 81-89.
                                    [<a href="https://pdf.sciencedirectassets.com/271222/1-s2.0-S0730725X17X0009X/1-s2.0-S0730725X17302369/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIBeu6qh6D7%2B%2BYFXwL7LPtCAJd0mc0OXF9YzI7aTNl7JKAiB2rKLstcvYAuAnyIkFjTx%2B9tK7b9vy9i1rBWFvXql7ZyqzBQhcEAUaDDA1OTAwMzU0Njg2NSIMh2IDHqyLJ5bZWARgKpAFJv5RGBNLIG%2FoBpldOFYKk1hzbpfXJX1kzjyUol2XKi2rdhV3oMfhl5OrbjCxuWhiCg0KwJ4%2BdRsx8Nr6vLyQnP%2Blysc7OG%2Bj0AmwM6uh0PEeAMZT7nFSrd%2Fw13dMQUQtbLOjnwXuUEsjZLCHbFCHPAtoQv4kgmLdGKsyfkzZuv5tu3qqt7n6sjgeQNGWQx9ZGapiO4GFOdFq3H4CtEdoVvNpfIq6Eyh0GZYIHutbNr1C4eivMbwSvAYoBNVvqH321hugzTjoepnZCqJmMB137t%2FhEQYT3S0IJH7bjz4PxALSbHOY%2FbbhWOs%2B2d1gHjtxtyJPRxusstMQQxdpdqEJd8J5Z8LQl3jE91Iuj3yScWYCVXRyb%2FE8erbkLUiR5Mn9r1PFm43zL7D55DifFFzOJt6op83FEmrAhZ720ohZ4M9Z5GPECzTEMGqEENHxUgKk70DS3Hk6RT%2FO0HHWRdcAcKPFq46o3kxf3VBoIWuZxtXKb0jl%2Bm%2B%2BpT08G3VFqcO9N1LTPuQaztJKhbhwrewShjmRo%2FaF9Pb2GuyLTN85RFNqMUMigk%2FTjjZmmxVaiOpWNVhcbheqCQLMnRsA5XkxBNr1rqaxLNEz4ZhXlxR0TfpN6lYHvuqk2XPpcmgyp91xBU4pPf0n35JsmmTySaG9jfTGm4jjzjHrPerViSNklxT0hhaOcF6eUbdD44wCebUnEs42pvjEUihv2hYAsl00Z8vxsJKKyxRU5%2FJ72%2FUZ51GsPLy0ChUe9DHr6fdkt8Y90MjoN3FlfjrNCiBzD7SH9V2aCSBPpxRsdCtIsCeTQ%2FmrN%2BiXDzHVR8QbeBD6BRtW9E9aus3F%2BC7E8NopI8yXwlSqsPR2kyPssYUot0IQtr0wqMCDwAY6sgFdEFVhK6DwwZ3xbCg2vFI5D%2BSzKomEZPHn7ewK6F5nJJx86F9q54ErpfPug9Lewp4K8TQHs8CN0d5Dw1bGgqh6ESmQSxlaM4ThRmzMPoEA0XuPw8K6W3ITbwidUUpiq4FkGI29iGKxlfbgCSPYlmFz5I74tgaNaFHv1dhzYwL8KKQTzPVhWUnLy6h064clAE9H%2BldBHPH96fozCLy4x6znKvYP0VJBkuz3o9qW4zkBMsTI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T114844Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYZX633P3W%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=1699d84e614319c07c258016879db99bdcb2a4fab5b123fb06f9728601542489&hash=3d7ee73135893b99718b11e47f98a08e4aaf8b60989aa3aae23f6f546920e356&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0730725X17302369&tid=spdf-173952fa-8ed3-4b4c-8c47-ae92ae82c852&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57050d55015d&rr=931bb1f2f9aedd36&cc=cn&kca=eyJrZXkiOiJid3lDS2Z3WnFGdllVZFhmb3BkUFh0L0JsUHg0OXpTUUpHWHVmb2pMR05vWm91eklTaUg5SXN0T3BIcmthUC9LQ2ttR1UvQ2p6VHpPdXBhQXdsNElVbitYZnRYemFKYW5SL2U3VzlJVTF3eFFjNnU3N0xBMEpLaldyVnlsdFRBWkt3UWE2U00rNEJBL2pvRHgxdXhIN2ZLKy9pME0wY1BSTjJOUnVPZUVIeDVwdm16UCIsIml2IjoiZmQ5ZjMwOWVmODliYWE4MDRlOTcyMzZmOTQ2YmVhMjAifQ==_1744890529506">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{DUAN201881,title = {Efficient operator splitting algorithm for joint sparsity-regularized SPIRiT-based parallel MR imaging reconstruction},journal = {Magnetic Resonance Imaging},volume = {46},pages = {81-89},year = {2018},issn = {0730-725X},doi = {https://doi.org/10.1016/j.mri.2017.10.013},url = {https://www.sciencedirect.com/science/article/pii/S0730725X17302369},author = {Jizhong Duan and Yu Liu and Peiguang Jing},keywords = {Parallel magnetic resonance imaging, Auto-calibrating, Self-consistent parallel imaging (SPIRiT), Operator splitting, Joint total variation, FISTA, Barzilai and Borwein method},abstract = {Self-consistent parallel imaging (SPIRiT) is an auto-calibrating model for the reconstruction of parallel magnetic resonance imaging, which can be formulated as a regularized SPIRiT problem. The Projection Over Convex Sets (POCS) method was used to solve the formulated regularized SPIRiT problem. However, the quality of the reconstructed image still needs to be improved. Though methods such as NonLinear Conjugate Gradients (NLCG) can achieve higher spatial resolution, these methods always demand very complex computation and converge slowly. In this paper, we propose a new algorithm to solve the formulated Cartesian SPIRiT problem with the JTV and JL1 regularization terms. The proposed algorithm uses the operator splitting (OS) technique to decompose the problem into a gradient problem and a denoising problem with two regularization terms, which is solved by our proposed split Bregman based denoising algorithm, and adopts the Barzilai and Borwein method to update step size. Simulation experiments on two in vivo data sets demonstrate that the proposed algorithm is 1.3 times faster than ADMM for datasets with 8 channels. Especially, our proposal is 2 times faster than ADMM for the dataset with 32 channels.}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Luming Zhang, <b>Peiguang Jing#</b>, Yuting Su, Chao Zhang, Ling Shao. SnapVideo: Personalized Video Generation for a Sightseeing Trip. IEEE Transactions on Cybernetics, 2017 47 (11), 3866-3878. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7516655">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{7516655,author={Zhang, Luming and Jing, Peiguang and Su, Yuting and Zhang, Chao and Shaoz, Ling}journal={IEEE Transactions on Cybernetics}, title={SnapVideo: Personalized Video Generation for a Sightseeing Trip}, year={2017},volume={47}, number={11}, pages={3866-3878},keywords={Semantics;Mobile handsets;Probabilistic logic;Feature extraction;Google;Video recording;Quality assessment;Aesthetic;comprehensive views;scenic spots;SnapVideo;video clips},doi={10.1109/TCYB.2016.2585764}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>Peiguang Jing</b>, Yuting Su, Liqiang Nie, Huimin Gu. Predicting Image Memorability Through Adaptive Transfer Learning From External Sources. IEEE Transactions on Multimedia, 2017, 19(5): 1050-1062. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7797211">pdf</a>] 
                                    [<a href=" https://github.com/peiguangjing/image-memorability">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{7797211, author={Jing, Peiguang and Su, Yuting and Nie, Liqiang and Gu, Huimin}, journal={IEEE Transactions on Multimedia}, title={Predicting Image Memorability Through Adaptive Transfer Learning From External Sources}, year={2017},volume={19}, number={5}, pages={1050-1062},keywords={Visualization;Semantics;Feature extraction;Predictive models;Adaptation models;Learning systems;Dictionaries;Image attribute;memorability prediction;regression;transfer learning;visual feature},doi={10.1109/TMM.2016.2644866}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Jing Zhang, Chuanzhong Xu, <b>Peiguang Jing</b>, Yuting Su*. A tensor-driven temporal correlation model for video sequence classification. IEEE Signal Processing Letters, 2016, 23(9): 1246-1249.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7486042">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{7486042,author={Zhang, Jing and Xu, Chuanzhong and Jing, Peiguang and Zhang, Chengqian and Su, Yuting},journal={IEEE Signal Processing Letters}, title={A Tensor-Driven Temporal Correlation Model for Video Sequence Classification}, year={2016}, volume={23},number={9},pages={1246-1249},keywords={Tensile stress;Video sequences;Correlation;Feature extraction;Linear programming;Time series analysis;Learning systems;Autoregressive;tensor decomposition;video sequence classification},doi={10.1109/LSP.2016.2577601}}')">bib</a>]             
                                </p>
                                </li>

                                <li class="item"><p align="justify"> Yanwei Pang, Zhong Ji*, <b>Peiguang Jing</b>, Xuelong Li. Ranking graph embedding for learning to rerank. IEEE Transactions on Neural Networks and Learning Systems, 2013, 24(8): 1292-1303.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6508899">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{6508899,author={Pang, Yanwei and Ji, Zhong and Jing, Peiguang and Li, Xuelong}, journal={IEEE Transactions on Neural Networks and Learning Systems},  title={Ranking Graph Embedding for Learning to Rerank}, year={2013},volume={24},number={8},pages={1292-1303}, keywords={Dimensionality reduction;graph embedding;image search reranking;learning to rank},doi={10.1109/TNNLS.2013.2253798}}')">bib</a>]             
                                </p>
                                </li>


                                <li class="item"><p align="justify"> 褚晶辉,史李栋,<b>井佩光</b>,吕卫.适用于目标检测的上下文感知知识蒸馏网络.浙江大学学报(工学版),2022,56(03):503-509.
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>井佩光</b>,李亚鑫,苏育挺.一种多模态特征编码的短视频多标签分类方法.西安电子科技大学学报,2022,49(04):109-117.
                                </p>
                                </li>

                                <li class="item"><p align="justify"> 张丽娟,崔天舒,<b>井佩光</b>,苏育挺.基于深度多模态特征融合的短视频分类.北京航空航天大学学报，2021，47(03)：478-485.
                                    [<a href="https://bhxb.buaa.edu.cn/bhzk/article/doi/10.13700/j.bh.1001-5965.2020.0457">pdf</a>] 
                                </p>
                                </li>

                                <li class="item"><p align="justify"> 李云,卢志翔,刘姝伊,王粟,吕梓民,<b>井佩光</b>.基于深度多模态关联学习的短视频多标签分类研究[J].数据分析与知识发现,2024,8(07):77-88.
                                    [<a href="https://manu44.magtech.com.cn/Jwk_infotech_wk3/CN/Y2024/V8/I7/77">pdf</a>] 
                                </li>

                                <li class="item"><p align="justify"> 吕卫,韩镓泽,褚晶辉,<b>井佩光</b>.基于多模态自注意力网络的视频记忆度预测[J].吉林大学学报(工学版),2023,53(04):1211-1219.
                                    [<a href="http://xuebao.jlu.edu.cn/gxb/EN/abstract/abstract14707.shtml">pdf</a>]
                                </p>
                                </li>

                                <li class="item"><p align="justify"> 苏育挺,王骥,赵玮,<b>井佩光</b>.基于动态图卷积的图像情感分布预测[J].吉林大学学报(工学版),2023,53(09):2601-2610.
                                    [<a href="https://xuebao.jlu.edu.cn/gxb/CN/Y2023/V53/I9/2601">pdf</a>]
                                </p>
                                </li>

                                <li class="item"><p align="justify"> 苏育挺,王富铕,<b>井佩光</b>.深度多模态不确定度的短视频事件检测方法[J].哈尔滨工业大学学报,2024,56(05):36-45.
                                </p>
                                </li>

                                <li class="item"><p align="justify"> <b>井佩光</b>,田雨豆,汪少初,李云,苏育挺.基于动态扩散图卷积的交通流量预测算法[J].吉林大学学报(工学版),2024,54(06):1582-1592.
                                    [<a href="http://xuebao.jlu.edu.cn/gxb/CN/abstract/abstract15155.shtml">pdf</a>]
                                </p>
                                </li>

                                <li class="item"><p align="justify"> 李云,孙山林,黄晴,<b>井佩光</b>.基于多路混合注意力机制的水下图像增强网络[J].电子与信息学报,2024,46(01):118-128.
                                    [<a href="https://jeit.ac.cn/cn/article/pdf/preview/10.11999/JEIT230495.pdf">pdf</a>]
                                </p>
                                </li>

                                <li class="item"><p align="justify"> 苏育挺,景梦瑶,<b>井佩光</b>,刘先燚.基于光度立体和深度学习的电池缺陷检测方法[J].吉林大学学报(工学版),2024,54(12):3653-3659.
                                    [<a href="http://xuebao.jlu.edu.cn/gxb/CN/10.13229/j.cnki.jdxbgxb.20230130">pdf</a>]
                                </p>
                                </li>


                                <div id="show-more" class="show-more" onclick="showMorePapers()">Show More</div>
                                <div id="show-less" class="show-less" onclick="showLessPapers()">Show Less</div>
                                <p><a href="https://scholar.google.com/citations?user=Kr-OYyQAAAAJ&hl=zh-CN">完整论文目录见Google学术</a></p>       
                            </ol>
                        </div>



                        <div id="2025" class="paper-list">
                            <ol>

                                 <li class="justify"><p align="justify"> <b>ICCV2025</b> | MPBR: Multimodal Progressive Bidirectional Reasoning for Open-Set Fine-Grained Recognition
                                    [<a href="">pdf</a>] 
                                    [<a href="">code</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '')">bib</a>]            
                                </p>
                                </li>

                                <li class="justify"><p align="justify"> Wei Lu, He Zhao, Dubuke Ma, <b>Peiguang Jing</b>, LUFormer: A luminance-informed localized transformer with frequency augmentation for nighttime flare removal, Neural Networks, Volume 190, 2025, 107660, ISSN 0893-6080.
                                    [<a href="https://pdf.sciencedirectassets.com/271125/1-s2.0-S0893608025X00071/1-s2.0-S0893608025005404/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjECUaCXVzLWVhc3QtMSJGMEQCIHd2KAB3O%2Fa0gG2%2FzZxHoI1dnydkJjXHQnvjkN%2BBbKyiAiA2OMIvAgFrHyNgAHTc8yCMyTgZeXfmmjAItrKDVPPVmyqyBQg9EAUaDDA1OTAwMzU0Njg2NSIMUQSJAB5dfFP4UyA2Ko8FJwIZhWKklfqMUe2k2yff%2FuWN0EceIqhC%2FKO6Woel%2BuUkunPm4YDT%2FQP1LhsumUEGWvkGDfjttZDGLZNxCi6WrDtD822vDw3WBaTZInHmhVV7KwW44JKc9qT5o2%2FKf4di6HBVcdQaPEQEqfgEnH1E1xB8kJ1kY3Htb6NeAsTsoAG09%2Fv9%2BQOymcUZGEVGQ0Feahwn6My3G7wvbvNYpFLTMJqdGTANOSLrLZwH0HkTaHO8kybvOe4QikD30%2BC2%2F0CHWw1ZII%2BzoIuFFwhBTnoC83tB%2BAwASYx3oR2cC3uFi08qTLH4xRgMd7vTZXVhBwFXqfdAp2F%2Fo8GvK8d8bPV7VPKFvlpKEYus2iIBnmv3n68vTthizLmacarzgyrG4BvOy8iSgHyzaLgGn0rIUmf8lI1Tn0aJXPvAmsJnIumIrPalFp523gRVVMxpnv7SMtRdKE%2BeC%2BMC%2B7yhJpPlEbdXWtSxvpYGu24r7cQaSJgmjVheAeBoztsdyUyNm8tcGYjAP8ncNgF811OUDU5pOyay4ERHvSrz7%2BzOPEvoG53kiwhEcRNAvRp2iWCdI7BfgcyVJ15zB0t0DkK6v06rTdOv5a1zsx1oh8fA%2Fgua9JZiBX5ZA6vUciFtkOhN7zrq12zD7gNibC4J7Jz%2BkWAbTgA7lQNWoe8oGi1Oix0H5GmExO0ndiPm7B0vYwPLIBeHdVkYSC667ucmYT%2BAHPb5irk39lxm3GFtoPz5ypc3e1e6SIiLXF9OZsjr%2FaLGBtmXn7JMEQZLB47asGg4zRio5K8YEzgcUL0MZyRmjahlJxKbn%2FLZ9opDAYVo3v1Rfur1ljeqIvx5XUeAB9OUCXX2ZPsUGHXTKyQREGzQMNNkl0P35DDxqNfDBjqyAY6Zh0zpEaVpnfTl0APbvY59r5VetdL81Ljx%2BH4Lk3v%2F9iiZ4S2HW%2BjPoJ8ajJCZvNpt8tgB0zGp2YncwYYI3jxeajIO%2FhsbHtuJhHGPm0ekaA0gl%2FMD9JPvWAHXFqCjDjfnN%2BDvtWWvwyqUYl3%2B%2FyktTuRPaTI73oIcTYFW5FjhfopMXJCtKg2SJhMFCzSj15BXfV03ezxJDQTuiGtj2ooPC404MLfqNiVd0ajAlqpTBi0%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250715T045518Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY5EUIDS6Q%2F20250715%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8292d8d673de5dba4973df85fa81fac049e59824e29156ac19849bc72caec0b9&hash=fb9a02475f86388c6d7e3ae3c76249640b111973d40969c3321b637172f8aecd&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0893608025005404&tid=spdf-edab3ea5-f723-451a-9aa0-700b1e0b2df5&sid=f22364994026e54d6f2b0a7-e63ba6ed783fgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=190c5b51095602020454&rr=95f6aab3afb485bb&cc=cn&kca=eyJrZXkiOiJQMXVEcG9qVm9BdUowL245L1E0azhIa2lCTnlPYjB1UEZNU3crMThJSUhQSERHNGIyWWVwYXpjZ1k2OEFGbXhnMXA3MUVkbkZJdCt6MzRIOG9aNGRMTE0vcUNpcldVODh5UkcybTRpR0pGL2pGVUUwLzlCOGdWYlgvZ05xblJFWDhwRDJEWjY1RGIxQmtUN1RLSUNCSnBRRWtkQjVhTUdyYWFNLzBTM1BWaEl1eXBGK1BBPT0iLCJpdiI6IjNhMzNmNzA0ZjM2MDZjMjM5ODkxN2E3MThlOTIzYjk0In0=_1752555327519">pdf</a>] 
                                    [<a href="">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@article{LU2025107660,title = {LUFormer : A luminance-informed localized transformer with frequency augmentation for nighttime flare removal},journal = {Neural Networks},volume = {190},pages = {107660},year = {2025},issn = {0893-6080},doi = {https://doi.org/10.1016/j.neunet.2025.107660},url = {https://www.sciencedirect.com/science/article/pii/S0893608025005404},author = {Wei Lu and He Zhao and Dubuke Ma and Peiguang Jing},keywords = {Image restoration, Flare removal, Multi-domain information fusion, Wavelet transforms},abstract = {Flare caused by unintended light scattering or reflection in night scenes significantly degrades image quality. Existing methods explore frequency factors and semantic priors but fail to comprehensively integrate all relevant information. To address this, we propose LUFormer, a luminance-informed Transformer network with localized frequency augmentation. Central to our approach are two key modules: the luminance-guided branch (LGB) and the dual domain hybrid attention (DDHA) unit. The LGB provides global brightness semantic priors, emphasizing the disruption of luminance distribution caused by flare. The DDHA improves deep flare representation in both the spatial and frequency domains. In the spatial domain, it broadens the receptive field through pixel rearrangement and cross-window dilation, while in the frequency domain, it emphasizes and amplifies low-frequency components via a compound attention mechanism. Our approach leverages the LGB, which globally guides semantic refinement, to construct a U-shaped progressive focusing framework. In this architecture, the DDHA locally augments multi-domain features across multiple scales. Extensive experiments on real-world benchmarks demonstrate that the proposed LUFormer outperforms state-of-the-art methods. The code is publicly available at: https://github.com/HeZhao0725/LUFormer.}}')">bib</a>]            
                                </p>
                                </li>
                                
                                <li><p align="justify">	Yuting Su, Enqi Su, Weiming Wang, <b>Peiguang Jing</b>, Dubuke Ma, Fu Lee Wang. Adversarial neighbor perception network with feature distillation for anomaly detection[J]. Expert Systems with Applications, 2025, 274: 126911.
                                    [<a href="https://pdf.sciencedirectassets.com/271506/1-s2.0-S0957417425X00063/1-s2.0-S0957417425005330/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCYq0Q%2BHnlmKl946A4ZefNBwFdnq%2B9HmqZ90JONwEofpgIhALdimQlffBIuUgLo0iNQnynWIE7D5Q4vuFlzql3lOXjTKrMFCEUQBRoMMDU5MDAzNTQ2ODY1Igy1udJTUyN19wdvn8QqkAWPW%2BM6eQA9SyScaznujkipWhc8lBcYopK5N35AifXWHHngHVKP0slZzEWf1e5BXZC0tGpinSJxgUNCB04Cx%2BUbeA1e3csgy2jn3KdpsJjLrIHH6USqLoLsyPw8IJFA2WIID5uJxXCn9lWWHvim4OkbRSR%2B2RsiaVJEGmMBGEznQoVQu0pqrv7%2BOyT0Zm0sUumLfklxx%2F5BR%2FRirKwzg1DH%2ByWMGQrCSto0eZS1UIUhls6gWfRQw2pXG6C5zm%2Bv%2FYr%2BBFG%2FQ0dPqIFLV60XHeEFe9kDLyC%2Bl2k3CQIKpiNdU4Sbd62b2s6cSRaxrFeEdti4gbptO6U7c2FB4yQAVnbZ3t5cwSikWqOeX3HRnYnLaDCFhk89agUUzJ3Um2nm6%2FfnCPN35GZoih9DnS6TBuutqhO%2FP4nhxnlw4waJ5phAGBBdINf0HatXORF8GVR6J%2FdB9XI7Q3UkyZuAOnDKQ8owiXNCAk7aZTkzBVobOSGpplk4%2Bt5NHFPbrvInC55K77J1%2FK3v9XUp3Cn2RemSKp%2BlGDo8iXUxvigmjup%2Fxh0r8yNU8t3hIy%2FRbvMaAglx21H6kychfldDiimeR4jHlraW%2B1M%2B6KDvyvPJH9nmHAE8%2FELvStMqlTh13Xn4FWYzIpUlxNicPJdADiuPgn1mixTRHfS4wI67Yohkv9VSq0DfAt8NBeL9cQI1ZHkZQa9Cs5WDQ0x9VQLMn6XdQaOL0LeJy4C7UCkLFDGvQ2FQp8sHOt2ZiNAOUs73A9v4EGOyWD4KUp6N2xxFkj2NShIzHjeMhRTtDSkrdo%2F1X7FuYkuYeNnvncFia3L7aIXJO3J535ZPi7qJhOE54432cbE%2FRkzjyacMll4MVReBxAdVyMyKJDCkq%2F6%2FBjqwAXkalfwd5qcBPa6E2jqPTeY4VMiivNuYmIj%2FzMSYUYEVDV9b2ZrVLVy4jKU0%2FUExIL9IT%2F1vJu%2F0zixRPrBFe3REibki3OFdZMOJ77uHtk2ObdWTU9Rg89Ahen8BsUTMSSQt9pxpASmQDchLFx9NlAl7NgWTMzV9%2FCnFv3HCRQSmeKzzMSfY9hlcdF7QRKLfgrKy21zuRmHlxeKzZRxLWYRlX06c6OYhZ8Mm%2FamOygpc&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250416T125548Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUUOQNEKJ%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=6e556688e94531b556fb04980fa19f78433f4e5b29dad7a35c9513adf13d077f&hash=6f3c98d33e774e6da2f85427864179254b7c1368db9fdde6008dd26b30072bef&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0957417425005330&tid=spdf-43a64672-31f2-456b-9643-44ebec4f93d1&sid=5b329c8456b1f446271952a8b57bb1750a63gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57540b52035f&rr=9313d6d07e83ddc0&cc=cn&kca=eyJrZXkiOiJSWmVDMFUvQ3gvdFhXV25EM1d3cUVrNHFlbHVYVzZPSDBUd2R6MWVtekdNc25mMkdGMGozQVVuazJKU1V4VTlaRjBSbHNNM09Pdk4zMEZXUHJqTDdsSHRsdmwyUmlXQ1VLVjc0MHF2ZElzNFFDc0UrZG9LZ09qSXJvQlNUMEgwQmVKakIzUEp1b2FUNzhSMTB3RkZTVTNPVEdORnV1aHBmNVhwS0x2M2RPSzJrTFJNbyIsIml2IjoiMWQ2ZTQ2NzUyMDNjNmJlYzE2OWI5N2JlMGM3ZmNjZjIifQ==_1744808153802">pdf</a>] 
                                    [<a href=" https://github.com/thesusu/ANP-FD">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@article{SU2025126911,title = {Adversarial neighbor perception network with feature distillation for anomaly detection},journal = {Expert Systems with Applications},volume = {274},pages = {126911},year = {2025},issn = {0957-4174},doi = {https://doi.org/10.1016/j.eswa.2025.126911},url = {https://www.sciencedirect.com/science/article/pii/S0957417425005330},author = {Yuting Su and Enqi Su and Weiming Wang and Peiguang Jing and Dubuke Ma and Fu Lee Wang},keywords = {Anomaly detection, Unsupervised methods, Neighbor perception, Spatial location, Feature distillation},abstract = {Anomaly detection has become a research hotspot in the field of intelligent manufacturing, which has attracted strong attention from academia and industry. Although the unsupervised methods based on reconstruction have shown promising results in anomaly detection, they still have the problems that the representation of abnormal features is not significantly different and the representation of normal features is not accurate enough. To solve these problems, we propose an adversarial neighbor perception network with feature distillation (ANP-FD) for anomaly detection, which includes multi-scale neighbor perception, robust distillation recovery, and co-attention adversarial detection modules. First, the multi-scale features pass through the neighbor awareness to predict accurate spatial location information to improve the reconstruction accuracy. Then, the abnormal features are eliminated by a robust trainable distillation structure, which expands the representation difference of abnormal features during reconstruction. In addition, the co-attention adversarial detection can accurately detect and locate anomalies in the multi-scale feature space. The experimental results on the MVTec, BTAD, MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our proposed method achieves better performance than current state-of-the-art (SOTA) approaches. Especially, the proposed method achieves 99.5 AUROC% and 94.5 AUPRO% on the MVTec. We also achieve superior performance in few-shot scenarios. Code: https://github.com/thesusu/ANP-FD.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify">	Yu Qiao, Wei Lu, <b>Peiguang Jing</b>, Weiming Wang, Yuting Su. Multimodal Dual-Graph Collaborative Network With Serial Attentive Aggregation Mechanism for Micro-Video Multi-Label Classification[J]. IEEE Transactions on Multimedia, 2025.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10924420">pdf</a>] 
                                    [<a href=" https://github.com/HappyField-glitch/MDGCN">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10924420,author={Qiao, Yu and Lu, Wei and Jing, Peiguang and Wang, Weiming and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={Multimodal Dual-Graph Collaborative Network With Serial Attentive Aggregation Mechanism for Micro-Video Multi-Label Classification}, year={2025},volume={},number={},pages={1-12},keywords={Correlation;Multi label classification;Videos;Graph convolutional networks;Data mining;Semantics;Feature extraction;Decoding;Training;Representation learning;Micro-video;multimodal representations;multi-label classification;graph convolutional network},doi={10.1109/TMM.2025.3542895}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify">	Chen Zhao, Mengyuan Yu, Fan Yang, <b>Peiguang Jing</b>. VIIS: Visible and Infrared Information Synthesis for Severe Low-Light Image Enhancement. In Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision, 2025: 2174-2184.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10943586">pdf</a>] 
                                    [<a href=" https://github.com/Chenz418/VIIS">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@INPROCEEDINGS{10943586,author={Zhao, Chen and Yu, Mengyuan and Yang, Fan and Jing, Peiguang}, booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, title={VIIS: Visible and Infrared Information Synthesis for Severe Low-Light Image Enhancement}, year={2025},volume={},number={}, pages={2174-2184},keywords={Computer vision;Limiting;Fuses;Computational modeling;Noise reduction;Diffusion models;Image augmentation;Image restoration;Image enhancement;Image fusion},doi={10.1109/WACV61041.2025.00218}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yuting Su, Peng Ma, Weiming Wang, Shaochu Wang, Yuting Wu, Yun Li, <b>Peiguang Jing</b>. AMDANet: Augmented Multi-scale Difference Aggregation Network for Image Change Detection[J]. IEEE Transactions on Geoscience and Remote Sensing, 2025. 63,5616012
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10891412">pdf</a>] 
                                    [<a href=" https://github.com/mp-st/AMDANet">code</a>] [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10891412,author={Su, Yuting and Ma, Peng and Wang, Weiming and Wang, Shaochu and Wu, Yuting and Li, Yun and Jing, Peiguang},journal={IEEE Transactions on Geoscience and Remote Sensing}, title={AMDANet: Augmented Multiscale Difference Aggregation Network for Image Change Detection}, year={2025},volume={63},number={},pages={1-12},keywords={Feature extraction;Data mining;Transformers;Deep learning;Remote sensing;Electronic mail;Attention mechanisms;Accuracy;Vectors;Training;Change detection (CD);difference feature;feature aggregation;information augmentation},doi={10.1109/TGRS.2025.3542814}}')">bib</a>]             
                                </p>
                                </li>

                            <ol>
                        </div>
                        

                        <div id="2024" class="paper-list">
                            <ol>

                                <li><p align="justify"> Wei Lu, Yujia Zhai, Jiaze Han, <b>Peiguang Jing*</b>, Yu Liu, Yuting Su. VMemNet: A Deep Collaborative Spatial-Temporal Network with Attention Representation for Video Memorability Prediction. IEEE Transactions on Multimedia, 2024, 26: 4926-4937
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298788">pdf</a>] 
                                     [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10298788,author={Lu, Wei and Zhai, Yujia and Han, Jiaze and Jing, Peiguang and Liu, Yu and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={VMemNet: A Deep Collaborative Spatial-Temporal Network With Attention Representation for Video Memorability Prediction}, year={2024},volume={26},number={},pages={4926-4937},keywords={Visualization;Semantics;Feature extraction;Predictive models;Task analysis;Streaming media;Collaboration;Video memorability;Attention mechanism;Spatial-temporal features},doi={10.1109/TMM.2023.3327861}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Haoyi Sun, Liqiang Nie, Yun Li*, Yuting Su. Deep Multi-modal Hashing with Semantic Enhancement for Multi-label Micro-video Retrieval, IEEE Transactions on Knowledge and Data Engineering, 2024,36(10): 5080-5091
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10330746">pdf</a>] 
                                    [<a href=" https://github.com/haoyi199815/DMHSE">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10330746,author={Jing, Peiguang and Sun, Haoyi and Nie, Liqiang and Li, Yun and Su, Yuting},journal={IEEE Transactions on Knowledge and Data Engineering}, title={Deep Multi-Modal Hashing With Semantic Enhancement for Multi-Label Micro-Video Retrieval}, year={2024}, volume={36},number={10},pages={5080-5091},keywords={Semantics;Hash functions;Encoding;Representation learning;Convolutional neural networks;Quantization (signal);Kernel;Deep hashing;micro-video retrieval;multi-label;multi-modality},doi={10.1109/TKDE.2023.3337077}}')">bib</a>]             
                                </p>
                                </li>
                                
                                <li><p align="justify"> <b>Peiguang Jing</b>, Kai Cui, Weili Guan, Liqiang Nie, Yuting Su. Category-aware Multimodal Attention Network for Fashion Compatibility Modeling. IEEE Transactions on Multimedia, 2024.25: 9120-9131
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10049142">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10049142,author={Jing, Peiguang and Cui, Kai and Guan, Weili and Nie, Liqiang and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={Category-Aware Multimodal Attention Network for Fashion Compatibility Modeling}, year={2023},volume={25},number={},pages={9120-9131},keywords={Visualization;Convolutional neural networks;Clothing;Representation learning;Semantics;Feature extraction;Correlation;Dynamic graph convolutional network;fashion compatibility modeling;multimodal representation},doi={10.1109/TMM.2023.3246796}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Xuan Zhao, Fugui Fan, Fan Yang, Yun Li, Yuting Su. Multimodal Progressive Modulation Network for Micro-video Multi-label Classification. IEEE Transactions on Multimedia, 2024.26: 10134-10144
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10572319">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10572319,author={Jing, Peiguang and Zhao, Xuan and Fan, Fugui and Yang, Fan and Li, Yun and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={Multimodal Progressive Modulation Network for Micro-Video Multi-Label Classification}, year={2024},volume={26},number={},pages={10134-10144},keywords={Modulation;Correlation;Semantics;Representation learning;Visualization;Task analysis;Fans;Micro-video;multi-label classification;multimodal representation},doi={10.1109/TMM.2024.3405724}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Kai Zhang, Xianyi Liu, Yun Li, Yu Liu, Yuting Su. Dual Preference Perception Network for Fashion Recommendation in the Social Internet of Things. IEEE Internet of Things Journal, 2024, 11(5): 7893-7903 
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10286302">pdf</a>] 
                                    [<a href=" https://github.com/KaiZhang1228/DP2Net">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10286302,author={Jing, Peiguang and Zhang, Kai and Liu, Xianyi and Li, Yun and Liu, Yu and Su, Yuting}, journal={IEEE Internet of Things Journal}, title={Dual Preference Perception Network for Fashion Recommendation in Social Internet of Things}, year={2024},volume={11},number={5},pages={7893-7903},keywords={Social Internet of Things;Smart homes;Task analysis;Transformers;Motion pictures;Clothing;Graph neural networks;Fashion recommendation;heterogeneous graph;preference perception;Social Internet of Things (SIoT)},doi={10.1109/JIOT.2023.3319386}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Kai Cui, Jing Zhang, Yun Li*, Yuting Su. Multimodal High-order Relationship Inference Network for Fashion Compatibility Modeling in Internet of Multimedia Things[J]. IEEE Internet of Things Journal, 2024,11(1): 353–365
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10149524">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10149524,author={Jing, Peiguang and Cui, Kai and Zhang, Jing and Li, Yun and Su, Yuting}, journal={IEEE Internet of Things Journal}, title={Multimodal High-Order Relationship Inference Network for Fashion Compatibility Modeling in Internet of Multimedia Things}, year={2024},volume={11},number={1}, pages={353-365},keywords={Visualization;Task analysis;Internet of Things;Correlation;Semantics;Feature extraction;Multimedia communication;Fashion compatibility modeling (FCM);high-order relationship inference;Internet of Multimedia Things (IoMT);multimodal representation learning},doi={10.1109/JIOT.2023.3285601}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Xiaoyu Liu, Xuehui Wang, Yuting Su. Deep Matrix Factorization with Complementary Semantic Aggregation for Micro-Video Multi-Label Classification. IEEE Signal Processing Letters, 2024,31: 1685-1689
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10345640">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10345640,author={Jing, Peiguang and Liu, Xiaoyu and Wang, Xuehui and Su, Yuting}, journal={IEEE Signal Processing Letters}, title={Deep Matrix Factorization With Complementary Semantic Aggregation for Micro-Video Multi-Label Classification}, year={2024},volume={31},number={},pages={1685-1689},keywords={Semantics;Decoding;Feature extraction;Correlation;Robustness;Matrix decomposition;Termination of employment;Micro-video;multi-label classification;deep matrix factorization;low-rank representation},doi={10.1109/LSP.2023.3340097}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Fugui Fan, Yuting Su, Liqiang Nie, <b>Peiguang Jing*</b>, Daozheng Hong, Yu Liu. Dual-domain Aligned Deep Hierarchical Matrix Factorization Method for Micro-video Multi-label Classification. IEEE Transactions on Multimedia, 2024,26: 2598-2607
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10214127">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10214127,author={Fan, Fugui and Su, Yuting and Nie, Liqiang and Jing, Peiguang and Hong, Daozheng and Liu, Yu},journal={IEEE Transactions on Multimedia}, title={Dual-Domain Aligned Deep Hierarchical Matrix Factorization Method for Micro-Video Multi-Label Classification}, year={2024},volume={26},number={},pages={2598-2607},keywords={Semantics;Correlation;Visualization;Task analysis;Matrix decomposition;Estimation;Training;Micro-video;multi-label classification;semantic alignment;deep matrix factorization},doi={10.1109/TMM.2023.3301224}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Fugui Fan, <b>Peiguang Jing</b>, Liqiang Nie, Haoyu Gu, Yuting Su*. SADCMF: Self-Attentive Deep Consistent Matrix Factorization for Micro-Video Multi-Label Classification. IEEE Transactions on Multimedia, 2024, 26: 10331-10341
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10540235">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10540235,author={Fan, Fugui and Jing, Peiguang and Nie, Liqiang and Gu, Haoyu and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={SADCMF: Self-Attentive Deep Consistent Matrix Factorization for Micro-Video Multi-Label Classification}, year={2024},volume={26}, number={},pages={10331-10341},keywords={Correlation;Semantics;Task analysis;Collaboration;Visualization;Social networking (online);Fans;Micro-video;multi-label classification;deep matrix factorization;self-attention},doi={10.1109/TMM.2024.3406196}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing#</b>, Xianyi Liu#, Lijuan Zhang, Yun Li, Yu Liu, Yuting Su. Multimodal Attentive Representation Learning for Micro-video Multi-label Classification. ACM Transactions on Multimedia Computing, Communications and Applications. 2024, 20(6): 182, pp 1-23.
                                    [<a href="https://dl.acm.org/doi/pdf/10.1145/3643888">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{10.1145/3643888,author = {Jing, Peiguang and Liu, Xianyi and Zhang, Lijuan and Li, Yun and Liu, Yu and Su, Yuting},title = {Multimodal Attentive Representation Learning for Micro-video Multi-label Classification},year = {2024},issue_date = {June 2024},publisher = {Association for Computing Machinery},address = {New York, NY, USA},volume = {20},number = {6},issn = {1551-6857},url = {https://doi.org/10.1145/3643888},doi = {10.1145/3643888},abstract = {As one of the representative types of user-generated contents (UGCs) in social platforms, micro-videos have been becoming popular in our daily life. Although micro-videos naturally exhibit multimodal features that are rich enough to support representation learning, the complex correlations across modalities render valuable information difficult to integrate. In this paper, we introduced a multimodal attentive representation network (MARNET) to learn complete and robust representations to benefit micro-video multi-label classification. To address the commonly missing modality issue, we presented a multimodal information aggregation mechanism module to integrate multimodal information, where latent common representations are obtained by modeling the complementarity and consistency in terms of visual-centered modality groupings instead of single modalities. For the label correlation issue, we designed an attentive graph neural network module to adaptively learn the correlation matrix and representations of labels for better compatibility with training data. In addition, a cross-modal multi-head attention module is developed to make the learned common representations label-aware for multi-label classification. Experiments conducted on two micro-video datasets demonstrate the superior performance of MARNET compared with state-of-the-art methods.},journal = {ACM Trans. Multimedia Comput. Commun. Appl.},month = mar,articleno = {182},numpages = {23},keywords = {Micro-video, multimodal representations, multi-label, graph network}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Fangyu Zuo, <b>Peiguang Jing</b>, Jinglin Sun, Jizhong Duan, Yong Ji, Yu Liu. Deep Learning-based Eye-Tracking Analysis for Diagnosis of Alzheimer's Disease Using 3D Comprehensive Visual Stimuli, IEEE Journal of Biomedical and Health Informatics,2024,28(5): 2781-2793
                                    [<a href="https://arxiv.org/pdf/2303.06868">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10433663,author={Zuo, Fangyu and Jing, Peiguang and Sun, Jinglin and Duan, Jizhong and Ji, Yong and Liu, Yu},journal={IEEE Journal of Biomedical and Health Informatics}, title={Deep Learning-Based Eye-Tracking Analysis for Diagnosis of Alzheimer’s Disease Using 3D Comprehensive Visual Stimuli},  year={2024},volume={28}, number={5},pages={2781-2793},keywords={Visualization;Gaze tracking;Biomarkers;Three-dimensional displays;Medical diagnostic imaging;Alzheimers disease;Convolutional neural networks;Deep learning;Heat maps;Alzheimers disease (AD);eye-tracking;visual attention;convolutional neural network},doi={10.1109/JBHI.2024.3365172}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yun Li, Xianyi Liu, Lijuan Zhang, Haoyu Tian, <b>Peiguang Jing</b>. Multimodal semantic enhanced representation network for micro-video event detection. Knowledge-Based Systems, 2024, 301: 112255.
                                    [<a href="https://www.sciencedirect.com/science/article/pii/S095070512400889X">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{LI2024112255,title = {Multimodal semantic enhanced representation network for micro-video event detection},journal = {Knowledge-Based Systems},volume = {301},pages = {112255},year = {2024},issn = {0950-7051},doi = {https://doi.org/10.1016/j.knosys.2024.112255},url = {https://www.sciencedirect.com/science/article/pii/S095070512400889X},author = {Yun Li and Xianyi Liu and Lijuan Zhang and Haoyu Tian and Peiguang Jing},keywords = {Micro-video, Event detection, Multimodal representation, Visual concepts},abstract = {Currently, micro-videos have gained widespread acceptance as a prominent form of user-generated content across various social media platforms. Accurate event analysis of micro-videos can greatly enhance the many diverse social media platforms applications. Although some studies have shown promising results from a multimodal perspective, there is still a challenge in extracting informative cues from inaccurate modalities, particularly for text modality that is prone to inaccuracies and noise. In this paper, we propose a multimodal semantically enhanced representation network (MSERN) for micro-video event detection. To better address inaccurate and noisy text sentences, we first extract visual concepts in the form of adjective-noun pairs (ANPs), through a fine-grained common representation module, to complement the textual descriptions. To maximize the acquisition of modality-specific cues from both visual and textual modalities, we then implement a coarse-grained private representation module to ensure that private representations encompass unique facets of the modalities beyond the common perspective. Finally, because two modules will collaborate, the fine-grained common and coarse-grained private representations are integrated to ensure a reinforced micro-video representation. We evaluate our proposed method on a micro-video event detection dataset and the experimental results show a superior performance compared to the state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yuting Su, Weikang Wang, Jing Liu, <b>Peiguang Jing</b>. Collaborative spatial-temporal video salient object detection with cross attention transformer. Signal Processing, 2024, 224: 109612.
                                    [<a href="https://pdf.sciencedirectassets.com/271605/1-s2.0-S0165168424X00079/1-s2.0-S0165168424002317/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEND%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIELkbnFFLwE6Ug%2B6gp1PEoTX9dGwMEf369RGSbWp1KYtAiEAjzasGV6xOqiyvdYXMXK0kBrrufZIsO8u6fGGgaRVkUMqswUIWBAFGgwwNTkwMDM1NDY4NjUiDN1OPuoKo7bp6DofECqQBZk1WuhFJ0j3Dwov2LjVZjYlAUS4R%2FI8kw0u2HzhFI7kbW8AGRo9NWoeC3ilGNhLhv3QBrWGHRVDvZtLQU9IIaogEA3I5MU4iIDhMYN79hKqPXtOU209pRPN9Il04mRxrXRHDP%2F6mjCMg2OtNtDkM9tfS9aHpUqFkh4rj40C7g9XGO9%2BbRrHrmYdBnbwm%2BI2k85MBISrSbOe1uEBwsmrJOHsWygF5jQ9B%2BE1R8Pa37oYRKP5okTP6%2F9m5pJfYoLprKpWaY9suAJiR%2BYPJz%2FoGdE4LXFgGrDlEm%2F9%2FzYEGfNWxY5K1MZfhlRZwSqeEIjvuvRJzdkXZ6%2F%2BpK3ve%2F8FI44FXS1A4o2FGWFhhNDyky3UaOspzUTUivudCi0uh9sha8e2pRLQUGE%2BU2M%2B8np8FOJ9lTeVR7pNSwcRfJCcwkIo%2FRBHloaeJN9m1hEEcr%2BwXhdTxORtb8qHAaeCWrkjhkItbqZJx9Gm0kwA9t7E6j7eUQ6otcniXJWB5nXxeo2nRq6hAO%2BgcbsKUcbav8pVLp2riPp%2Bdr1lFwPYeLCMeqTzNLCn6%2B2ZaGqprSNBu%2BbSJHf6213l6ubbD3L90qHo8qxFV%2BvOh0t0hcnoU2615GD2pX9ji53Scn11xrsTXEXtX0GkLMmCKQw1EbLZcTvvTwNhzX5wQr0cNRQh%2Bd%2F0BwrsdtKDmFaiAsYhCve9HsmsycXK%2FGVj2amRbYdb%2FOrIzo8glK7%2B1EckXcD2VcYY9vN2LF%2FcUPKa3vNUMf7KWCZrs%2F3UqlJagmvg3DvsrcqqHssZqS4m982DqHb4wqfysA16AlHTZoxVQ4kzp7WtbhGeurbv86GOqdN3u6sOOyDDeFTd8sdq6vZpJd%2BfKLNUVW0fMOTQgsAGOrEBb5UJtoMLFmSkjIgnRASGY8LPLz%2F79t1nCQaj8FHd%2F4531QBxAl%2F%2B17tVI5%2B53189Yvf3qDww%2BQcwOUVUJcIbS0pWslTzn0I2u5ptHCeY25gBqHZ%2BOCMcUxoskiZTz8WEKe8zDagUCIS61csSqLfOQpn%2FoYAztKuZqKoLRCsy3cmz39R8SoGCC32UqnJhSvK81d8JRNvzYqcScCiYbRz8MyVpPUV4kpfQwnYkV54UWaCu&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T075406Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7AMCNL2K%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=412c33d6d11a8784f803d9e3819b64bf3e371118d7b700469200f02bb885a125&hash=446d9b71178609e40941a3c604cfc031fe468dee01d01319e356e3e58c07476c&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0165168424002317&tid=spdf-50c065e2-a499-4ae5-8757-1fa1da381470&sid=df1a433c23951642440ae8189ae06de94898gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57065a05540a&rr=931a5a3e1a8a0f2c&cc=cn">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{SU2024109612,title = {Collaborative spatial-temporal video salient object detection with cross attention transformer},journal = {Signal Processing},volume = {224},pages = {109612},year = {2024},issn = {0165-1684},doi = {https://doi.org/10.1016/j.sigpro.2024.109612},url = {https://www.sciencedirect.com/science/article/pii/S0165168424002317},author = {Yuting Su and Weikang Wang and Jing Liu and Peiguang Jing},keywords = {Video salient object detection, Siamese feature extractor, Deep level set, Vision transformer},abstract = {Noticing the effectiveness of explicit motion encoding in optical flow, a variety of recent works employ flow-guided two-branch structures to handle the video salient object detection (VSOD) task. However, most of them ignore the semantic gap between the moving objects and the salient objects. Besides, the long-range dependency is not sufficiently investigated due to the local convolution operation. To tackle these problems, in this paper, we propose a novel collaborative spatio-temporal feature fusion method based on the cross-attention transformer for VSOD. A Siamese feature extractor is introduced to jointly optimize the motion and static feature extraction with reduced network parameters. The deep level set method is employed to segment the moving objects from optical flow, and thus the dependence on saliency groundtruth is greatly reduced. Moreover, a cross-attention transformer is proposed to jointly optimize and fuse the static and motion features, as well as investigate long-range dependency. Experimental results on six commonly used video salient object datasets demonstrate that our method achieves state-of-the-art performance among all VSOD algorithms.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Huaiyan Jiang, Han Wang, T Pan, Y Liu, <b>P Jing</b>, Y Liu. Mobile Application and Machine Learning-Driven Scheme for Intelligent Diabetes Progression Analysis and Management Using Multiple Risk Factors. Bioengineering, 2024, 11(11): 1053.
                                    [<a href="https://www.mdpi.com/2306-5354/11/11/1053">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@Article{bioengineering11111053,AUTHOR = {Jiang, Huaiyan and Wang, Han and Pan, Ting and Liu, Yuhang and Jing, Peiguang and Liu, Yu},TITLE = {Mobile Application and Machine Learning-Driven Scheme for Intelligent Diabetes Progression Analysis and Management Using Multiple Risk Factors},JOURNAL = {Bioengineering},VOLUME = {11},YEAR = {2024},NUMBER = {11},ARTICLE-NUMBER = {1053},URL = {https://www.mdpi.com/2306-5354/11/11/1053},PubMedID = {39593713},ISSN = {2306-5354},ABSTRACT = {Diabetes mellitus is a chronic disease that affects over 500 million people worldwide, necessitating personalized health management programs for effective long-term control. Among the various biomarkers, glycated hemoglobin (HbA1c) is a crucial indicator for monitoring long-term blood glucose levels and assessing diabetes progression. This study introduces an innovative approach to diabetes management by integrating a mobile application and machine learning. We designed and implemented an intelligent application capable of collecting comprehensive data from diabetic patients, creating a novel diabetes dataset named DiabMini with 127 features of 88 instances, including medical information, personal information, and detailed nutrient intake and lifestyle. Leveraging the DiabMini, we focused the analysis on HbA1c dynamics due to their clinical significance in tracking diabetes progression. We developed a stacking model combining eXtreme Gradient Boosting (XGBoost), Support Vector Classifier (SVC), Extra Trees (ET), and K-Nearest Neighbors (KNN) to explore the impact of various influencing factors on HbA1c dynamics, which achieved a classification accuracy of 94.23%. Additionally, we applied SHapley Additive exPlanations (SHAP) to visualize the contributions of risk factors to HbA1c dynamics, thus clarifying the differential impacts of these factors on diabetes progression. In conclusion, this study demonstrates the potential of integrating mobile health applications with machine learning to enhance personalized diabetes management.},DOI = {10.3390/bioengineering11111053}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Fugui Fan, Yuting Su, Yun Liu*, <b>Peiguang Jing</b>, Kaihua Qu, Yu Liu. Multimodal deep hierarchical semantic-aligned matrix factorization method for micro-video multi-label classification. Information Processing & Management, 2024, 61(5): 103798.
                                    [<a href="https://pdf.sciencedirectassets.com/271647/1-s2.0-S0306457324X00035/1-s2.0-S0306457324001572/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIHRkeXEKS%2BYjn8nr6EPZhx4GfkFnlXw8IEQacBMsAx6eAiEA%2BY%2FevYnfIzGdAiw%2BzOmUzVyMVuDANlX7VST57mCIjVMqswUIWhAFGgwwNTkwMDM1NDY4NjUiDJ8IoV9IjuVT70%2FazCqQBWaUFzIWX8F5E18WPCrLvG%2BRAKldkOCkd%2FsWrSm0gJK7u0yHT3slRdv0gdubVdOP9NoZWOZVBZyx7YH8sTswu0lYvsP9PPhzdJgBSO06ef%2FE22%2F3HJdkk51l8T3TLdFXOmHx3Q8U9LOM12tpD2JJNnn1%2BmImpvUkNKljW%2FEJVf8aKckx7IAyFYVpPbXp07l0u43qOvlUWry36B7YlnBubefxen90%2Fp9G4t4SM9TL5xn%2FhNzc1Blvvh1XuFPut9PvvpQjdSeou4wyTb%2B2U23Yi4ib20wx3OoWEd0%2FqfZIg9G0RcFuYQoMXUGpmocQlAXxWWFIdMWpVIYRuA1e2ItTo20mP8fxv%2F09GK9WsNt1%2FuGdzMFivMQM%2Bw771l442sU1vb1o1HXRPpQg29QkvedGXg0rfR%2Bel99uGS%2FoH2Do22jYiaoWfSkP0SebY%2B6xuFEDYmrGsJ3hkt992vFfDj5kvurA130TW9XsKqwPtBwt3K0EzCFmqskbz16UxM8mOOnaok3750VLqKfm0SVaV1ZaVQIwj5o8uFLTNYfQ7S9oKK1YVFpFfSpPQOOw6USIo%2FwpP%2Byvv9iTzn7womcDpGkM3%2FZr9O9S6l%2BaC9SPG6waUx048Otm6LBZdH1xK87emKzGx5BHMsv8RKE8GoxJkzopRS0gzqk3S%2B62aP3HmTfOkG2ABN%2F%2BNBX1%2FcvG96PsBa0QXhaKrPvX4%2Fzk3fIf%2FCY6y55uydh%2BFoX%2F3J4nLMXefzirskEyEmPB3pVjc2IqasjExAnOtXNtymg6u%2BvogHSmrPFEAPQuY5Q1a84kLxHImpkSeJQVHoTAsSfcTyD1I6hMgWJwjmEM3Bht5mM2Bmp924W0143d2s1iHvgqZSRbTA7fMNX2gsAGOrEBEzvtD%2BeUMbgNF1Nb69RvHPvceGi6bwspUuLPDmrMRcy81%2F5axwSCEngJpJuPcIS6Y27BJFi%2BVqXCFUAd4VBQ4jfnCutDfxCyOmxD1AUk0sMjW5HlMpQr9RtHoPfsVT%2Bp5bcLxxhuNuBtfVEGtSRcsRaSfWSFPWIa32oHumd1SZeUaFFH5ZbYIBxs7D4KqNb%2F9TFGDC8YQHUgmOSIAUUjq9xnnNF0Ce3DwLvfk8q0ArWh&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T084616Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6IGWISCJ%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=2974e14487ac500149e3d7cd6d472ef46e95b15c0c52ab277837965e18b2bf0e&hash=eba5b4f808aec8d691ea93d05a95ef07d559b7c246051954c1ebaafdac8704f3&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0306457324001572&tid=spdf-9b3bfb43-a843-47a2-9d23-9024f23d03b5&sid=df1a433c23951642440ae8189ae06de94898gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57060e52065a&rr=931aa6a528dfe2dd&cc=cn">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{FAN2024103798,title = {Multimodal deep hierarchical semantic-aligned matrix factorization method for micro-video multi-label classification},journal = {Information Processing & Management}volume = {61},number = {5},pages = {103798},year = {2024},issn = {0306-4573},doi = {https://doi.org/10.1016/j.ipm.2024.103798},url = {https://www.sciencedirect.com/science/article/pii/S0306457324001572},author = {Fugui Fan and Yuting Su and Yun Liu and Peiguang Jing and Kaihua Qu and Yu Liu},keywords = {Micro-video understanding, Multi-label learning, Multimodal learning, Low-rank learning, Deep matrix factorization},abstract = {As one of the typical formats of prevalent user-generated content in social media platforms, micro-videos inherently incorporate multimodal characteristics associated with a group of label concepts. However, existing methods generally explore the consensus features aggregated from all modalities to train a final multi-label predictor, while overlooking fine-grained semantic dependencies between modality and label domains. To address this problem, we present a novel multimodal deep hierarchical semantic-aligned matrix factorization (DHSAMF) method, which is devoted to bridging the dual-domain semantic discrepancies and the inter-modal heterogeneity gap for solving the multi-label classification task of micro-videos. Specifically, we utilize deep matrix factorization to individually explore the hierarchical modality-specific representations. A series of semantic embeddings is introduced to facilitate latent semantic interactions between modality-specific representations and label features in a layerwise manner. To further improve the representation ability of each modality, we leverage underlying correlation structures among instances to adequately mine intra-modal complementary attributes, and maximize the inter-modal alignment by aggregating consensus attributes in an optimal permutation. The experimental results conducted on the MTSVRC and VidOR datasets have demonstrated that our DHSAMF outperforms other state-of-the-art methods by nearly 3% and 4% improvements in terms of the AP metric.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Jing Liu, Lele Sun, Weizhi Nie, <b>Peiguang Jing</b>, Yuting Su. Graph Disentangled Contrastive Learning with Personalized Transfer for Cross-Domain Recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(8): 8769-8777.
                                    [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28723">pdf</a>]              
                                </p>
                                </li>   

                            </ol>
                        </div>


                        <div id="2023" class="paper-list">
                            <ol>

                                <li><p align="justify"> Wei Lu, Jiaxin Lin, <b>Peiguang Jing*</b>, Yuting Su. A Multimodal Aggregation Network With Serial Self-Attention Mechanism for Micro-Video Multi-Label Classification. IEEE Signal Processing Letters, 2023, 30: 60-64.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10032700">pdf</a>] 
                                    [<a href=" https://github.com/peiguangjing/MANET">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10032700,author={Lu, Wei and Lin, Jiaxin and Jing, Peiguang and Su, Yuting}, journal={IEEE Signal Processing Letters}, title={A Multimodal Aggregation Network With Serial Self-Attention Mechanism for Micro-Video Multi-Label Classification}, year={2023},volume={30}, number={},pages={60-64},keywords={Feature extraction;Visualization;Correlation;Trajectory;Task analysis;Mobile ad hoc networks;Measurement;Micro-video;multi-label classification;multimodal;self-attention},doi={10.1109/LSP.2023.3240889}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yuting Su, Wei Zhao, <b>Peiguang Jing*</b>, Liqiang Nie. Exploiting Low-rank Latent Gaussian Graphical Model Estimation for Visual Sentiment Distribution. IEEE Transactions on Multimedia, 2023, 25: 1243-1255
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9672699">pdf</a>] 
                                    [<a href="https://github.com/peiguangjing/LGGME">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9672699,author={Su, Yuting and Zhao, Wei and Jing, Peiguang and Nie, Liqiang},journal={IEEE Transactions on Multimedia}, title={Exploiting Low-Rank Latent Gaussian Graphical Model Estimation for Visual Sentiment Distributions}, year={2023},volume={25}, number={},pages={1243-1255},keywords={Correlation;Visualization;Graphical models;Covariance matrices;Multivariate regression;Estimation;Sentiment analysis;Gaussian graphical model;low-rank representation;visual sentiment analysis},doi={10.1109/TMM.2022.3140892}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Wei Lu, Desheng Li, Liqiang Nie, <b>Peiguang Jing*</b>, Yuting Su. Learning Dual Low-rank Representation for Multi-label Micro-video Classification. IEEE Transactions on Multimedia, 2023, 25:77-89
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9585369">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9585369,author={Lu, Wei and Li, Desheng and Nie, Liqiang and Jing, Peiguang and Su, Yuting}, journal={IEEE Transactions on Multimedia}, title={Learning Dual Low-Rank Representation for Multi-Label Micro-Video Classification}, year={2023},volume={25},number={},pages={77-89},keywords={Correlation;Matrix decomposition;Task analysis;Semantics;Dictionaries;Estimation;Multimedia Web sites;Micro-video;multi-label classification;multi-modality;low-rank representation},doi={10.1109/TMM.2021.3121567}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Xianyi Liu, Ji Wang, Yinwei Wei, Liqiang Nie, Yuting Su. StyleEDL: Style-Guided High-order Attention Network for Image Emotion Distribution Learning. In Proceedings of ACM International Conference on Multimedia. 2023: 853-861.
                                    [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612040">pdf</a>] 
                                    [<a href="  https://github.com/liuxianyi/StyleEDL">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@inproceedings{10.1145/3581783.3612040,author = {Jing, Peiguang and Liu, Xianyi and Wang, Ji and Wei, Yinwei and Nie, Liqiang and Su, Yuting},title = {StyleEDL: Style-Guided High-order Attention Network for Image Emotion Distribution Learning},year = {2023},isbn = {9798400701085},publisher = {Association for Computing Machinery},address = {New York, NY, USA},url = {https://doi.org/10.1145/3581783.3612040},doi = {10.1145/3581783.3612040},abstract = {Emotion distribution learning has gained increasing attention with the tendency to express emotions through images. As for emotion ambiguity arising from humans subjectivity, substantial previous methods generally focused on learning appropriate representations from the holistic or significant part of images. However, they rarely consider establishing connections with the stylistic information although it can lead to a better understanding of images. In this paper, we propose a style-guided high-order attention network for image emotion distribution learning termed StyleEDL, which interactively learns stylistic-aware representations of images by exploring the hierarchical stylistic information of visual contents. Specifically, we consider exploring the intra- and inter-layer correlations among GRAM-based stylistic representations, and meanwhile exploit an adversary-constrained high-order attention mechanism to capture potential interactions between subtle visual parts. In addition, we introduce a stylistic graph convolutional network to dynamically generate the content-dependent emotion representations to benefit the final emotion distribution learning. Extensive experiments conducted on several benchmark datasets demonstrate the effectiveness of our proposed StyleEDL compared to state-of-the-art methods. The implementation is released at: https://github.com/liuxianyi/StyleEDL.},booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},pages = {853–861},numpages = {9},keywords = {emotion distribution learning, high-order, style-guided, stylistic gcn},location = {Ottawa ON, Canada},series = {MM 23}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Wei Lu, Yang Jiang, <b>Peiguang Jing</b>, Jinghui Chu, Fugui Fan. A Novel Channel Pruning Approach based on Local Attention and Global Ranking for CNN Model Compression. In Proceedings of IEEE International Conference on Multimedia and Expo, 2023: 1433-1438.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10220022">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@INPROCEEDINGS{10220022, author={Lu, Wei and Jiang, Yang and Jing, Peiguang and Chu, Jinghui and Fan, Fugui},booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)}, title={A Novel Channel Pruning Approach based on Local Attention and Global Ranking for CNN Model Compression}, year={2023},volume={},number={},pages={1433-1438},keywords={Training;Image coding;Correlation;Convolutional neural networks;Noise measurement;Model Compression;Channel Pruning;Noisy Weight;Attention Mechanism},doi={10.1109/ICME55011.2023.00248}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yun Li, Shuyi Liu, Xuejun Wang, <b>Peiguang Jing*</b>. Self-supervised deep partial adversarial network for micro-video multimodal classification. Information Sciences, 2023, 630: 356-369.
                                    [<a href="https://pdf.sciencedirectassets.com/271625/1-s2.0-S0020025523X00073/1-s2.0-S0020025522014177/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEND%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIE3rFT7uGzZ3zAhxXoJQMRYblePPUhB89cO1xMr66dcjAiBHUFfHwD7MEgaMsA7wliJOY%2BiAVxvFBiO7Yg5AJ%2FJuMSqzBQhYEAUaDDA1OTAwMzU0Njg2NSIMSBlmIlL9ISltJyeuKpAFNNBLSdDuSEhNJXcy6Fqni5PwBAhSuAChVkfxUZLrfvuZcQmb7u0UMpdsg1d0Al3zN4pqdSfA2OVwwIpKAJJ0W2g27ShJT3Ciy%2BHB92zTqMzEsO0GYHMKaU8J%2BS4hzadRGVybG1p8%2FF6AfOijzStNcwUuHMdDR5ERtin6Han6UY3j7e7TsEcro6Ubx0pMDuvMoQNTqjzMTFhnADiizVCa7e3vKVI8gic8K8os6cVCHyGgWCeR7LReczesLtqWxcAta0HYvWpe0L6X6aK%2BNGqDVm8kAY4d8qo3MmyHIdF75ye0fGhvxhBRCL2s1Jo8bkGUmVtaeF91oeSIaFVLKDBPexOVq4VwhtXE%2F5ngSq62z0ozuDMscf3g%2BP0f04GRvqkdhvuPvNvuiGwhgan%2Bz91JB0vkjpy4ZODpjCJx%2Bm8WPQCa%2F4NH%2BmFsge0QLdlnCk9P8IZ2OidRble31UHPHpEspEbyRP9PgCLGxN9eZuNgHOlbbHjWAaeJDys49iXgWK9g92NtdFoun%2BqHL73vyNJ9ZoMpyRqt5ki%2Fu0lpyacW0ZyL4b3y5c%2BpVjhc4n7uUCw45jiHyu5pypcGbJG78GBWVx8kiZ%2B2i1QN8zgstC9MYbsNUQUAlxS%2FcFzIocX9wGvtbX2IGWYJjlkeQ9N3pV2K%2FgmVCp6lKm0k6sZKB8iG9I%2Bb8j0tGqpRJHzMiNCgXpvpA71U0JYq1Z6ZZ%2FAalXtYsw9ZfOpQNFIcLmXg0Au5Rrcu4yA4M0shQXjyweV2Y992SPwymwilCDFo2vDF57R6dbuZHngZhoLpMWlqjuICV9MzTy7FsLBuMzCoBWKe4iZ%2BizXy%2F3xpSZ0MZ3IPGBiJoW8JwejKNlx%2FqffUMmSwC7Ywr9eCwAY6sgGV8ybzVfsG98jjbddINqkRqPAWY0GBmEDR6njKKTkiRcdUM233o%2B1uK90OniAL8EHg9R%2BmOdt73UgHgyo4W%2BEsSsvYfaxR%2FAy%2Bll5KiTs4sb0fsxIbF6h0ywj0QXyVUt%2Fb15d0QfyhYK1WxZmUKIZsZlZl%2FqQQbYnKUUnwav0S6WNc0gtKSZbp%2BejqBL5i3mjm3rxMVQF%2FVGQrlnVyVFSheP2Rp8nfwFJ7%2BDNsNDlGjxh%2B&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T083420Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYTS5KHDKB%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=64263521b5a40c8b8c29c771e84581589b9b53e51b5ede8def6dc3a05cdfbff2&hash=cdbb43ef129f030256be725fa54650751f7b9b8aaf8f67e4031d6174dc07ec0e&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0020025522014177&tid=spdf-f89fecc9-0276-494a-a794-b01d6bef3ca0&sid=df1a433c23951642440ae8189ae06de94898gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57065651550b&rr=931a952de836b852&cc=cn&kca=eyJrZXkiOiJNMzB0UFdISzlidmtVWGpBZTdxRWVzQUZPOHFqd2QwNnpaQ3lBaGk2UWpKL2I2b0Z0c2h0RDg0S0xVaXc1ZlNOUEVycFBraXUvMmhqUXlLTTNiSWxmZHNXelZlclh3YWhJY1crcDFxRjJCWDh0YzRRODNjYWxXck5veHpENHdWV0tmcVhxa1ptd0FVeHFFTWxZRTVIU09UYncydHVpTU5sUTgweEhLVy9nbHdZWXAxayIsIml2IjoiZWI2ZGMxYzdiYTRiM2QwMTMwYTc0ZDlmNzE3ZTc1MmEifQ==_1744878866938">pdf</a>] 
                                    [<a href=" https://github.com/peiguangjing/SDMAN">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@article{LI2023356,title = {Self-supervised deep partial adversarial network for micro-video multimodal classification},journal = {Information Sciences},volume = {630},pages = {356-369},year = {2023},issn = {0020-0255},doi = {https://doi.org/10.1016/j.ins.2022.11.111},url = {https://www.sciencedirect.com/science/article/pii/S0020025522014177},author = {Yun Li and Shuyi Liu and Xuejun Wang and Peiguang Jing},keywords = {Micro-video classification, Multimodal representation, Self-supervised learning},abstract = {Micro-videos have gained popularity on various social media platforms because they provide a great medium for real-time storytelling. Although micro-videos can be naturally characterized by several modalities, for situations with uncertain missing modalities, a flexible multimodal representation learning framework that integrates complementary and consistent information has been difficult to develop. To better deal with the issue regarding incomplete modalities in multimodal micro-video classification, in this paper, we propose a self-supervised deep multimodal adversarial network (SDMAN) to learn comprehensive and robust micro-video representations. Specifically, we first consider a parallel multi-head attention (MHA) encoding module that simultaneously learns the representations of complete and incomplete modality groupings. We then present a multimodal self-supervised cycle generative adversarial network module, in which multiple generative adversarial networks are explored to transfer the information obtained from the complete modality grouping to the incomplete modality groupings. As a result, complementarity and consistency are mutually promoted among the modalities. Furthermore, experiments conducted on a large-scale micro-video dataset demonstrate that the SDMAN performs better than the state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yunpeng Yin, Han Wang, Shuai Liu, <b>Peiguang Jing</b>, Yu Liu. Internet of Things for Diagnosis of Alzheimer’s Disease: A Multimodal Machine Learning Approach Based on Eye Movement Features. IEEE Internet of Things Journal, 2023,10(13): 11476-11485
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10044924">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10044924,author={Yin, Yunpeng and Wang, Han and Liu, Shuai and Sun, Jinglin and Jing, Peiguang and Liu, Yu},journal={IEEE Internet of Things Journal}, title={Internet of Things for Diagnosis of Alzheimer’s Disease: A Multimodal Machine Learning Approach Based on Eye Movement Features}, year={2023}, volume={10},number={13},pages={11476-11485},keywords={Feature extraction;Internet of Things;Diseases;Cloud computing;Sensitivity;Task analysis;Gaze tracking;Alzheimer’s disease (AD);eye tracking;feature fusion;Internet of Things (IoT);multimodal machine learning (MMML);oculomotor behavior},doi={10.1109/JIOT.2023.3245067}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Jinglin Sun, Zhipeng Wu, Han Wang, <b>Peiguang Jing</b> and Yu. Liu. A Novel Integrated Eye-Tracking System With Stereo Stimuli for 3-D Gaze Estimation, IEEE Transactions on Instrumentation and Measurement, vol. 72, pp. 1-15, 2023,
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9964289">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9964289,author={Sun, Jinglin and Wu, Zhipeng and Wang, Han and Jing, Peiguang and Liu, Yu}, journal={IEEE Transactions on Instrumentation and Measurement}, title={A Novel Integrated Eye-Tracking System With Stereo Stimuli for 3-D Gaze Estimation}, year={2023}, volume={72},number={},pages={1-15},keywords={Three-dimensional displays;Gaze tracking;Estimation;Cameras;Tracking;Head;Glass;Eye-tracking;gaze estimation;integrated system;multisource feature;stereo imaging},doi={10.1109/TIM.2022.3225009}}')">bib</a>]             
                                </p>
                                </li>

                            </ol>
                        </div>


                        <div id="2022" class="paper-list">
                            <ol>
                                
                                <li><p align="justify"> <b>Peiguang Jing</b>, Jing Zhang, Liqiang Nie, Shu Ye, Jing Liu, Yuting Su*. Tripartite Graph Regularized Latent Low-Rank Representation for Fashion Compatibility Prediction. IEEE Transactions on Multimedia, 2022, 24: 1277-1287. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9367009">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9367009,author={Jing, Peiguang and Zhang, Jing and Nie, Liqiang and Ye, Shu and Liu, Jing and Su, Yuting},journal={IEEE Transactions on Multimedia}, title={Tripartite Graph Regularized Latent Low-Rank Representation for Fashion Compatibility Prediction}, year={2022},volume={24},number={},pages={1277-1287},keywords={Feature extraction;Task analysis;Correlation;Semantics;Clothing;Visualization;Industries;Correlation;fashion compatibility;graph regularization;latent low-rank representation},doi={10.1109/TMM.2021.3062736}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Jinglin Sun, Yu Liu, Hao Wu, <b>Peiguang Jing*</b>, Yong Ji. A novel deep learning approach for diagnosing Alzheimer's disease based on eye-tracking data. Frontiers in Human Neuroscience, 2022,16: 01-12 
                                    [<a href="https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.972773/full ">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{10.3389/fnhum.2022.972773, AUTHOR={Sun, Jinglin  and Liu, Yu  and Wu, Hao  and Jing, Peiguang  and Ji, Yong },TITLE={A novel deep learning approach for diagnosing Alzheimers disease based on eye-tracking data}, JOURNAL={Frontiers in Human Neuroscience},    VOLUME={Volume 16 - 2022},YEAR={2022},URL={https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.972773},DOI={10.3389/fnhum.2022.972773},ISSN={1662-5161},ABSTRACT={<p>Eye-tracking technology has become a powerful tool for biomedical-related applications due to its simplicity of operation and low requirements on patient language skills. This study aims to use the machine-learning models and deep-learning networks to identify key features of eye movements in Alzheimers Disease (AD) under specific visual tasks, thereby facilitating computer-aided diagnosis of AD. Firstly, a three-dimensional (3D) visuospatial memory task is designed to provide participants with visual stimuli while their eye-movement data are recorded and used to build an eye-tracking dataset. Then, we propose a novel deep-learning-based model for identifying patients with Alzheimers Disease (PwAD) and healthy controls (HCs) based on the collected eye-movement data. The proposed model utilizes a nested autoencoder network to extract the eye-movement features from the generated fixation heatmaps and a weight adaptive network layer for the feature fusion, which can preserve as much useful information as possible for the final binary classification. To fully verify the performance of the proposed model, we also design two types of models based on traditional machine-learning and typical deep-learning for comparison. Furthermore, we have also done ablation experiments to verify the effectiveness of each module of the proposed network. Finally, these models are evaluated by four-fold cross-validation on the built eye-tracking dataset. The proposed model shows 85% average accuracy in AD recognition, outperforming machine-learning methods and other typical deep-learning networks.</p>}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Yuechen Shang, Liqiang Nie, Yuting Su*, Jing Liu, Meng Wang. Learning Low-rank Sparse Representations with Robust Relationship Inference for Image Memorability Prediction. IEEE Transactions on Multimedia, 2021, 23: 2259-2272 (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9141424">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9141424,author={Jing, Peiguang and Shang, Yuechen and Nie, Liqiang and Su, Yuting and Liu, Jing and Wang, Meng}, journal={IEEE Transactions on Multimedia}, title={Learning Low-Rank Sparse Representations With Robust Relationship Inference for Image Memorability Prediction}, year={2021},volume={23},number={},pages={2259-2272},keywords={Visualization;Robustness;Sparse matrices;Correlation;Predictive models;Task analysis;Adaptation models;Image memorability prediction;low-rank;sparse;relationship structure},doi={10.1109/TMM.2020.3009485}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Yuting Su*, Zhengnan Li, Liqiang Nie. Learning robust affinity graph representation for multi-view clustering. Information Sciences, 2021 544: 155-167. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://pdf.sciencedirectassets.com/271625/1-s2.0-S0020025520X00296/1-s2.0-S0020025520306575/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIDuJzN8UMX%2BkkoeaeyxodFOT%2BIfcToWWbswd2ueNsPNwAiBJRkzcg17CDTlUOBBNgNMtChdlQJ6nEEcg4BR6aw%2BAxCqzBQhcEAUaDDA1OTAwMzU0Njg2NSIMt4sE9JrKubhIANmVKpAFq74qhfMTknoS0%2F8jpisS6zKa%2BnEhIdauK%2Bh7C7JanVRLslMWq33BQOv%2Ffq4ZCOZNqrc7WBbc1QQ%2BFp%2Fb%2FL8xCjK3I6YjZiUQrZAvZaqi%2FSzVIRe7nTIVZ8gr1b2MqJrBlb2jYBcvDdZc%2FGLFedp7Oob1T1RxqzBysdMPBXWC%2FbiIPpR8W8GrTaxAVECZOL7DumunYxLEvxwL9p%2BRZL331gvK3M2gQwwtHstWRoE7xXNl%2FP0ZySettW17dQXGYvbfQ%2B8twq9LC3rS30dGogX5q4egth3BLpIIf3%2Bf90FyiRpofHjhiF3DqI%2BGum16KqFdO4fTXXXHFqQZkOfmY9T4x2ok2xKIQGwEQyUvLwN8bhGoI0hcq4e%2BNtuNh7DXbXwhSxSyS3F8n7FbAkjKnWC%2BTld5%2FudD0xuNtF3YNychG5HO%2B1XyHyZuBzfaIc1jGZx7RlRo1Th%2FPVQU5w9HlR0Xg4%2Fiy%2F9CVOn0PnE5ecofKJmbGZ3Pj0IY4X2yp5QUoJRUG63%2BLXnl8Kw4uACO7QacBHTXHxNOhwCLYcTFAGlEVVIryztyt4Eissz%2B2qQUkmTr31kPtt%2BYxW5zWsJU4DS1EXcB%2BmtBpwEQi0wxvq0BzIDFXkIX5lfXjj5lQzJO65t9c%2BBP7IuliGNtP%2Fcf%2BtmR%2F6mtSgyMsbIV4hfjxh7reZx2hSHz%2F2MR12HTiy%2BiSKwomRl3Bjl1tbtWM9RT4TepmR8LSZiyvR8bZyBQkOhETmIK8eZBioAzSBTfwAbgMyu1Y7DHxgo5dsLLd2k2gY%2FmGah8uVQJcPcbV0ZM%2BjUkMTXkIykCgXmPvuHXVgNQgP195IT48GNCgFtz1QR2OuRox3z4gtjpDkiF5ng8TaQ%2FtqYwma2DwAY6sgHQyNhF5IDg1xQTfx80x9DdiMytomPm2gL0G9SUDTKLvTi%2BKzhWM%2FOqNhw1z8M%2FvQKOqVVH5XuVjBCKUjxflmXkSJuGP8gPYmvaEmdAgB4sZqOl2aPjJGYYbjdBNk06XfN4TiWyLNGrmxa2VOEA%2FIW1gqmA9mkaxnxYZs9bFIFkvxq2ZKk%2F8zxy0wI5195UzR5khP26xQAY9z8jlDy5DCWvVLQpiG1tqfg9%2BXI4x4pWO25H&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T104045Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY5QZGX7PU%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0df94fbf1593d03c18ec03d4606c192b2f9d5c94d67515b2325b419fa9d0ce24&hash=ce91e4d921908aeadc50e052f267ee3a01402f180da0f7ee23a2e4a6997cd75e&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0020025520306575&tid=spdf-19dae603-0ee3-4c70-9baf-5d493b851468&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57055b015257&rr=931b4e58cb26e2f7&cc=cn&kca=eyJrZXkiOiIxZHgxTVFRUTJGV1lVQmtWb0pMVnBTbUhydHRYVWhadFFMSysvYVhnRFk4WUxoU3hTMldJSXowUTBwMVE4cVg0TjJVbHFpWHdIOHpYemJoSGJnZExKSFpkYk1VeEx2VXhjZHpDM28vQUZ4aVNhVU92RWNlcklxcTBmZXVBb1pFWGhtemdIOUs4eHdPZXlnMk1UU0dBTHdWY04zUnhnTEZjd3FDeUZTcnZ3bU1uS3BBPSIsIml2IjoiM2I1MzBiMGMzOGY1NWY2MzEzNDZlMzc1ZWNlN2U0ZWQifQ==_1744886449846">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{JING2021155,title = {Learning robust affinity graph representation for multi-view clustering},journal = {Information Sciences},volume = {544},pages = {155-167},year = {2021},issn = {0020-0255},doi = {https://doi.org/10.1016/j.ins.2020.06.068},url = {https://www.sciencedirect.com/science/article/pii/S0020025520306575},author = {Peiguang Jing and Yuting Su and Zhengnan Li and Liqiang Nie},keywords = {Multi-view clustering, Feature selection, Graph representation, Grassmann manifold},abstract = {Recently, an increasingly pervasive trend in real-word applications is that the data are collected from multiple sources or represented by multiple views. Owing to the powerful ability of affinity graph in capturing the structural relationships among samples, constructing a robust and meaningful affinity graph has been extensively studied, especially in spectral clustering tasks. However, conventional spectral clustering extended to multi-view scenarios cannot obtain the satisfactory performance due to the presence of noise and the heterogeneity among different views. In this paper, we propose a robust affinity graph learning framework to deal with this issue. First, we employ an improved feature selection algorithm that integrates the advantages of hypergraph embedding and sparse regression to select significant features such that more robust graph Laplacian matrices for various views on this basis can be constructed. Second, we model hypergraph Laplacians as points on a Grassmann manifold and propose a Consistent Affinity Graph Learning (CAGL) algorithm to fuse all views. CAGL aims to learn a latent common affinity matrix shared by all Laplacian matrices by taking both the clustering quality evaluation criterion and the view consistency loss into account. We also develop an alternating descent algorithm to optimize the objective function of CAGL. Experiments on five publicly available datasets demonstrate that our proposed method obtains promising results compared with state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yuting Su, Junyu Xu, Daozheng Hong, Fugui Fan, Jing Zhang*, <b>Peiguang Jing</b>. Deep low-rank matrix factorization with latent correlation estimation for micro-video multi-label classification. Information Sciences, 2021, 575: 587-598.
                                    [<a href="https://pdf.sciencedirectassets.com/271625/1-s2.0-S0020025521X00217/1-s2.0-S0020025521007118/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIGH3Mg3rrYvVf%2BbX062HR6vHqEH76XU%2FpDI%2Bk0olE6RPAiAYmehfd%2BEswZ6g4YJQcmgnNjUuBObgjlpD4tNcfnl%2BPSqzBQhbEAUaDDA1OTAwMzU0Njg2NSIMGj0wTFnXElCJPMJAKpAFdgvIfZcj8d9LMcDQgSvaaLNawFEwWebInf2nKLnqswdVQbUMNawe0wskYKySivy3BN4QPVmY69jXdJWsbzuDY6esBH8aFE%2BET%2FjtF8FYjPDxAnKFEggfpDnAJx1JLxk65TbVIm4YPPUtgX%2BTRl1y3YlKja0Yn6iWOfgPkomsgk1pabYXRCKU%2FFOMdLrtOUyetrOpllPMW9K7947mpCiG4EHaNO8Dw4vt7UQAR1xDzpErnQNaTzqYOQxV9RE4oSSHI2U49rgaZAQmdRUZaTErSLf6AP%2BXtRZDQ%2FfqVlmhQkTtRA41i5Guu2tv7UkE4sc7plsshY30DMh2iwl4YnmSZkQLT%2B1OR5UZx8m81a3K81mIXqm21K6gqdNFGfeQhwko5Fy642485D%2Fn9zWAAa5z90tHCAo7MRCAghElUmI%2B3XNVL0FH7SapuRUraOExWFxTWp97LaqBoAce63Q1kLKL2lbCLNRZetA%2FXsRYZcVVuntSTF0CDbkCtgja0c1Z8o%2BQHBWMilNsc4BMmFnmX98ABX5AxdW7P4%2Bic0eIY4VIZJxt0QQHP%2Bc%2FaosL%2FOhhQQUNXV8L1lMTRNRwqWMGbJA1K8GTCBxHbp9iW1bBaqrh93iuqD0PbPyu5%2BQfvfLVGLuUJVxOIKEbZqXkevbE3ytR1iYWyYMAhd7jn7Mm5B2WoDtca1cue3Veu%2BnqkO2bnMjGSOrCgVP6TEA9qX2HG0xf9XDcW4xs3vS6wwWCxyO8iLME9G%2FvPSFGvGZ7IWdx0TcUZObufLwLt2mpVZoRoTFbUtVq%2BldUW6UYJi6VknwnG4itauehlTbUiL8ozDLWNzB67glEQGtXfz%2Fcg9PHxkZFa7Zvj%2Bi1pnFEFetyqJCJVl4wzJqDwAY6sgGSV3t9aC9jqfSoy6BllsZXbnntI5kFkUi0M2eS6pI6jwwONhMvlotq1DF2Q7cH1MSB8ccOp6qoRz5c3ekoqa7V4t3W%2FSecLzwKzzj5E4EnU3cU5qGN4b0n%2F2TfuPqgK0lw%2BBvnyM0%2FpRv8shCtn51qvNVSfQWH%2BtGpDkTtBtalEDl7UYC7PJwtijvWuahpiXcc%2F%2Bm4mbnSg7IFqFyLT9xIYPQ8kDBbi4uabvoAy3dsW0Gs&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T104343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6MRHU7B7%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=a65eb6e3c0446fadcc6032249c68800b10a2706137164761b47aac528a95607a&hash=5ae0fb4740dda58252e041a35dde9bc4ebe48d58c3540c3b351a6df3b9b48bb2&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0020025521007118&tid=spdf-002b5cf0-0500-44c1-8612-316087e1276e&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57055a56055d&rr=931b52b2f950e2f7&cc=cn">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{SU2021587,title = {Deep low-rank matrix factorization with latent correlation estimation for micro-video multi-label classification},journal = {Information Sciences},volume = {575},pages = {587-598},year = {2021},issn = {0020-0255},doi = {https://doi.org/10.1016/j.ins.2021.07.021},url = {https://www.sciencedirect.com/science/article/pii/S0020025521007118},author = {Yuting Su and Junyu Xu and Daozheng Hong and Fugui Fan and Jing Zhang and Peiguang Jing},keywords = {Deep matrix factorization, Micro-video, Multi-label learning, Label correlation, Low-rank constraint},abstract = {Currently, micro-videos are becoming an increasingly prevailing form of user-generated contents (UGCs) on various social platforms. Several studies have been conducted to explore the semantics of micro-videos and the behavior of individuals for various tasks, such as venue categorization, popularity prediction, and personalized recommendation. However, few studies have been dedicated to solving micro-video multi-label classification. More importantly, learning intrinsic and robust feature representations for micro-videos is still a complicated and challenging problem. In this paper, we propose a deep matrix factorization with latent correlation estimation (DMFLCE) for micro-video multi-label classification. In DMFLCE, we develop a deep matrix factorization component constrained by a low-rank constraint to learn the lowest-rank representations for micro-videos and the intrinsic characterizations for latent attributes simultaneously. To explicitly exhibit the dependencies of the learned latent attributes and labels for improved classification performance, we construct two inverse covariance estimation components to automatically encode correlation patterns with respect to the latent attributes and labels. Experiments conducted on a publicly available large-scale micro-video dataset demonstrate the effectiveness of our proposed method compared with state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Jing Liu, Xin Wen, Weizhi Nie, Yuting Su, <b>Peiguang Jing</b>, Xiaokang Yang. Residual-guided multiscale fusion network for bit-depth enhancement. IEEE Transactions on Circuits and Systems for Video Technology, 2021, 32(5): 2773-2786.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9491068">pdf</a>] 
                                    [<a href=" https://github.com/TJUMMG/RMFNet">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9491068,author={Liu, Jing and Wen, Xin and Nie, Weizhi and Su, Yuting and Jing, Peiguang and Yang, Xiaokang},journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={Residual-Guided Multiscale Fusion Network for Bit-Depth Enhancement},  year={2022},volume={32},number={5},pages={2773-2786}, keywords={Radio frequency;Feature extraction;Task analysis;Image edge detection;Image restoration;Noise measurement;Spatial resolution;Bit-depth enhancement;multiscale architecture;shuffling operations;residual guidance;feature fusion},doi={10.1109/TCSVT.2021.3098707}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Fugui Fan, Yuting Su, <b>Peiguang Jing</b>, Wei Lu*. A dual rank-constrained filter pruning approach for convolutional neural networks. IEEE Signal Processing Letters, 2021, 28: 1734-1738.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9506816">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9506816,author={Fan, Fugui and Su, Yuting and Jing, Peiguang and Lu, Wei},journal={IEEE Signal Processing Letters}, title={A Dual Rank-Constrained Filter Pruning Approach for Convolutional Neural Networks}, year={2021},volume={28},number={},pages={1734-1738},keywords={Manifolds;Adaptation models;Correlation;Adaptive systems;Adaptive filters;Computer architecture;Information filters;Filter pruning;Grassmann manifold;adaptive affinity graph;low-rank;high-rank},doi={10.1109/LSP.2021.3101670}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Xiao Jin, <b>Peiguang Jing</b>, Jiesheng Wu, Jing Xu, Yuting Su. Visual Sentiment Classification via Low-rank Regularization and Label Relaxation. IEEE Transactions on Cognitive and Developmental Systems, 2021, 14 (4): 1678-1690.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9658319">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9658319,author={Jin, Xiao and Jing, Peiguang and Wu, Jiesheng and Xu, Jing and Su, Yuting},journal={IEEE Transactions on Cognitive and Developmental Systems}, title={Visual Sentiment Classification via Low-Rank Regularization and Label Relaxation}, year={2022},volume={14},number={4},pages={1678-1690}, keywords={Visualization;Sparse matrices;Feature extraction;Machine learning;Dictionaries;Learning systems;Classification algorithms;Sentiment analysis;Dictionary learning;low-rank regularization;subspace learning;visual sentiment classification},doi={10.1109/TCDS.2021.3135948}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Jinghui Chu, Jiawei Feng, <b>Peiguang Jing</b>, Wei Lu. Joint Co-Attention And Co-Reconstruction Representation Learning For One-Shot Object Detection. In Proceedings of IEEE International Conference on Image Processing, 2021: 2229-2233.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9506387">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@INPROCEEDINGS{9506387,author={Chu, Jinghui and Feng, Jiawei and Jing, Peiguang and Lu, Wei},booktitle={2021 IEEE International Conference on Image Processing (ICIP)}, title={Joint Co-Attention And Co-Reconstruction Representation Learning For One-Shot Object Detection}, year={2021},volume={},number={},pages={2229-2233},keywords={Training;Degradation;Correlation;Conferences;Object detection;Feature extraction;Proposals;Object detection;one-shot learning;high-order feature fusion;low-rank co-reconstruction},doi={10.1109/ICIP42928.2021.9506387}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify">	Wenjie Wang, Lingyu Duan, Hao Jiang, <b>Peiguang Jing</b>, Xuemeng Song, Liqiang Nie. Market2Dish: health-aware food recommendation. ACM Transactions on Multimedia Computing, Communications, and Applications, 2021, 17(1): 1-19.
                                    [<a href="https://dl.acm.org/doi/pdf/10.1145/3418211">pdf</a>] 
                                    [<a href=" https://github.com/WenjieWWJ/FoodRec">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@article{10.1145/3418211,author = {Wang, Wenjie and Duan, Ling-Yu and Jiang, Hao and Jing, Peiguang and Song, Xuemeng and Nie, Liqiang},title = {Market2Dish: Health-aware Food Recommendation},year = {2021},issue_date = {February 2021},publisher = {Association for Computing Machinery},address = {New York, NY, USA},volume = {17},number = {1},issn = {1551-6857},url = {https://doi.org/10.1145/3418211},doi = {10.1145/3418211},abstract = {With the rising incidence of some diseases, such as obesity and diabetes, the healthy diet is arousing increasing attention. However, most existing food-related research efforts focus on recipe retrieval, user-preference-based food recommendation, cooking assistance, or the nutrition and calorie estimation of dishes, ignoring the personalized health-aware food recommendation. Therefore, in this work, we present a personalized health-aware food recommendation scheme, namely, Market2Dish, mapping the ingredients displayed in the market to the healthy dishes eaten at home. The proposed scheme comprises three components, namely, recipe retrieval, user health profiling, and health-aware food recommendation. In particular, recipe retrieval aims to acquire the ingredients available to the users and then retrieve recipe candidates from a large-scale recipe dataset. User health profiling is to characterize the health conditions of users by capturing the textual health-related information crawled from social networks. Specifically, to solve the issue that the health-related information is extremely sparse, we incorporate a word-class interaction mechanism into the proposed deep model to learn the fine-grained correlations between the textual tweets and pre-defined health concepts. For the health-aware food recommendation, we present a novel category-aware hierarchical memory network–based recommender to learn the health-aware user-recipe interactions for better food recommendation. Moreover, extensive experiments demonstrate the effectiveness of the health-aware food recommendation scheme.},journal = {ACM Trans. Multimedia Comput. Commun. Appl.},month = apr,articleno = {33},numpages = {19},keywords = {User health profiling, health-aware food recommendation, recipe retrieval}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Shu Ye, Liqiang Nie, Jing Liu, Yuting Su*. Low-rank regularized multi-representation learning for fashion compatibility prediction. IEEE Transactions on Multimedia, 2020, 22(6): 1555-1566. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8853293">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8853293,author={Jing, Peiguang and Ye, Shu and Nie, Liqiang and Liu, Jing and Su, Yuting}, journal={IEEE Transactions on Multimedia}, title={Low-Rank Regularized Multi-Representation Learning for Fashion Compatibility Prediction}, year={2020},volume={22},number={6},pages={1555-1566},keywords={Sparse matrices;Matrix decomposition;Clothing;Feature extraction;Task analysis;Manifolds;Visualization;Image understanding;fashion compatibility;low-rank constraint;sparse representation;subspace learning},doi={10.1109/TMM.2019.2944749}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yuting Su, Daozheng Hong, Yang Li, <b>Peiguang Jing*</b>. Low-Rank Regularized Deep Collaborative Matrix Factorization for Micro-Video Multi-Label Classification. IEEE Signal Processing Letters, 2020, 27: 740-744. 
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9050869">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{9050869,author={Su, Yuting and Hong, Daozheng and Li, Yang and Jing, Peiguang}, journal={IEEE Signal Processing Letters}, title={Low-Rank Regularized Deep Collaborative Matrix Factorization for Micro-Video Multi-Label Classification}, year={2020},volume={27},number={},pages={740-744}, keywords={Matrix decomposition;Feature extraction;Collaboration;Covariance matrices;Task analysis;Correlation;Semantics;Micro-video;multi-label classification;deep matrix factorization;low-rank},doi={10.1109/LSP.2020.2983831}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Yuting Su*, Xiao Jin, Chengqian Zhang. High-Order Temporal Correlation Model Learning for Time-Series Prediction. IEEE Transactions on Cybernetics, 2019, 49(6): 2385-2397.(<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8359393">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8359393,author={Jing, Peiguang and Su, Yuting and Jin, Xiao and Zhang, Chengqian}, journal={IEEE Transactions on Cybernetics}, title={High-Order Temporal Correlation Model Learning for Time-Series Prediction}, year={2019},volume={49},number={6},pages={2385-2397},keywords={Tensile stress;Predictive models;Hidden Markov models;Data models;Time series analysis;Correlation;Analytical models;Autoregressive (AR);temporal correlation;tensor decomposition;time-series prediction},doi={10.1109/TCYB.2018.2832085}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Wei Lu, Fugui Fan, Jinghui Chu, <b>Peiguang Jing*</b>, Yuting Su. Wearable Computing for Internet of Things: A Discriminant Approach for Human Activity Recognition. IEEE Internet of Things Journal, 2019, 6(2): 2749-2759. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8480641">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8480641,author={Lu, Wei and Fan, Fugui and Chu, Jinghui and Jing, Peiguang and Yuting, Su}, journal={IEEE Internet of Things Journal}, title={Wearable Computing for Internet of Things: A Discriminant Approach for Human Activity Recognition}, year={2019},volume={6},number={2},pages={2749-2759},keywords={Feature extraction;Accelerometers;Internet of Things;Activity recognition;Medical services;Data mining;Transforms;Dimensionality reduction;human activity recognition (HAR);Internet of Things (IoT);S transform (ST);wearable computing},doi={10.1109/JIOT.2018.2873594}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Yuting Su, Liqiang Nie, Humin Gu, Jing Liu*, Meng Wang. A Framework of Joint Low-rank and Sparse Regression for Image Memorability Prediction. IEEE Transactions on Circuits and Systems for Video Technology, 2019,29(5): 1296-1309. (<i><strong>ESI高被引</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8353302">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8353302,author={Jing, Peiguang and Su, Yuting and Nie, Liqiang and Gu, Huimin and Liu, Jing and Wang, Meng}, journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={A Framework of Joint Low-Rank and Sparse Regression for Image Memorability Prediction}, year={2019},volume={29},number={5},pages={1296-1309},keywords={Sparse matrices;Visualization;Robustness;Matrix decomposition;Task analysis;Approximation algorithms;Heuristic algorithms;Image memorability prediction;low-rank;sparse regression;subspace learning},doi={10.1109/TCSVT.2018.2832095}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Yuting Su, Zhengnan Li, Jing Liu, Liqiang Nie. Low-rank regularized tensor discriminant representation for image set classification. Signal Processing, 2019, 156: 62-70.
                                    [<a href="https://pdf.sciencedirectassets.com/271605/1-s2.0-S0165168418X00115/1-s2.0-S0165168418303505/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQC8Cm5u9w1SFohjM6VkImmuvyUAT%2F3D0ECB%2FbAZHG8DvAIhAJ7kHay5lVmWKDfTbKiWTX6mPlgbz3OnXLr68QyRNfZnKrMFCFsQBRoMMDU5MDAzNTQ2ODY1Igwkmn3ZyxbWePRHfUgqkAVsILY3S2T4WcYi7gsAuTSVNTu58TUmSBGLEtC97oWTaYfu4NOuF9Sms6e9E1Ke%2FtpI5asyqT%2B0j5Mjf3crhVPRjs5HAGdkCfHd%2BcniWcCQ0e4cGkJhoIsvzZAIbW5sSbPpayGjsWZ653W%2B8yA5E2oIDKxHuIQ1ecuafGCyFkP%2BRCa6jYZxcC7SFElt7IcwGy%2FzmZQ%2FTjVtVFQNLCN3Y8LRxQs7IayXm%2FvBRPnDfu2amF0YsP2f6%2FxPtBKoXi%2BsRnxS%2BpPYGT1NPVDeDnczv5wnVCGK3%2BfpOseW10GV9Fudb3%2Ba18IElvolBVPXcylwuG3obpIjsS1ZxWo4h9tWBt4r%2BC7BPNO1nmhbcT1VA1WRplJgudSUcQIp0wiErrFhtENdJSeyyJCdvCznI7xHL1hD6NX3l4VzOvuz7I7U7uOJ5xUIrKSevFuk9FmhIH0bMk5GTWjBQmDHdljS7u%2FIyO4Rk9QeT%2BRYbnxYK1oo1ZKwvJaa6eBP7G2GDcVEKFVVX0JyJQx%2FqboEsy5P8j%2FYRNzwfh0hmekAsOhmQMIYbgVDCh%2BgtnydiIKxVPeesfsCSKJOBj71XHT52GYe5i7kUQZf88BqnA55XcqbNULqPdBxoM1cUmY%2BW2cUkzbJctA6MXCc3PSotA6y7eNcIWJPEA%2B8WciWpEbegu0wvyDJJQN61u34Zk3ERMonZqTa0uW9RaYLc00JtViL1m%2BRO%2BE0%2F3RU35gmcW%2FxM6zkDP7iagJwhuzvEI6gYPL2H9NGi%2Fg3C%2FD5aUFaOfpjMl0aCxGhOPfPeM4GkjU54YLbTfNeXlRPtkD295x%2FFRcSCXQkLcmRyJ7kLFnwk887RsMqK5%2BBFMtu3j9tv5pNnAqPwcJyWPJvSTD0noPABjqwAeRsJkELHdS7XRdmJjIN7sOT7w6oPHb9p33Fb822WidaCpziEZ18d6QM4NYuU%2BqswPn0RaTPsTLrU6wBhhkaGegEmPP3QVy17x4saJ5hz8ZTbrWruBw3HbZ2BHzPStduowZW1MFS0airPBh1HfnO67VQTPv2dXzO%2F981%2BIMoIJQSj2%2FdE3Z%2FdrIih%2BlKWwIfKoiEUmebGSz6IHgnJ66IS%2FhW4VS264HI6BIdg0%2FcYkJv&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T111427Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7HDTEJ6F%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=432ea077112a1c29e5c7fae0c026fcb802483d760df6ee3857bfa58e66dc2162&hash=0afb427a628043b2bdd455cb07aa33ee46caa5ed20ecd8bbfd4fbbf6c63f654e&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0165168418303505&tid=spdf-09e2ccd8-4907-48f4-9c4f-d533789cdf4c&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c570558020556&rr=931b7fb91d3df9ad&cc=cn&kca=eyJrZXkiOiJ5UndETlQ1Zm1OMWtiUFZUWXJWVzlOdEZ4SDFQR1VhVHZ4ZWpveGpLR3N1TzMxbXpHK2xkQkNnRUR4aWIvdmhMckpxaEYyUXRPbytJTFlQTUdHbll0WmdWTUx0SkRjakRCb0ExNk90bjRmSFRjT2xWNG9CMmg2T0ljSUQ1OHJXSVpLR2I5T1c0V0g4Q0RIRUY1SXJMTjlhYmFFU0xpbGVobUMrNytBRWw3QnFYd21FPSIsIml2IjoiZThhM2EyMWNhYzEyNjMzZDZlZjQxNGE5NzE3ZDIzNmQifQ==_1744888472266">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{JING201962,title = {Low-rank regularized tensor discriminant representation for image set classification},journal = {Signal Processing},volume = {156},pages = {62-70},year = {2019},issn = {0165-1684},doi = {https://doi.org/10.1016/j.sigpro.2018.10.018},url = {https://www.sciencedirect.com/science/article/pii/S0165168418303505},author = {Peiguang Jing and Yuting Su and Zhengnan Li and Jing Liu and Liqiang Nie},keywords = {Image set classification, Low-rank, Tensor discriminant representation, Grassmann manifold},abstract = {Although image set classification has attracted great attention in computer vision and pattern recognition communities, however, learning a compact and discriminative representation is still a challenge. In this paper, we present a novel tensor discriminant representation learning method to better solve the image classification task. Specifically, we first exploit the advantages of Grassmann manifold and tensor to model image sets as a high-order tensor. We then propose a transductive low-rank regularized tensor discriminant representation algorithm referred as LRRTDR to learn more intrinsic representations for image sets. Our proposed LRRTDR mainly contains two components: low-rank tensor embedding and discriminant graph embedding. The low-rank tensor embedding is to learn the lowest-rank representation from a low-dimensional subspace, which is spanned by a set of latent basis matrices. The discriminant graph embedding is to further enhance the discriminant ability of the learned representations under the graph embedding framework. To solve the optimization problem of LRRTDR, we develop an alternating direction scheme based on Iterative Shrinkage Thresholding Algorithm (ISTA). Experimental results on five publicly available datasets demonstrate that our proposed algorithm not only converges with few iterations, but also achieves better accuracy compared with state-of-the-art methods.}}')">bib</a>]             
                                </p>
                                </li>
                                
                                <li><p align="justify"> Jing Liu, Wanning Sun, Yuting Su, <b>Peiguang Jing*</b>, Xiaokang Yang. BE-CALF: bit-depth enhancement by concatenating all level features of DNN, IEEE Transactions on Image Processing, 2019, 28 (10), 4926-4940. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8713480">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8713480,author={Liu, Jing and Sun, Wanning and Su, Yuting and Jing, Peiguang and Yang, Xiaokang},journal={IEEE Transactions on Image Processing}, title={BE-CALF: Bit-Depth Enhancement by Concatenating All Level Features of DNN}, year={2019},volume={28},number={10},pages={4926-4940},keywords={Image reconstruction;Feature extraction;Image color analysis;Deep learning;Task analysis;Signal processing algorithms;Neural networks;Bit-depth enhancement;deep learning;convolutional neural network;high dynamic range imaging;skip connections},doi={10.1109/TIP.2019.2912294}}')">bib</a>]             
                                </p>
                                </li>
                                
                                <li><p align="justify"> Jing Liu, Pingping Liu, Yuting Su, <b>Peiguang Jing*</b>, Xiaokang Yang. Spatiotemporal Symmetric Convolutional Neural Network for Video Bit-Depth Enhancement, IEEE Transactions on Multimedia, 2019, 21(9),2397-2406.  (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8636159">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8636159,author={Liu, Jing and Liu, Pingping and Su, Yuting and Jing, Peiguang and Yang, Xiaokang}, journal={IEEE Transactions on Multimedia}, title={Spatiotemporal Symmetric Convolutional Neural Network for Video Bit-Depth Enhancement}, year={2019},volume={21},number={9},pages={2397-2406},keywords={Image resolution;Convolutional codes;Spatiotemporal phenomena;Dynamic range;Transforms;Distortion;Correlation;Video bit-depth enhancement;encoder-decoder network;spatiotemporal symmetry;convolutional neural networks;feature fusion},doi={10.1109/TMM.2019.2897909}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Xue Dong, Xuemeng Song, Fuli Feng, <b>Peiguang Jing</b>, Xin-Shun Xu, Liqiang Nie. Personalized Capsule Wardrobe Creation with Garment and User Modeling. In Proceedings of ACM International Conference on Multimedia, 2019: 302-310.
                                    [<a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3350905">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@inproceedings{10.1145/3343031.3350905,author = {Dong, Xue and Song, Xuemeng and Feng, Fuli and Jing, Peiguang and Xu, Xin-Shun and Nie, Liqiang},title = {Personalized Capsule Wardrobe Creation with Garment and User Modeling},year = {2019},isbn = {9781450368896},publisher = {Association for Computing Machinery},address = {New York, NY, USA},url = {https://doi.org/10.1145/3343031.3350905},doi = {10.1145/3343031.3350905},abstract = {Recent years have witnessed a growing trend of building the capsule wardrobe by minimizing and diversifying the garments in their messy wardrobes. Thanks to the recent advances in multimedia techniques, many researches have promoted the automatic creation of capsule wardrobes by the garment modeling. Nevertheless, most capsule wardrobes generated by existing methods fail to consider the user profile, including the user preferences, body shapes and consumption habits, which indeed largely affects the wardrobe creation. To this end, we introduce a combinatorial optimization-based personalized capsule wardrobe creation framework, named PCW-DC, which jointly integrates both garment modeling (textiti.e., wardrobe compatibility) and user modeling (textiti.e., preferences, body shapes). To justify our model, we construct a dataset, named bodyFashion, which consists of $116,532$ user-item purchase records on Amazon involving 11,784 users and 75,695 fashion items. Extensive experiments on bodyFashion have demonstrated the effectiveness of our proposed model. As a byproduct, we have released the codes and the data to facilitate the research community.},booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},pages = {302–310},numpages = {9},keywords = {user modeling, fashion analysis, compatibility learning},location = {Nice, France},series = {MM 19}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Yuting Su*, Liqiang Nie, Xu Bai, Jing Liu, Meng Wang. Low-rank Multi-view Embedding Learning for Micro-video Popularity Prediction. IEEE Transactions on Knowledge and Data Engineering, 2018, 30(8): 1519-1532. (<i><strong>ESI高被引</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8233154">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8233154,author={Jing, Peiguang and Su, Yuting and Nie, Liqiang and Bai, Xu and Liu, Jing and Wang, Meng},journal={IEEE Transactions on Knowledge and Data Engineering}, title={Low-Rank Multi-View Embedding Learning for Micro-Video Popularity Prediction}, year={2018}, volume={30},number={8},pages={1519-1532}, keywords={Videos;Feature extraction;Sparse matrices;Matrix decomposition;Noise measurement;Social network services;Low-rank learning;multi-view fusion;subspace learning;popularity prediction;micro-video analysis},doi={10.1109/TKDE.2017.2785784}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Anan Liu, Yingdi Shi, <b>Peiguang Jing*</b>, Jing Liu, Yuting Su. Structured Low-rank Inverse-covariance Estimation for Visual Sentiment Distribution Prediction. Signal Processing, 2018, 152: 206-216. 
                                    [<a href="https://pdf.sciencedirectassets.com/271605/1-s2.0-S0165168418X00073/1-s2.0-S0165168418301981/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQD4oIz5RlZgDdkt40B%2FdMVvunDpW3mXY%2FAvO97%2B6A62XQIhAKQv1cZasIPjxU3sI%2FumbknBenfrMyQoRY8OfdVV7N7AKrMFCFwQBRoMMDU5MDAzNTQ2ODY1IgzT3VJde88fjrm7CFoqkAWVcIJoFYKLGeTNa80MDS9BP4OXAjQPQrvEvPpQWB1dbvVgGkZSenIsutJFNc%2BECVIwFv29%2Fn52HHBjFBONqL2VoMu3d5mA%2BWx83OQJnE%2BFSpEz0S5fKQKH4mKdk8b4O4mzIzwIA0Sxh7FleaciTNWd4JPLjJOdVNswDGnXO5xFRz9JQGtLszj%2FwhC7VjPyLWNGatHLNtNl8HdsqNECVklXsyydK1xqh5SEmmIVFxUmrl7WK85p7sfnmH2mBzhNPe7OsYN7vnhtngC%2BkA80eXdBnMtEignbqBOT1LTF7sJMnBPogB2OBCEi%2BQ2hH%2F5FzMsU3CCcGrHH8ZqTPlIYHo82ltylPX8UGbHDkxFDbia8Z6X6UfDs3snL7UieXUdzvEOxP5PrXOfyDWCRy3fHlN9pgmxtL%2BcXeAZZOSFLS%2FVVIojH4EQdXqGnXqidKkjyO0oOhRzLH8DQj4A1lfEodrHc4hDATrkVhHgcUdwNZ91pAuloOLDqeWVTZKOotIWcOQ63f21BGn%2BdYYbTVJB8SzcSmhJnm7ZtGxeQVWkvXaMlxjTjogqrgD6sfzTLX13eif%2Fz1GD1J0qmL0WnKntS6ou8nSO6RO9mnk4c0v%2FQGOQTPnLZ%2B51MTwPs2%2BSiSTJa3EJeRXRjpOCWYy9qyxu5X7hi3x7Dszl2u7AndNmvfBVMGy%2BzouSak1biDFClDc9LQI1YnCftG%2BRBSi3kRmxPMrSVvYExUKV4hMV%2BeUnBvPyxwuNoumkHu0epA4iXzYSxkQ67g3iMB04ojChCTYLdljNUdh9jp%2BEEpc44wi6SFaJxiR9opOSU2zVJzLvZgaP7WzMOEPJWjSL9U6jH1GEqGk2okpH63ELLQ3u7dO%2FZy3byHDCLsIPABjqwASa6f93vmDJVwGfTI29Vhw0RN4Meu%2FB%2BLDFVgU1QUNofc8SXQ2QuPqJmdC%2FxMo4C%2FuYjeJldiY5%2FVsWlm9KaRlpkGJM1Vvo7R6K2fBSHdYllueZmZY9GWd92kje28viJo5bImrvtspPYbVw06dZMhK6r63vQukC5%2Bfa0HWzMFAGVWB02tgtvaxb8322LarsuDwBXq1nP9AspeAV82sGlAumIzxhmlO4wPZabi9DiEMzH&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T114413Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYW3ZNN2BF%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0feca1d76919b9f923acebe4e523492d802bc316c02f3ebabaa4f8b55ff5d45c&hash=3d8188b755d5cc3396deb080fdebebeb0d8ba2af3046499a332d07a434bde3f6&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0165168418301981&tid=spdf-1db732c0-725b-48d6-b063-fe82a162004e&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57050e06525d&rr=931bab52d8fbdd36&cc=cn">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{LIU2018206,title = {Structured low-rank inverse-covariance estimation for visual sentiment distribution prediction},journal = {Signal Processing},volume = {152},pages = {206-216},year = {2018},issn = {0165-1684},doi = {https://doi.org/10.1016/j.sigpro.2018.06.001},url = {https://www.sciencedirect.com/science/article/pii/S0165168418301981},author = {Anan Liu and Yingdi Shi and Peiguang Jing and Jing Liu and Yuting Su},keywords = {Image sentiment, Label distribution learning, Structured sparsity, Low-rank},abstract = {Visual sentiment analysis has aroused considerable attention with the increasing tendency of expressing sentiments via images. Most previous studies mainly focus on predicting the most dominant sentiment categories of images while neglecting the sentiment ambiguity problem caused by the fact that the image sentiments elicited from viewers are very subjective and different. To tackle this problem, many research efforts have been devoted to visual sentiment distribution prediction, in which an image is characterized by a distribution over a set of sentiment labels rather than a single label or multiple labels. In this paper, we propose a structured low-rank inverse-covariance estimation algorithm for visual sentiment distribution prediction. The proposed model incorporates low-rank and inverse-covariance regularization terms into a unified framework to learn more robust feature representation and more reasonable prediction model simultaneously. In particular, low-rank regularization term plays a pivotal role in capturing the low-rank structure embedded in data and seeking the lowest-rank representation of samples in a latent low-dimensional subspace. Inverse-covariance regularization term is introduced to enforce the structured sparsity of regression coefficients by taking the multi-output structure into account. We also develop an alternative heuristic optimization algorithm to optimize our objective function. Experiment results on three publicly available datasets, i.e., Emotion6, Flickr_LDL and Twitter_LDL, using six measurements demonstrate the superior prediction performance compared with state-of-the-art algorithms.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Jing Zhang, Xinhui Li, <b>Peiguang Jing*</b>, Jing Liu, Yuting Su. Low-rank regularized heterogeneous tensor decomposition for subspace clustering, IEEE Signal Processing Letters, 2018, 25 (3), 333-337.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8025385">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{8025385,author={Zhang, Jing and Li, Xinhui and Jing, Peiguang and Liu, Jing and Su, Yuting},journal={IEEE Signal Processing Letters}, title={Low-Rank Regularized Heterogeneous Tensor Decomposition for Subspace Clustering}, year={2018},volume={25},number={3},pages={333-337},keywords={Tensile stress;Signal processing algorithms;Matrix decomposition;Clustering algorithms;Robustness;Sparse matrices;Algorithm design and analysis;Low-rank;subspace clustering;tensor decomposition},doi={10.1109/LSP.2017.2748604}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Jizhong Duan, Yu Liu, <b>Peiguang Jing</b>. Efficient operator splitting algorithm for joint sparsity-regularized SPIRiT-based parallel MR imaging reconstruction[J]. Magnetic Resonance Imaging, 2018, 46: 81-89.
                                    [<a href="https://pdf.sciencedirectassets.com/271222/1-s2.0-S0730725X17X0009X/1-s2.0-S0730725X17302369/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIBeu6qh6D7%2B%2BYFXwL7LPtCAJd0mc0OXF9YzI7aTNl7JKAiB2rKLstcvYAuAnyIkFjTx%2B9tK7b9vy9i1rBWFvXql7ZyqzBQhcEAUaDDA1OTAwMzU0Njg2NSIMh2IDHqyLJ5bZWARgKpAFJv5RGBNLIG%2FoBpldOFYKk1hzbpfXJX1kzjyUol2XKi2rdhV3oMfhl5OrbjCxuWhiCg0KwJ4%2BdRsx8Nr6vLyQnP%2Blysc7OG%2Bj0AmwM6uh0PEeAMZT7nFSrd%2Fw13dMQUQtbLOjnwXuUEsjZLCHbFCHPAtoQv4kgmLdGKsyfkzZuv5tu3qqt7n6sjgeQNGWQx9ZGapiO4GFOdFq3H4CtEdoVvNpfIq6Eyh0GZYIHutbNr1C4eivMbwSvAYoBNVvqH321hugzTjoepnZCqJmMB137t%2FhEQYT3S0IJH7bjz4PxALSbHOY%2FbbhWOs%2B2d1gHjtxtyJPRxusstMQQxdpdqEJd8J5Z8LQl3jE91Iuj3yScWYCVXRyb%2FE8erbkLUiR5Mn9r1PFm43zL7D55DifFFzOJt6op83FEmrAhZ720ohZ4M9Z5GPECzTEMGqEENHxUgKk70DS3Hk6RT%2FO0HHWRdcAcKPFq46o3kxf3VBoIWuZxtXKb0jl%2Bm%2B%2BpT08G3VFqcO9N1LTPuQaztJKhbhwrewShjmRo%2FaF9Pb2GuyLTN85RFNqMUMigk%2FTjjZmmxVaiOpWNVhcbheqCQLMnRsA5XkxBNr1rqaxLNEz4ZhXlxR0TfpN6lYHvuqk2XPpcmgyp91xBU4pPf0n35JsmmTySaG9jfTGm4jjzjHrPerViSNklxT0hhaOcF6eUbdD44wCebUnEs42pvjEUihv2hYAsl00Z8vxsJKKyxRU5%2FJ72%2FUZ51GsPLy0ChUe9DHr6fdkt8Y90MjoN3FlfjrNCiBzD7SH9V2aCSBPpxRsdCtIsCeTQ%2FmrN%2BiXDzHVR8QbeBD6BRtW9E9aus3F%2BC7E8NopI8yXwlSqsPR2kyPssYUot0IQtr0wqMCDwAY6sgFdEFVhK6DwwZ3xbCg2vFI5D%2BSzKomEZPHn7ewK6F5nJJx86F9q54ErpfPug9Lewp4K8TQHs8CN0d5Dw1bGgqh6ESmQSxlaM4ThRmzMPoEA0XuPw8K6W3ITbwidUUpiq4FkGI29iGKxlfbgCSPYlmFz5I74tgaNaFHv1dhzYwL8KKQTzPVhWUnLy6h064clAE9H%2BldBHPH96fozCLy4x6znKvYP0VJBkuz3o9qW4zkBMsTI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250417T114844Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYZX633P3W%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=1699d84e614319c07c258016879db99bdcb2a4fab5b123fb06f9728601542489&hash=3d7ee73135893b99718b11e47f98a08e4aaf8b60989aa3aae23f6f546920e356&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0730725X17302369&tid=spdf-173952fa-8ed3-4b4c-8c47-ae92ae82c852&sid=ed3ef35f88385649fc3a2920b4947c2e5fd5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=05085c5c57050d55015d&rr=931bb1f2f9aedd36&cc=cn&kca=eyJrZXkiOiJid3lDS2Z3WnFGdllVZFhmb3BkUFh0L0JsUHg0OXpTUUpHWHVmb2pMR05vWm91eklTaUg5SXN0T3BIcmthUC9LQ2ttR1UvQ2p6VHpPdXBhQXdsNElVbitYZnRYemFKYW5SL2U3VzlJVTF3eFFjNnU3N0xBMEpLaldyVnlsdFRBWkt3UWE2U00rNEJBL2pvRHgxdXhIN2ZLKy9pME0wY1BSTjJOUnVPZUVIeDVwdm16UCIsIml2IjoiZmQ5ZjMwOWVmODliYWE4MDRlOTcyMzZmOTQ2YmVhMjAifQ==_1744890529506">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@article{DUAN201881,title = {Efficient operator splitting algorithm for joint sparsity-regularized SPIRiT-based parallel MR imaging reconstruction},journal = {Magnetic Resonance Imaging},volume = {46},pages = {81-89},year = {2018},issn = {0730-725X},doi = {https://doi.org/10.1016/j.mri.2017.10.013},url = {https://www.sciencedirect.com/science/article/pii/S0730725X17302369},author = {Jizhong Duan and Yu Liu and Peiguang Jing},keywords = {Parallel magnetic resonance imaging, Auto-calibrating, Self-consistent parallel imaging (SPIRiT), Operator splitting, Joint total variation, FISTA, Barzilai and Borwein method},abstract = {Self-consistent parallel imaging (SPIRiT) is an auto-calibrating model for the reconstruction of parallel magnetic resonance imaging, which can be formulated as a regularized SPIRiT problem. The Projection Over Convex Sets (POCS) method was used to solve the formulated regularized SPIRiT problem. However, the quality of the reconstructed image still needs to be improved. Though methods such as NonLinear Conjugate Gradients (NLCG) can achieve higher spatial resolution, these methods always demand very complex computation and converge slowly. In this paper, we propose a new algorithm to solve the formulated Cartesian SPIRiT problem with the JTV and JL1 regularization terms. The proposed algorithm uses the operator splitting (OS) technique to decompose the problem into a gradient problem and a denoising problem with two regularization terms, which is solved by our proposed split Bregman based denoising algorithm, and adopts the Barzilai and Borwein method to update step size. Simulation experiments on two in vivo data sets demonstrate that the proposed algorithm is 1.3 times faster than ADMM for datasets with 8 channels. Especially, our proposal is 2 times faster than ADMM for the dataset with 32 channels.}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Luming Zhang, <b>Peiguang Jing#</b>, Yuting Su, Chao Zhang, Ling Shao. SnapVideo: Personalized Video Generation for a Sightseeing Trip. IEEE Transactions on Cybernetics, 2017 47 (11), 3866-3878. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7516655">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{7516655,author={Zhang, Luming and Jing, Peiguang and Su, Yuting and Zhang, Chao and Shaoz, Ling}journal={IEEE Transactions on Cybernetics}, title={SnapVideo: Personalized Video Generation for a Sightseeing Trip}, year={2017},volume={47}, number={11}, pages={3866-3878},keywords={Semantics;Mobile handsets;Probabilistic logic;Feature extraction;Google;Video recording;Quality assessment;Aesthetic;comprehensive views;scenic spots;SnapVideo;video clips},doi={10.1109/TCYB.2016.2585764}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> <b>Peiguang Jing</b>, Yuting Su, Liqiang Nie, Huimin Gu. Predicting Image Memorability Through Adaptive Transfer Learning From External Sources. IEEE Transactions on Multimedia, 2017, 19(5): 1050-1062. (<i><strong>中科院分区Top期刊</strong></i>)
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7797211">pdf</a>] 
                                    [<a href=" https://github.com/peiguangjing/image-memorability">code</a>][<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{7797211, author={Jing, Peiguang and Su, Yuting and Nie, Liqiang and Gu, Huimin}, journal={IEEE Transactions on Multimedia}, title={Predicting Image Memorability Through Adaptive Transfer Learning From External Sources}, year={2017},volume={19}, number={5}, pages={1050-1062},keywords={Visualization;Semantics;Feature extraction;Predictive models;Adaptation models;Learning systems;Dictionaries;Image attribute;memorability prediction;regression;transfer learning;visual feature},doi={10.1109/TMM.2016.2644866}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Jing Zhang, Chuanzhong Xu, <b>Peiguang Jing</b>, Yuting Su*. A tensor-driven temporal correlation model for video sequence classification. IEEE Signal Processing Letters, 2016, 23(9): 1246-1249.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7486042">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{7486042,author={Zhang, Jing and Xu, Chuanzhong and Jing, Peiguang and Zhang, Chengqian and Su, Yuting},journal={IEEE Signal Processing Letters}, title={A Tensor-Driven Temporal Correlation Model for Video Sequence Classification}, year={2016}, volume={23},number={9},pages={1246-1249},keywords={Tensile stress;Video sequences;Correlation;Feature extraction;Linear programming;Time series analysis;Learning systems;Autoregressive;tensor decomposition;video sequence classification},doi={10.1109/LSP.2016.2577601}}')">bib</a>]             
                                </p>
                                </li>

                                <li><p align="justify"> Yanwei Pang, Zhong Ji*, <b>Peiguang Jing</b>, Xuelong Li. Ranking graph embedding for learning to rerank. IEEE Transactions on Neural Networks and Learning Systems, 2013, 24(8): 1292-1303.
                                    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6508899">pdf</a>] 
                                    [<a href="#" onclick="copyTextAndNotify(event, '@ARTICLE{6508899,author={Pang, Yanwei and Ji, Zhong and Jing, Peiguang and Li, Xuelong}, journal={IEEE Transactions on Neural Networks and Learning Systems},  title={Ranking Graph Embedding for Learning to Rerank}, year={2013},volume={24},number={8},pages={1292-1303}, keywords={Dimensionality reduction;graph embedding;image search reranking;learning to rank},doi={10.1109/TNNLS.2013.2253798}}')">bib</a>]             
                                </p>
                                </li>

                            </ol>
                        </div>
                    

                        <div id="2021" class="paper-list">
                            <ol>

                                <li><p align="justify"> 褚晶辉,史李栋,<b>井佩光</b>,吕卫.适用于目标检测的上下文感知知识蒸馏网络.浙江大学学报(工学版),2022,56(03):503-509.
                                </p>
                                </li>

                                <li><p align="justify"> <b>井佩光</b>,李亚鑫,苏育挺.一种多模态特征编码的短视频多标签分类方法.西安电子科技大学学报,2022,49(04):109-117.
                                </p>
                                </li>

                                <li><p align="justify"> 张丽娟,崔天舒,<b>井佩光</b>,苏育挺.基于深度多模态特征融合的短视频分类.北京航空航天大学学报，2021，47(03)：478-485.
                                    [<a href="https://bhxb.buaa.edu.cn/bhzk/article/doi/10.13700/j.bh.1001-5965.2020.0457">pdf</a>] 
                                </p>
                                </li>

                                <li><p align="justify"> 李云,卢志翔,刘姝伊,王粟,吕梓民,<b>井佩光</b>.基于深度多模态关联学习的短视频多标签分类研究[J].数据分析与知识发现,2024,8(07):77-88.
                                    [<a href="https://manu44.magtech.com.cn/Jwk_infotech_wk3/CN/Y2024/V8/I7/77">pdf</a>] 
                                </li>

                                <li><p align="justify"> 吕卫,韩镓泽,褚晶辉,<b>井佩光</b>.基于多模态自注意力网络的视频记忆度预测[J].吉林大学学报(工学版),2023,53(04):1211-1219.
                                    [<a href="http://xuebao.jlu.edu.cn/gxb/EN/abstract/abstract14707.shtml">pdf</a>]
                                </p>
                                </li>

                                <li><p align="justify"> 苏育挺,王骥,赵玮,<b>井佩光</b>.基于动态图卷积的图像情感分布预测[J].吉林大学学报(工学版),2023,53(09):2601-2610.
                                    [<a href="https://xuebao.jlu.edu.cn/gxb/CN/Y2023/V53/I9/2601">pdf</a>]
                                </p>
                                </li>

                                <li><p align="justify"> 苏育挺,王富铕,<b>井佩光</b>.深度多模态不确定度的短视频事件检测方法[J].哈尔滨工业大学学报,2024,56(05):36-45.
                                </p>
                                </li>

                                <li><p align="justify"> <b>井佩光</b>,田雨豆,汪少初,李云,苏育挺.基于动态扩散图卷积的交通流量预测算法[J].吉林大学学报(工学版),2024,54(06):1582-1592.
                                    [<a href="http://xuebao.jlu.edu.cn/gxb/CN/abstract/abstract15155.shtml">pdf</a>]
                                </p>
                                </li>

                                <li><p align="justify"> 李云,孙山林,黄晴,<b>井佩光</b>.基于多路混合注意力机制的水下图像增强网络[J].电子与信息学报,2024,46(01):118-128.
                                    [<a href="https://jeit.ac.cn/cn/article/pdf/preview/10.11999/JEIT230495.pdf">pdf</a>]
                                </p>
                                </li>

                                <li><p align="justify"> 苏育挺,景梦瑶,<b>井佩光</b>,刘先燚.基于光度立体和深度学习的电池缺陷检测方法[J].吉林大学学报(工学版),2024,54(12):3653-3659.
                                    [<a href="http://xuebao.jlu.edu.cn/gxb/CN/10.13229/j.cnki.jdxbgxb.20230130">pdf</a>]
                                </p>
                                </li>

                            </ol>
                        </div>
                    </section>

        

                    <p id="services" style="height:4rem;margin: 0;">&nbsp;</p>
                    <section style="font-size: 1.8rem;">
                        <h3><strong>学术兼职</strong></h3>
                        <table style="width:100%; font-size:18px; border: 0;">
                            
                            <tr style="height: 30px;">
                                <th>学会会员：<th>
                            </tr>
                            <tr style="height: 30px;">
                                <td><ul>
                                     <li style="height: 30px;">IEEE计算机学会</li>
                                     <li style="height: 30px;">中国电子学会</li>
                                     <li>中国图象图形学学会</li>
                                    </ul></td>                               
                            </tr>

                            <tr style="height: 30px;">
                                <th>国际SCI期刊的客座编委：<th>
                            </tr>
                            <tr style="height: 30px;">
                                <td><ul>
                                     <li style="height: 30px;">Information Processing & Management</li>
                                     <li>Multimedia Tools and Applications</li>
                                    </ul></td>                              
                            </tr>

                            <tr style="height: 30px;">
                                <th>国际一流期刊或审稿人：<th>
                            </tr>
                            <tr style="height: 30px;">
                                <td><ul>
                                     <li style="height: 30px;">IEEE TPAMI/TNNLS/TIP/TGRS/TKDE/TMM/TCSVT/TCYB/SPL</li>
                                     <li style="height: 30px;">ACM TWEB/TOMM/MM</li>
                                     <li>ACM TWEB/TOMM/TIS/Inforamtion Sciences/IPM/Information Fusion/Neural Networks/Pattern Recognition/Science in China: Information Science/中国科学：技术科学</li>
                                    </ul></td>                             
                            </tr>   
                            
                            <tr style="height: 30px;">
                                <th>会议：<th>
                            </tr>
                            <tr style="height: 30px;">
                                <td><ul>
                                     <li style="height: 30px;">ICCV/NIPS/AAAI/MM</li>
                                    </ul></td>                             
                            </tr>  



                        </table>





                    </section>


                    <p id="address" style="height:4rem;margin: 0;">&nbsp;</p>
                    <section style="font-size: 1.8rem;">
                        <h3><strong>通讯信息</strong></h3>
                        <ul>
                            <p><strong>E-mail：</strong><a href="mailto:pgjing@tju.edu.cn">pgjing@tju.edu.cn</a></p>
                            <p><strong>通讯地址：</strong>天津大学电气自动化与信息工程学院26教学楼D区330，天津300072，中国</p>
                           <!-- <a href="https://clustrmaps.com/site/1c5nd"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=6vmM-gHXbUgz9Jofm0HjOaP9uua8KiBQaYUO_qjei14&cl=ffffff" /></a>   -->
                           <a href="https://clustrmaps.com/site/1c5nd" title="ClustrMaps" target="_blank" style="display: inline-block; text-decoration: none;">
                            <img 
                              src="//www.clustrmaps.com/map_v2.png?d=6vmM-gHXbUgz9Jofm0HjOaP9uua8KiBQaYUO_qjei14&cl=ffffff" 
                              alt="Visitor Map"
                              style="
                                width: 200px; 
                                height: auto; 
                                border: 2px solid rgb(255, 255, 255); 
                                border-radius: 10px; 
                                box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);
                                transition: transform 0.3s ease;
                              "
                              onmouseover="this.style.transform='scale(1.05)'"
                              onmouseout="this.style.transform='scale(1)'"
                            />
                          </a>
            

                        </ul>
                    </section>


                </div>
            </div>
        </dic>
    </div>

    <script type="text/javascript" src="common/js/jquery.min.js"></script>
    <script type="text/javascript" src="common/js/bootstrap.min.js"></script>
    <Script>
        $('.moreclick').unbind().on('click',function(){
            $(this).hide();
            $('.more').show();
            $('.morehide').show();
        })
        $('.morehide').unbind().on('click',function(){
            $(this).hide();
            $(".moreclick").show();
            $('.more').hide();
        })
        $('.navbar-nav li').unbind().on('click',function(){
            if($(this).hasClass("active")){
                console.log(1)
            }else{
                $(this).addClass("active");
                $(this).siblings().removeClass('active');
            }
        })



        function showPapers(year) {
            // 获取所有论文列表和标签
            var paperLists = document.querySelectorAll('.paper-list');
            var tabs = document.querySelectorAll('.tab');
            
            // 隐藏所有论文列表
            paperLists.forEach(function(list) {
                list.classList.remove('active');
            });

            // 移除所有标签的激活状态
            tabs.forEach(function(tab) {
                tab.classList.remove('active');
            });

            // 显示选择的年份的论文列表
            document.getElementById(year).classList.add('active');

            // 设置当前标签为激活状态
            document.querySelector('.tab[onclick="showPapers(\'' + year + '\')"]').classList.add('active');
        }

        function showRecentPapers(year) {
            var currentYear = year;
            // var lastYear = currentYear - 1;
            
            // 显示当前年论文列表
            document.getElementById(currentYear).classList.add('active');
            // document.getElementById(lastYear).classList.add('active');

            // 激活当前年标签
            document.querySelector('.tab[onclick="showPapers(\'' + currentYear + '\')"]').classList.add('active');
            // document.querySelector('.tab[onclick="showPapers(\'' + lastYear + '\')"]').classList.add('active');
        }

        // 默认显示最近一年的论文
        showRecentPapers(2026);




        // 显示前8条论文
        document.addEventListener("DOMContentLoaded", function() {
            var newsItems = document.querySelectorAll('.item');
            for (var i = 0; i < 8; i++) {
                if (newsItems[i]) {
                    newsItems[i].style.display = 'list-item';
                }
            }
        });


        document.getElementById('show-less').style.display = 'none'; // 隐藏Show Less按钮
        // Show More功能
        function showMorePapers() {
            var newsItems = document.querySelectorAll('.item');
            newsItems.forEach(function(item) {
                item.style.display = 'list-item'; // 显示所有论文
            });
            document.getElementById('show-more').style.display = 'none'; // 隐藏Show More按钮
            document.getElementById('show-less').style.display = 'inline-block'; // 显示Show Less按钮
        }

        // Show Less功能
        function showLessPapers() {
            var newsItems = document.querySelectorAll('.item');
            for (var i = 0; i < newsItems.length; i++) {
                if (i < 8) {
                    newsItems[i].style.display = 'list-item'; // 显示前8条论文
                } else {
                    newsItems[i].style.display = 'none'; // 隐藏其他论文
                }
            }
            document.getElementById('show-more').style.display = 'inline-block'; // 显示Show More按钮
            document.getElementById('show-less').style.display = 'none'; // 隐藏Show Less按钮
        }



        function copyTextAndNotify(event, text) {
            event.preventDefault();

            // 创建一个临时的textarea元素
            var textarea = document.createElement("textarea");
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            textarea.setSelectionRange(0, 99999); // 对于移动设备

            // 执行复制操作
            document.execCommand("copy");

            // 移除临时元素
            document.body.removeChild(textarea);

            // 显示提示信息
            var notification = document.getElementById("notification");
            notification.style.display = "block";

            // 3秒后隐藏提示信息
            setTimeout(function() {
                notification.style.display = "none";
            }, 3000);
        }


    </Script>

    
</body>
</html>
